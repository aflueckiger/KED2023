[
  {
    "objectID": "materials.html",
    "href": "materials.html",
    "title": "Materials",
    "section": "",
    "text": "Before you can actually start programming on your computer, you need to set up the technical environment. Depending on your operating system, the installation procedure may vary. This guide provides step-by-step instructions.\nIf anything does not work as described, get in touch so that I can help."
  },
  {
    "objectID": "materials.html#cheatsheet-data-wrangling-in-bash",
    "href": "materials.html#cheatsheet-data-wrangling-in-bash",
    "title": "Materials",
    "section": "Cheatsheet Data Wrangling in Bash",
    "text": "Cheatsheet Data Wrangling in Bash\nThe cheatsheet contains commands relevant for data wrangling task preceding any textual analysis. In addition to common Bash commands, it also features examples of regular expressions (RegEx)."
  },
  {
    "objectID": "materials.html#python-basics",
    "href": "materials.html#python-basics",
    "title": "Materials",
    "section": "Python Basics",
    "text": "Python Basics\nThe Jupyter Notebook contains basic Python statements that you need to know before you can make use of the third-party libraries."
  },
  {
    "objectID": "materials.html#nlp-with-python---an-example-analysis",
    "href": "materials.html#nlp-with-python---an-example-analysis",
    "title": "Materials",
    "section": "NLP with Python - An example analysis",
    "text": "NLP with Python - An example analysis\nIn this online-hosted notebook, you find many bits to start a powerful analysis based on simple techniques. It is a starters guide to enable social scientists to conduct NLP analysis. The kind of data, the relative simplicity of the methods, and the general approach are in line with social science practices and bring statistics together with the context of documents.\nYou can execute the code online to get a brief glimpse without bothering with installation."
  },
  {
    "objectID": "assignments/assignment_3/KED2023_assignment_3.html",
    "href": "assignments/assignment_3/KED2023_assignment_3.html",
    "title": "KED2023 Assignment 3: Python NLP",
    "section": "",
    "text": "Deadline: 20 May 2023, 23:59\nFile format: Jupyter Notebook (.ipynb) or executable Python script (.py)\nNaming schema: SURNAME_KED2023_3.ipynb or SURNAME_KED2023_3.py Replace SURNAME with your surname.\nTasks require Python commands only, not Bash.\nSubmit your solutions on time via the respective exercise module on OLAT. The module is only open until midnight.\nFind solutions individually. When you are stuck, post your issue in the OLAT forum and ask friends. In terms of programming, Google may be your best friend."
  },
  {
    "objectID": "assignments/assignment_3/KED2023_assignment_3.html#alternative-gender-instead-of-history",
    "href": "assignments/assignment_3/KED2023_assignment_3.html#alternative-gender-instead-of-history",
    "title": "KED2023 Assignment 3: Python NLP",
    "section": "Alternative: Gender instead of History",
    "text": "Alternative: Gender instead of History\nAre you rather tired of history and more interested in gender? Alternatively, you can divide the corpus by gender. Nevertheless, you should limit to German speeches hold from 1999 onwards for historical reasons3 (meta attributes Geschlecht, Sprache, Jahr)."
  },
  {
    "objectID": "assignments/assignment_3/KED2023_assignment_3.html#function-to-read-dataset",
    "href": "assignments/assignment_3/KED2023_assignment_3.html#function-to-read-dataset",
    "title": "KED2023 Assignment 3: Python NLP",
    "section": "Function to read Dataset",
    "text": "Function to read Dataset\ndef get_texts_from_csv(f_csv, text_column):\n    \"\"\"\n    Read dataset from a csv file and sequentially stream the rows,\n    including metadata.\n    \"\"\"\n    \n    # read dataframe\n    df = pd.read_csv(f_csv)\n    \n    # keep only documents that have text\n    filtered_df = df[df[text_column].notnull()]\n    \n    # iterate over rows in dataframe\n    for idx, row in filtered_df.iterrows():\n        \n        # read text and join lines (hard line-breaks)\n        text = row[text_column].replace('\\n', ' ')\n\n        # use all columns as metadata, except the column with the actual text\n        metadata = row.to_dict()\n        del metadata[text_column]\n\n        yield (text, metadata)\n\nf_csv = '../materials/dataset_speeches_federal_council_2019.csv'\ntexts = get_texts_from_csv(f_csv, text_column='Text')\n\ncorpus_speeches = textacy.Corpus(de, data=texts)"
  },
  {
    "objectID": "assignments/assignment_3/flueckiger_KED2023_3_solutions.html",
    "href": "assignments/assignment_3/flueckiger_KED2023_3_solutions.html",
    "title": "KED2023",
    "section": "",
    "text": "The working directory is set to KED2023 instead to the directory where this Jupyter notebook is located. Thus, I can specify the pathes relative to the top folder KED2023.\n\nimport os\nprint(\"Old working directory:\", os.getcwd())\nos.chdir('../..')\nprint(\"New working directory:\", os.getcwd())\n\nOld working directory: /home/alex/KED2023/assignments/assignment_3\nNew working directory: /home/alex/KED2023\n\n\nTo run this notebook as script from the command line, you can export it as .py file via the menubar and save it to assignments/assignment_3/flueckiger_KED2023_3_solutions.py.\nThen, you can navigate to the KED2023 directory and call:\npython assignments/assignment_3/flueckiger_KED2023_3_solutions.py\n\n\n\n\n# task 3: import the modules that are needed\nimport textacy\nimport pandas as pd\n\n\n# task 4: create a corpus\n# define new function to read text from csv file (copied as given in assignment)\n\ndef get_texts_from_csv(f_csv, text_column):\n\n    # read dataframe\n    df = pd.read_csv(f_csv)\n\n    # keep only documents that have text\n    filtered_df = df[df[text_column].notnull()]\n\n    # iterate over rows in dataframe\n    for idx, row in filtered_df.iterrows():\n\n        # read text and join lines (hard line-breaks)\n        text = row[text_column].replace(\"\\n\", \" \")\n\n        # use all columns as metadata, except the column with the actual text\n        metadata = row.to_dict()\n        del metadata[text_column]\n\n        yield (text, metadata)\n\n\n# set correct path relative to working directory (folder where you saved this script)\nf_csv = \"materials/data/dataset_speeches_federal_council_2019.csv\"\n\n# stream the csv-dataset by calling the function defined above\ntexts = get_texts_from_csv(f_csv, text_column=\"Text\")\n\n# load German language model\nde = textacy.load_spacy_lang(\"de_core_news_sm\")\n\n# create a corpus with all the texts\ncorpus_speeches = textacy.Corpus(de, data=texts)\n\n\n# task 5: two subcorpora\n# define two functions filtering by language and period\n# similar as the lambda functions shown in assignment, yet they may be simpler to understand\n\ndef filter_func_pre(doc):\n    return doc._.meta.get(\"Sprache\") == \"de\" and doc._.meta.get(\"Jahr\") < 2000\n\n\n# greater-equal to include the year 2000\ndef filter_func_post(doc):\n    return doc._.meta.get(\"Sprache\") == \"de\" and doc._.meta.get(\"Jahr\") >= 2000\n\n\n#########################################\n# ALTERNATIVE task 5: filter by gender instead of period\n# You just need to call these functions when creating the supcorpora below.\n# Everything else keeps the same.\ndef filter_func_women(doc):\n    return (\n        doc._.meta.get(\"Sprache\") == \"de\"\n        and doc._.meta.get(\"Jahr\") >= 1999\n        and doc._.meta.get(\"Geschlecht\") >= \"f\"\n    )\n\n\ndef filter_func_men(doc):\n    return (\n        doc._.meta.get(\"Sprache\") == \"de\"\n        and doc._.meta.get(\"Jahr\") >= 1999\n        and doc._.meta.get(\"Geschlecht\") >= \"m\"\n    )\n\n\n#########################################\n\n# create two new subcorpora after applying filter function\nsubcorpus_pre = textacy.corpus.Corpus(de, data=corpus_speeches.get(filter_func_pre))\nsubcorpus_post = textacy.corpus.Corpus(de, data=corpus_speeches.get(filter_func_post))\n\n\n# task 6: print number of docs for both subcorpora\nprint(\"# documents in Subcorpus 1 (before 2000):\", subcorpus_pre.n_docs)\nprint(\"# documents in Subcorpus 2 (as of 2000):\", subcorpus_post.n_docs)\n\n# documents in Subcorpus 1 (before 2000): 63\n# documents in Subcorpus 2 (as of 2000): 97\n\n\n\n# task 7: export the vocabulary of both subcorpora into file\n\n# lowercased, unigram\nvocab_pre = subcorpus_pre.word_counts(\n    by=\"lower_\",\n    filter_stops=True,\n    filter_punct=True,\n    filter_nums=True,\n    weighting=\"freq\",  # get relative frequency instead of absolute\n)\nvocab_post = subcorpus_post.word_counts(\n    by=\"lower_\",\n    filter_stops=True,\n    filter_punct=True,\n    filter_nums=True,\n    weighting=\"freq\",\n)\n\n# sort vocabulary by descending frequency\nvocab_sorted_pre = sorted(vocab_pre.items(), key=lambda x: x[1], reverse=True)\nvocab_sorted_post = sorted(vocab_post.items(), key=lambda x: x[1], reverse=True)\n\n# write to file, one word and its frequency per line\nfname = \"assignments/assignment_3/vocab_frq_pre.txt\"\nwith open(fname, \"w\") as f:\n    for word, frq in vocab_sorted_pre:\n        line = f\"{word}\\t{frq}\\n\"\n        f.write(line)\n\nfname = \"assignments/assignment_3/vocab_frq_post.txt\"\nwith open(fname, \"w\") as f:\n    for word, frq in vocab_sorted_post:\n        line = f\"{word}\\t{frq}\\n\"\n        f.write(line)\n\n\n\n\n\n\nThe top ten words look surprisingly similiar. Tradition seems to be reflected not only in the ritual itself but in the language of these speeches as well.\nAt a second glance, there are interesting differences though: - speakers start to talk more about “europa” and “EU” after 2000 - nowadays, the term “Eidgenossenschaft” is primarily used by right-wing people and got generally replaced by “Schweiz” - gender became important: greeting women as “Damen” besides “Herren” (notably, there are not only “Schweizerinnen und Schweizer”) - “Sicherheit” becomes a hot topic after the relatively calm post-war period - increased talking about “Kultur”, “Identität”, “Werte”. Who are we (no longer limited to Swiss people) in a globalized and multi-cultural world?\nTo compare the vocabulary in a more systematic way than simple eye-balling, it helps to compute the relative change in frequency between the epochs. This goes beyond what we have touched upon in the seminar. Yet, it is worth to look at it.\n\n# create dataframe for both subcorpora\ndf_pre = pd.DataFrame(vocab_sorted_pre, columns=[\"term\", \"frq\"])\ndf_post = pd.DataFrame(vocab_sorted_post, columns=[\"term\", \"frq\"])\n\n# merge them into a single dataset on the basis of term\ndf = df_pre.merge(df_post, on=\"term\", how=\"inner\", suffixes=(\"_pre\", \"_post\"))\n\n# compute the relative difference and sort by it\ndf[\"diff\"] = df[\"frq_post\"] - df[\"frq_pre\"]\ndf.sort_values(\"diff\", inplace=True)\n\n# save the results as csv dataset\nfname = \"assignments/assignment_3/vocab_diff_periods.csv\"\ndf.to_csv(fname)\n\n\n# show terms with biggest increase wtr to relative frequency\ndf.tail(20)\n\n\n\n\n\n  \n    \n      \n      term\n      frq_pre\n      frq_post\n      diff\n    \n  \n  \n    \n      82\n      feiern\n      0.000437\n      0.000719\n      0.000282\n    \n    \n      305\n      bevölkerung\n      0.000185\n      0.000477\n      0.000292\n    \n    \n      220\n      kultur\n      0.000235\n      0.000536\n      0.000301\n    \n    \n      949\n      bürgerinnen\n      0.000067\n      0.000386\n      0.000319\n    \n    \n      927\n      erfolg\n      0.000067\n      0.000392\n      0.000325\n    \n    \n      195\n      wohlstand\n      0.000269\n      0.000595\n      0.000326\n    \n    \n      1212\n      identität\n      0.000050\n      0.000392\n      0.000342\n    \n    \n      355\n      braucht\n      0.000168\n      0.000510\n      0.000342\n    \n    \n      344\n      herren\n      0.000168\n      0.000549\n      0.000381\n    \n    \n      1260\n      wichtig\n      0.000050\n      0.000438\n      0.000388\n    \n    \n      158\n      gemeinsam\n      0.000302\n      0.000693\n      0.000391\n    \n    \n      66\n      weg\n      0.000487\n      0.000883\n      0.000395\n    \n    \n      185\n      gesellschaft\n      0.000269\n      0.000667\n      0.000398\n    \n    \n      194\n      werte\n      0.000269\n      0.000706\n      0.000437\n    \n    \n      734\n      damen\n      0.000084\n      0.000530\n      0.000446\n    \n    \n      180\n      sicherheit\n      0.000286\n      0.000883\n      0.000597\n    \n    \n      14\n      schweizer\n      0.001059\n      0.001857\n      0.000798\n    \n    \n      15\n      menschen\n      0.001025\n      0.001896\n      0.000871\n    \n    \n      0\n      land\n      0.002571\n      0.003818\n      0.001247\n    \n    \n      1\n      schweiz\n      0.002554\n      0.006989\n      0.004435\n    \n  \n\n\n\n\n\n# show terms with biggest decrease wtr to relative frequency\ndf.head(20)\n\n\n\n\n\n  \n    \n      \n      term\n      frq_pre\n      frq_post\n      diff\n    \n  \n  \n    \n      2\n      unseres\n      0.002268\n      0.001013\n      -0.001255\n    \n    \n      11\n      eidgenossen\n      0.001126\n      0.000216\n      -0.000910\n    \n    \n      10\n      eidgenossenschaft\n      0.001210\n      0.000314\n      -0.000896\n    \n    \n      5\n      landes\n      0.001663\n      0.001020\n      -0.000644\n    \n    \n      9\n      staat\n      0.001277\n      0.000641\n      -0.000636\n    \n    \n      38\n      unsern\n      0.000639\n      0.000013\n      -0.000625\n    \n    \n      23\n      probleme\n      0.000891\n      0.000268\n      -0.000622\n    \n    \n      20\n      bund\n      0.000907\n      0.000288\n      -0.000620\n    \n    \n      13\n      volk\n      0.001109\n      0.000497\n      -0.000612\n    \n    \n      34\n      aufgaben\n      0.000655\n      0.000105\n      -0.000551\n    \n    \n      18\n      gemeinschaft\n      0.000924\n      0.000379\n      -0.000545\n    \n    \n      54\n      schweizervolk\n      0.000538\n      0.000020\n      -0.000518\n    \n    \n      26\n      willen\n      0.000790\n      0.000275\n      -0.000515\n    \n    \n      36\n      schweizerischen\n      0.000639\n      0.000124\n      -0.000514\n    \n    \n      51\n      innern\n      0.000538\n      0.000039\n      -0.000498\n    \n    \n      53\n      vaterland\n      0.000538\n      0.000039\n      -0.000498\n    \n    \n      27\n      kraft\n      0.000790\n      0.000294\n      -0.000496\n    \n    \n      42\n      abend\n      0.000605\n      0.000118\n      -0.000487\n    \n    \n      24\n      frieden\n      0.000840\n      0.000366\n      -0.000474\n    \n    \n      39\n      geist\n      0.000622\n      0.000163\n      -0.000458"
  },
  {
    "objectID": "assignments/assignment_1/KED2023_assignment_1_v2.html",
    "href": "assignments/assignment_1/KED2023_assignment_1_v2.html",
    "title": "KED2023 Assignment 1: Data Wrangling",
    "section": "",
    "text": "Requirements\n\nDeadline: 15 May 2023, 23:59\nFile format: executable shell script\nNaming schema: SURNAME_KED2023_1.sh\nReplace SURNAME with your surname.\nUse the shell template provided here.\nAll tasks require shell commands unless stated otherwise.\nSubmit your solutions on time via the respective exercise module on OLAT. The module is only open until midnight.\nFind solutions individually. When you are stuck, post your issue in the OLAT forum and ask friends. In terms of programming, Google may be your best friend.\n\n\n\nIntroduction\nYou learn how to perform basic shell commands and wrap them into a script to reproduce all steps.\nUse a text editor to write your script (e.g., Visual Studio). You may want to try out the commands directly in your shell and, after successfully running them, copy them over into your script. Make sure that you include commands and comments only, while excluding the preamble like Username@Computername$. The command history shows the history of all used commands.\nFollow this shell template when you write your script.\n\n\nOrganize your project\nIn this first task, you don’t need to provide any interpretation, only the raw commands.\nYou set the structures of a new project in this task. As a project grows over time, it is crucial to organize your work properly. Otherwise, you get lost or waste too much time to find a particular file.\n\nCreate a new project folder with the following name:\nKED2023_exercise_1\nWhere did you create your project folder? In addition to the command, write the absolute path as a # comment into your script.\nIn the folder you have created, make the following subfolders:\nreports, src, data, data/raw, data/interim\nIn a project, you may have thousands of text files named inconsistently. To simulate this, create empty files with the following commands in the folder data/raw:\ntouch data/raw/speeches_{2018..2022}_{a..z}.txt \ntouch data/raw/text_{2018..2022}_{1..12}_{1..30}.txt\nDon’t forget to add these commands to your script.\nOrganize these files per year without modifying the original data directly. Thus, create folders for each year (2018-2022) in data/interim. Copy the created .txt files from above into the folder of the corresponding year. For example, a file with 2023 in its name goes into the directory named 2022. Hint: Recall the expansion and wildcard operations.\nChange into the directory by using the absolute path to the main folder of this exercise KED2023_exercise_1. Use tree to check if they are located in the correct folders with the correct name. When you are using macOS, you may need to install the program first with brew install tree.\n\nBeyond this toy project, you may want to learn more about how to organize project. The cookie cutter website is a great resource that provides useful recommendation how to organize your data science project reasonably.\n\n\nReport on file collection\nIn this second task, please give a short explanation accompanying your command.\nWhat files do you have on your computer? Let’s create some reports. You are free to choose the name of your output files. Yet, please recall the conventions that help others to understand the purpose of your scripts and outputs.\n\nNavigate to the folder where you have saved most of your documents on the computer.1 Print the path to this directory.\nUse ls together with single and double asterisks to select all .pdf files in this folder (incl. subfolders) and write the output directly into a new file using operators. Check out this post on Stackoverflow how to use asteriks. Write a single command only.\nWrite a single command to list all files in the current directory ordered by date, select the oldest file and write the output into a new file using a pipe and an operator.\n\n\n\nTest your script\nThis task is a simple sanity check for your script. Your script has to pass this test, yet you don’t need to include it in your submission.\nYour deliverable has to be a runnable script comprising all the commands to accomplish the tasks above. To test your script, run the commands below. Once you call the script, it executes all commands, one after another. Everything should be reconstructed accordingly in the test folder. If not, correct the script so that it runs without any issue.\nmkdir test_script\ncd test_script\nbash PATH/SCRIPT_NAME.sh    # e.g. bash ../flueckiger_KED2023_1.sh\ncd ..\nrm -r test_script\n\n\nFeedback\nPlease answer the following questions at the end of your script. Start your answers with the # symbol to make them comments that are ignored when running the script.\n\nDo you have any questions concerning the exercise or the commands?\nHow long did it take to solve this exercise? Give a fair estimation.\n\n\n\n\n\n\nFootnotes\n\n\nOn Windows with Ubuntu installed, it should be located in /mnt/c/Users/YOUR_USERNAME (when you followed the installation guide, it can be accessed using the shortcut named documents. On macOS, it is located in /home/YOUR_USERNAME↩︎"
  },
  {
    "objectID": "assignments/assignment_1/KED2023_assignment_1.html",
    "href": "assignments/assignment_1/KED2023_assignment_1.html",
    "title": "KED2023 Assignment 1: Data Wrangling",
    "section": "",
    "text": "Requirements\n\nDeadline: 31 March 2023, 23:59\nFile format: executable shell script\nNaming schema: SURNAME_KED2023_1.sh\nReplace SURNAME with your surname.\nUse the shell template provided here.\nAll tasks require shell commands unless stated otherwise.\nSubmit your solutions on time via the respective exercise module on OLAT. The module is only open until midnight.\nFind solutions individually. When you are stuck, post your issue in the OLAT forum and ask friends. In terms of programming, Google may be your best friend.\n\n\n\nMotivation\nYou learn how to perform basic shell commands and wrap them into a script to reproduce all steps.\nUse a text editor to write your script (e.g., Visual Studio). You may want to try out the commands directly in your shell and, after successfully running them, copy them over into your script. The command history shows the history of all used commands.\nFollow this shell template when you write your script.\n\n\nOrganize your project\nIn this first task, you don’t need to provide any interpretation, only the raw commands.\nYou set the structures of a new project in this task. As a project grows over time, it is crucial to organize your work properly. Otherwise, you get lost or waste too much time to find a particular file.\n\nCreate a new project folder with the following name:\nKED2023_exercise_1\nWhere did you create your project folder? In addition to the command, write the absolute path as a # comment into your script.\nIn the folder you have created, make the following subfolders:\nreports, src, data, data/raw, data/interim\nIn a project, you may have thousands of text files named inconsistently. To simulate this, create empty files with the following commands in the folder data/raw:\ntouch data/raw/speeches_{2019..2022}_{a..z}.txt \ntouch data/raw/text_{2019..2022}_{1..12}_{1..30}.txt\nDon’t forget to add these commands to your script.\nOrganize these files per year without modifying the original data directly. Thus, create folders for each year (2019-2022) in data/interim. Copy the created .txt files from above into the folder of the corresponding year. For example, a file with 2023 in its name goes into the directory named 2022. Hint: Recall the expansion and wildcard operations.\n\nBeyond this toy project, you may want to learn more about how to organize your research project. The cookie cutter website is a great resource that provides useful information on reasonable organization of your data science project.\n\n\nReport on file collection\nIn this second task, please give a short explanation accompanying your command.\nWhat files do you have on your computer? Let’s create some reports. You are free to choose the directory and name for your output files. Yet, please recall the conventions that help others to understand the purpose of your scripts and outputs. For the following tasks, you need pipes and operators.\n\nCount the total number of all .pdf, .txt, and .docx files on your computer and write the resulting number into a new file using operators. Write a single command that searches, counts and subsequently writes the result into a file. For counting, you may want to use the wc -l command.\nWrite a single command to get the file names of the 20 oldest .pdf in your document folder, including any subfolder, and write them into a new file. Check out this post on Stackoverflow for a hint.\n\n\n\nTest your script\nThis task is a simple sanity check for your script. Your script has to pass this test, yet you don’t need to include it in your submission.\nYour deliverable has to be a runnable script comprising all the commands to accomplish the tasks above. To test your script, run the commands below. Once you call the script, it executes all commands, one after another. Everything should be reconstructed accordingly in the test folder. If not, correct the script so that it runs without any issue.\nmkdir test_script\ncd test_script\nbash PATH/SCRIPT_NAME.sh    # e.g. bash ../flueckiger_KED2023_1.sh\ncd ..\nrm -r test_script\n\n\nFeedback\nPlease answer the following questions at the end of your script. Start your answers with the # symbol to make them comments that are ignored when running the script.\n\nDo you have any questions concerning the exercise or the commands?\nHow long did it take to solve this exercise? Give a fair estimation."
  },
  {
    "objectID": "assignments/assignment_2/KED2023_assignment_2.html",
    "href": "assignments/assignment_2/KED2023_assignment_2.html",
    "title": "KED2023 Assignment 2: RegEx NLP",
    "section": "",
    "text": "Deadline: 15 April 2023, 23:59\nFile format: executable shell script\nNaming schema: SURNAME_KED2023_2.sh\nReplace SURNAME with your surname.\nUse the shell template provided here.\nAll tasks require shell commands unless stated otherwise.\nSubmit your solutions on time via the respective exercise module on OLAT. The module is only open until midnight.\nFind solutions individually. When you are stuck, post your issue in the OLAT forum and ask friends. In terms of programming, Google may be your best friend."
  },
  {
    "objectID": "assignments/assignment_2/KED2023_assignment_2.html#how-to-start",
    "href": "assignments/assignment_2/KED2023_assignment_2.html#how-to-start",
    "title": "KED2023 Assignment 2: RegEx NLP",
    "section": "How to start?",
    "text": "How to start?\nUse a text editor to write your script (e.g., Visual Studio). You may want to try out the commands directly in your shell and, after successfully running them, copy them over into your script. The command history shows the history of all used commands.\nFollow this shell template when you write your script.\nWhile coming up with a solution, you may find the following commands useful along the way:\n# have a look at the document\nmore newspaper_articles.txt\n\n# useful to check what egrep matches before you write into a file\n# colouring output is probably activated by default\negrep --colour \"pattern\" newspaper_articles.txt"
  },
  {
    "objectID": "lectures/md/KED2023_supplements.html",
    "href": "lectures/md/KED2023_supplements.html",
    "title": "The ABC of Computational Text Analysis",
    "section": "",
    "text": "Here I present some stuff that we did not cover in class."
  },
  {
    "objectID": "lectures/md/KED2023_supplements.html#tasks",
    "href": "lectures/md/KED2023_supplements.html#tasks",
    "title": "The ABC of Computational Text Analysis",
    "section": "Tasks",
    "text": "Tasks\n\nfind various ngrams with wildcards\ncheck gender specific language\n\nwhat follows she/he or her/his"
  },
  {
    "objectID": "lectures/md/KED2023_supplements.html#forms-of-data",
    "href": "lectures/md/KED2023_supplements.html#forms-of-data",
    "title": "The ABC of Computational Text Analysis",
    "section": "Forms of Data",
    "text": "Forms of Data\n\ncontent data\n\nclean, plain text data\npreferable as .txt\n\nmetadata ~ information about the actual data\n\npublishing date, authors, source, version\npreferable as .csv"
  },
  {
    "objectID": "lectures/md/KED2023_supplements.html#key-word-in-context-kwic",
    "href": "lectures/md/KED2023_supplements.html#key-word-in-context-kwic",
    "title": "The ABC of Computational Text Analysis",
    "section": "Key Word in Context (KWIC)",
    "text": "Key Word in Context (KWIC)\nptx -f -w 50 */*.txt > ptx.txt\negrep -i \"[a-z]  word\" ptx.txt"
  },
  {
    "objectID": "lectures/md/KED2023_supplements.html#select-column-in-dataset",
    "href": "lectures/md/KED2023_supplements.html#select-column-in-dataset",
    "title": "The ABC of Computational Text Analysis",
    "section": "Select Column in Dataset",
    "text": "Select Column in Dataset\ncut -d\\t -f1    # extract the 2nd column from a tab-separated file"
  },
  {
    "objectID": "lectures/md/KED2023_supplements.html#extract-texts-from-tsv",
    "href": "lectures/md/KED2023_supplements.html#extract-texts-from-tsv",
    "title": "The ABC of Computational Text Analysis",
    "section": "Extract texts from tsv:",
    "text": "Extract texts from tsv:\n\nhttp://www.theunixschool.com/2012/05/shell-read-text-or-csv-file-and-extract.html"
  },
  {
    "objectID": "lectures/md/KED2023_supplements.html#variables",
    "href": "lectures/md/KED2023_supplements.html#variables",
    "title": "The ABC of Computational Text Analysis",
    "section": "Variables",
    "text": "Variables\necho \"Starting program at $(date)\""
  },
  {
    "objectID": "lectures/md/KED2023_supplements.html#better-tokenization",
    "href": "lectures/md/KED2023_supplements.html#better-tokenization",
    "title": "The ABC of Computational Text Analysis",
    "section": "Better Tokenization",
    "text": "Better Tokenization\n\ntokenization ~ splitting into words\n\n# new, improved approach\ncat text.txt | tr -sc \"[a-zäöüA-ZÄÖÜ0-9-]\" \"\\n\"\n\n# old approach\ncat text.txt | tr ' ' '\\n'  \n\n\nTokenisierung = in Wörter splitten\nInterpunktion “klebt nicht mehr an Wörtern”\n-s = beliebig viele Zeichen\n-c = Komplement (also nicht diese Zeichen)\nangegebene Zeichen werden NICHT ersetzt"
  },
  {
    "objectID": "lectures/md/KED2023_supplements.html#batch-processing",
    "href": "lectures/md/KED2023_supplements.html#batch-processing",
    "title": "The ABC of Computational Text Analysis",
    "section": "Batch Processing",
    "text": "Batch Processing\nfor file in *.txt; do           # loop over all text files\n cat \"$file\" | pipe commands > \"proc_$file\"\ndone"
  },
  {
    "objectID": "lectures/md/KED2023_supplements.html#batch-renaming",
    "href": "lectures/md/KED2023_supplements.html#batch-renaming",
    "title": "The ABC of Computational Text Analysis",
    "section": "Batch Renaming",
    "text": "Batch Renaming\nrename  \" \" \"_\" *.txt   # replace spaces with underscores\n# since there are different versions, if this doesn't work try:\n# rename 's/ /_/' *.txt\ni=1\nfor file in *.txt; do           # loop over all text files\n mv -- \"$file\" \"text_$i.txt\"    # rename each file with a sequential number\n i=$((i+1))\ndone"
  },
  {
    "objectID": "lectures/md/KED2023_supplements.html#imperfect-data-a-tail-of-bias",
    "href": "lectures/md/KED2023_supplements.html#imperfect-data-a-tail-of-bias",
    "title": "The ABC of Computational Text Analysis",
    "section": "Imperfect Data: A Tail of Bias",
    "text": "Imperfect Data: A Tail of Bias\n\nsocial bias\n\nview from somewhere, stereotypes\n\ndata/archive holes\n\nlost, uncollected\n\ncorpus curation\n\nsupposition that key-word indicates topic\n\nnoise in data\n\nOCR errors, inconsistent spelling, non-content\n\n\n\n:point_right: think about the data and mitigate issues\n\n\nfehlende, rauschende, selektive & verzerrte Daten\nsozialer Kontext\n\nz.B. Budgetkürzung oder Neuausrichtung –> Wegfall von Thema\nSicht von weisen Männern auf Thema\n\nnon-content elements\n\nMetadaten, Kopfzeilen etc."
  },
  {
    "objectID": "lectures/md/KED2023_supplements.html#outlook-nlp-is-on-fire",
    "href": "lectures/md/KED2023_supplements.html#outlook-nlp-is-on-fire",
    "title": "The ABC of Computational Text Analysis",
    "section": "Outlook: NLP is on Fire 🔥",
    "text": "Outlook: NLP is on Fire 🔥\n\nsupervised machine learning\nyou can do basically anything with modern NLP\n\ntrain on human-annotated data\n\neffort, insights and quality may differ\n\nfor better or worse\n\n\n\n\ndie meisten haben ihr Schulwissen wieder vergessen, wieso kann das der Computer\nIntuition einfach, genaue technische Funktionsweise egal\n\nGenauigkeit wichtig, aber noch zu advanced\nbest-practice\n\nviel genauer dank Embeddings (self-supervised)"
  },
  {
    "objectID": "lectures/md/KED2023_supplements.html#mind-your-data",
    "href": "lectures/md/KED2023_supplements.html#mind-your-data",
    "title": "The ABC of Computational Text Analysis",
    "section": "Mind your Data",
    "text": "Mind your Data\n\nWho has a voice in your data?\n\nsocial context\n\nbigger is not necessarily better\n\nmore vs. more diverse data\n\nclean your data thoroughly\n\nnoisy vs. clean data"
  },
  {
    "objectID": "lectures/md/KED2023_supplements.html#anatomy-of-aicrawford2018-illustrated-by-the-amazon-echo",
    "href": "lectures/md/KED2023_supplements.html#anatomy-of-aicrawford2018-illustrated-by-the-amazon-echo",
    "title": "The ABC of Computational Text Analysis",
    "section": "Anatomy of AI[@Crawford2018] illustrated by the Amazon Echo",
    "text": "Anatomy of AI[@Crawford2018] illustrated by the Amazon Echo\n\n\nEs geht um mehr als nur Technologie\nTechnologie ist eingewoben ins Soziale\nSoziotechnische Systeme"
  },
  {
    "objectID": "lectures/md/KED2023_supplements.html#grid-example",
    "href": "lectures/md/KED2023_supplements.html#grid-example",
    "title": "The ABC of Computational Text Analysis",
    "section": "Grid Example",
    "text": "Grid Example\n\n:::\nCOL 1\n\ntext processing\n\n:::\n:::\nCOL 2\n\nexisting resources\ncreating new resources\n\n:::"
  },
  {
    "objectID": "lectures/md/KED2023_12.html",
    "href": "lectures/md/KED2023_12.html",
    "title": "The ABC of Computational Text Analysis",
    "section": "",
    "text": "presenting your projects :woman_technologist: :man_technologist::woman_technologist: :man_technologist::woman_technologist: :man_technologist:\ndiscussing your feedback"
  },
  {
    "objectID": "lectures/md/KED2023_12.html#project-presentations",
    "href": "lectures/md/KED2023_12.html#project-presentations",
    "title": "The ABC of Computational Text Analysis",
    "section": "Project Presentations",
    "text": "Project Presentations\n\n8 projects\n\n7-8 minutes per group\n+2 min feedback/questions everyone\n\npublish project on the course website if you like"
  },
  {
    "objectID": "lectures/md/KED2023_12.html#mini-projects",
    "href": "lectures/md/KED2023_12.html#mini-projects",
    "title": "The ABC of Computational Text Analysis",
    "section": "Mini-Projects",
    "text": "Mini-Projects\n\n\n\n\n\n\n\n\nName\nResearch question\n\n\n\n\nStefanie Jakober, Lorena Fähndrich, Serafina Bründler, Maria Krack\nAnalysis of Migros Luzern’ annual reports (2010-2020) on the shift of focus towards more sustainability-related topics and words\n\n\nValentina Meyer, Dario Haab, Nils Brun\nGeschlechter-Analyse von 2 Parteien anhand deren Parteiprogrammen\n\n\nSelina Seiler, Fiona Dudé, Selina Buser\nSpeeches from Olaf Scholz before and after being Kanzler\n\n\nJakob Henkel, Felix Sigrist, Livio Lombardo\nSpeeches vergleichen\n\n\nRuben Oliveira, Sidney Kämpfer\nCompare most used words in presidental speeches in the US across time and parties\n\n\nAnja Zürcher, Arianna Cambianica, Franciska Coric, Jasmin Wyss\nComparison of the mission statements of some Swiss banks\n\n\nCedric Bähler, Natalie Wüst, Leandra Ferrario, Valerio Moreno\nComparison of annual reports by Glencore\n\n\nKlara Förster, Tamara Gander, Lorena Graf, Lara Stiz\nSprachvergleich einer konservativen Partei, damals vs. heute"
  },
  {
    "objectID": "lectures/md/KED2023_12.html#the-stage-is-yours",
    "href": "lectures/md/KED2023_12.html#the-stage-is-yours",
    "title": "The ABC of Computational Text Analysis",
    "section": "The stage is yours…",
    "text": "The stage is yours…\n\nFeedback Fokus für Projekte\n\nMotivation: What did you explore?\nMethods: What data and methods did you use?\nDiscussion: What did you find out?\nBestimmung relevanter Wörter data-driven?\ninhaltlicher Fokus\nArten Preprocessing (Kopfzeilen, Zahlen)\nWie Ressource gefunden?\nProbleme/Überraschungen\nMöglichkeiten für weitergehende Untersuchungen"
  },
  {
    "objectID": "lectures/md/KED2023_12.html#i-hear-you",
    "href": "lectures/md/KED2023_12.html#i-hear-you",
    "title": "The ABC of Computational Text Analysis",
    "section": "I hear you …",
    "text": "I hear you …\n\n\n\n:thumbsup:\n\nknowledgeable + supportive\nwell-organized seminar\nhands-on experience\n\n\n\n\n:thumbsdown:\n\ngood exercises, yet to little time and no solutions\nlacking use case for Bash\nexplanations not always clear\ntoo fast and ambitious\n\n\n\n\n\nThank you!\n\n\nKurs zum 3ten Mal durchgeführt, erstes Mal vor Ort\n\nanspruchsvoll, aber lehrreich\nFordern bedingt auch Fördern\n\npositiv: eigentlicher Aufbau und Unterstützung\nnegativ: Erklärung nicht immer klar und fehlende Beispiele\nGrundproblematik\n\nwie viel in Kurs packen\nungeplante technische Schwierigkeiten (insb. Windows)\n\nPersönlisches Ziel: Rhetorik und Erklärungen verbessern\nTutorat wäre angebracht, aber keine Leute"
  },
  {
    "objectID": "lectures/md/KED2023_12.html#why-the-heck-did-we-learn-all-this-exploding_head",
    "href": "lectures/md/KED2023_12.html#why-the-heck-did-we-learn-all-this-exploding_head",
    "title": "The ABC of Computational Text Analysis",
    "section": "Why the heck did we learn all this? :exploding_head:",
    "text": "Why the heck did we learn all this? :exploding_head:\n\nText is the most abundant form of data. We want to analyze that.\nAI Information technology is transforming our society. We need to understand (to criticize)."
  },
  {
    "objectID": "lectures/md/KED2023_12.html#ensure-up-to-date-materials",
    "href": "lectures/md/KED2023_12.html#ensure-up-to-date-materials",
    "title": "The ABC of Computational Text Analysis",
    "section": "Ensure up-to-date Materials",
    "text": "Ensure up-to-date Materials\nupdate with git pull"
  },
  {
    "objectID": "lectures/md/KED2023_12.html#nice-work-woman_juggling-man_juggling",
    "href": "lectures/md/KED2023_12.html#nice-work-woman_juggling-man_juggling",
    "title": "The ABC of Computational Text Analysis",
    "section": "Nice Work :woman_juggling: :man_juggling:",
    "text": "Nice Work :woman_juggling: :man_juggling:"
  },
  {
    "objectID": "lectures/md/KED2023_12.html#crossed_fingers-it-is-getting-easier-from-here-on-relax-first-though-swimmer",
    "href": "lectures/md/KED2023_12.html#crossed_fingers-it-is-getting-easier-from-here-on-relax-first-though-swimmer",
    "title": "The ABC of Computational Text Analysis",
    "section": ":crossed_fingers:  It is getting easier from here on,  relax first though! :swimmer:",
    "text": ":crossed_fingers:  It is getting easier from here on,  relax first though! :swimmer:"
  },
  {
    "objectID": "lectures/md/KED2023_12.html#good-bye",
    "href": "lectures/md/KED2023_12.html#good-bye",
    "title": "The ABC of Computational Text Analysis",
    "section": "Good Bye…",
    "text": "Good Bye…"
  },
  {
    "objectID": "lectures/md/KED2023_03.html",
    "href": "lectures/md/KED2023_03.html",
    "title": "The ABC of Computational Text Analysis",
    "section": "",
    "text": "research workflow :bookmark_tabs:\ninteracting with computers :magic_wand:\ninstallation of programs :hammer:\n\n\n\nExkurs: wieso braucht es das ganze Tooling?\n\nWissenschaft als Praxis\n\nGrundverständis für Mensch-Maschine-Interaktion\nInstallation\n\nabschliessen/Fehler beheben\nZeitplan für Installation schwierig abzuschätzen\nFeierabend wenn System läuft"
  },
  {
    "objectID": "lectures/md/KED2023_03.html#recap-last-lecture",
    "href": "lectures/md/KED2023_03.html#recap-last-lecture",
    "title": "The ABC of Computational Text Analysis",
    "section": "Recap last Lecture",
    "text": "Recap last Lecture\n\ntextual data is challenging\nmachine-readable data\n\n\n\nBedeutung ist kontextabhängig\nUnvergleichbarkeit diskreter Symbole\nZiel: Abstraktion + Kontextualität kombinieren\n\nQuali + Quanti\nGeneralisierung + Rekontextualisierung\n\nFragen\n\nwichtigste Dateiformate?\n\ntxt, csv, tsv (xml)\n\nSinn von Texteditor?\n\nkeine Formatierung"
  },
  {
    "objectID": "lectures/md/KED2023_03.html#research-means-organizing",
    "href": "lectures/md/KED2023_03.html#research-means-organizing",
    "title": "The ABC of Computational Text Analysis",
    "section": "Research means Organizing",
    "text": "Research means Organizing\n\nHow a computational approach helps\n\nscripts as documentation :memo:\nefficient automation :recycle:\n\n“don’t repeat yourself”\n\nless error-prone :woman_facepalming: :man_facepalming:\nreproducible :100:\n\n\n\nWissenschaft auch praktische Seite, nicht nur theoretische\n\nchaotisch statt strukturiert (aufgeräumte Papers kommen erst ganz am Ende)\nOrganisation von Komplexität als Problem\n\nReproduzierbarkeit ist Arbeitsgrundlage und höchster Wert (mehr noch als Wahrheit)\n\nWiederverwendung von Code/Datensatz + Literatur/Theorien\n\nWieso computational approach?\n\nReproduzierbarkeit und Kritisierbarkeit garantiert\nAutomatisierung von repetitiven Aufgaben -> spart Zeit\n\nCode strukturiert/dokumentiert Arbeitsablauf/Entscheidungen\n\ngegen Vergessenheit\nschützt nicht vor Fehler, aber sichert inkrementellen Fortschritt\n\nwichtig für grössere Projekte + Abschlussarbeiten\nkurzfristig langsamer, langfristig effizienter\n\nkeine mühsame Fehlersuche, Wiederholung\n\n\nimage-src"
  },
  {
    "objectID": "lectures/md/KED2023_03.html#organizing-literature",
    "href": "lectures/md/KED2023_03.html#organizing-literature",
    "title": "The ABC of Computational Text Analysis",
    "section": "Organizing Literature",
    "text": "Organizing Literature\n\n\n\n\nmanage literature in one place\ngenerating bibliographies\ncollect with a click in your browser\n\n\n\n\n\nZotero is a free, open-source software for managing scientific literature\n\n\n\n\n\n\nOrganisation betrifft auch Literatur\nFragen, wer Literaturverwaltungssystem nutzt\n\nwas? wieso nützlich?\n\nZotero\n\nopen-source, viele Features, konstante Weiterentwicklung\n\nNutzen\n\nverschiedene Zitationsstile\neinmal indexiert, immer gleich\nRecherche: Metadaten automatisch setzen\nBibliographie automatisch zusammenstellen"
  },
  {
    "objectID": "lectures/md/KED2023_03.html#two-trends-in-computing",
    "href": "lectures/md/KED2023_03.html#two-trends-in-computing",
    "title": "The ABC of Computational Text Analysis",
    "section": "Two Trends in Computing",
    "text": "Two Trends in Computing\n\n\n\neveryday, mobile, cloud-centered, touch/click-based approach\n\nsingle-purpose application\nsmartphones + tablets\n\nengineering, open-source, machine-oriented approach\n\npowerful due to modularity\nworkflow for data science\n\n\n[see also @Healy2019]\n\n\n\n\nApple Original iPhone (2007)\n\n\n\n\n\n\nComputer haben sich massiv verändert -> am meisten an Oberfläche\nSpannungsfeld zweier Trends\n\nEinfachheit –> Eingeschränktheit; nur machen, was vorgesehen ist\nFlexibilität –> technisches Vorwissen\n\nApple: GUI/Bedienung Angleichung an physische Welt\nEngineering: schwieriger Einstieg/Lernen, dafür sehr viel effektiver/vielseitiger\n\nist alternativlos für Standardisierung unstandardisierter Daten\nkeine Zauberei, aber sehr nützlich\n\nvon Wissen/Tools der Engineers profitieren\n\nnicht selber Tools bauen\ngeniale Arbeitsabläufe + Modularität"
  },
  {
    "objectID": "lectures/md/KED2023_03.html#operating-systems-os",
    "href": "lectures/md/KED2023_03.html#operating-systems-os",
    "title": "The ABC of Computational Text Analysis",
    "section": "Operating Systems (OS)",
    "text": "Operating Systems (OS)\n\nBetween hardware and programs\n\nMicrosoft Windows\nmacOS\nLinux-based systems :nerd_face:\n\n\n\nOS\nWindows = Dominator, Alltag\nLinux = stable, secure, free, innovative. Became more user-friendly, sometimes still issue\nMac = restricted to Mac HW, Unix-Derivat, vereint beide Welten\nBetriebssysteme wie Fahrräder oder Autos\nSysteme gleichen sich immer mehr an\n\nInstallation von Linux in Windows"
  },
  {
    "objectID": "lectures/md/KED2023_03.html#user-interfaces",
    "href": "lectures/md/KED2023_03.html#user-interfaces",
    "title": "The ABC of Computational Text Analysis",
    "section": "User Interfaces",
    "text": "User Interfaces\n\n\n\ngraphical user interface (GUI)\n\n\n\n\ncommand line interface (CLI)\n\n\n\n\n\n\nhistorisch nur CLI zur Computersteuerung\nGUI von Apple entwickelt 1984, in 90er Standard\nCLI ist mehr als Sentimentalität\n\nmächtiger dank Automatisierbarkeit\nschneller\nauf Server einzig mögliche Interaktion"
  },
  {
    "objectID": "lectures/md/KED2023_03.html#an-awesome-programmer-saves-the-world",
    "href": "lectures/md/KED2023_03.html#an-awesome-programmer-saves-the-world",
    "title": "The ABC of Computational Text Analysis",
    "section": "“An awesome Programmer saves the World”",
    "text": "“An awesome Programmer saves the World”\n\n\nCLI auch in Popkultur, allerdings falsch dargestellt"
  },
  {
    "objectID": "lectures/md/KED2023_03.html#human-machine-interaction",
    "href": "lectures/md/KED2023_03.html#human-machine-interaction",
    "title": "The ABC of Computational Text Analysis",
    "section": "Human-Machine Interaction",
    "text": "Human-Machine Interaction\n\nHow to tell the computer effienctly\n\ntext commands instead of clicks\nsimilar to human languages\n\nsyntax (form)\nsemantics (meaning)\n\ncomputers are literalists\n\n\n\nAutomatisierung über GUI nicht möglich\n\nstatt “hier” klicken, einfach Skript\n\nviele verschiedene Programmiersprachen\nSyntax sehr restriktiv\n\nComputer sind ziemlich doof, aber sehr gehorsam.\nMachen genau, was man ihnen sagt, nichts mehr, nichts weniger. Nie."
  },
  {
    "objectID": "lectures/md/KED2023_03.html#programming",
    "href": "lectures/md/KED2023_03.html#programming",
    "title": "The ABC of Computational Text Analysis",
    "section": "Programming",
    "text": "Programming\n\ncommand languages\n\nBourne-Again shell (Bash), since 1989\n\nprogramming languages\n\nPython, R, Java, C++ etc.\n\ninstructions → program ≡ algorithm\n\ncooking\n\n\n\n\nCLI primär für Dateimanipulation/Verarbeitung\nPython\n\neinfach: reduziert auf das elementare\nPython is a general-purpose language whereas R is a statistical programming language.\n\nSoftware heisst neuerdings Algorithmus\n\nfalsch: “Algo als Böses/Mystisches”\nkorrekt: schrittweise Umwandlung von Input zu Output\nsoftware = program + data (complement HW)"
  },
  {
    "objectID": "lectures/md/KED2023_03.html#package-manager",
    "href": "lectures/md/KED2023_03.html#package-manager",
    "title": "The ABC of Computational Text Analysis",
    "section": "Package Manager",
    "text": "Package Manager\n\nautomation of software installation\n\nsystem: apt, Homebrew\nPython: pip, conda\n\nsoftware dependencies\nsimilar to app stores\n\n\n\nSW baut auf weiterer SW auf\n\nkeine vollständigen Programme\nbei Installation hunderte von Kompatibilitäts-Checks\n\nzentrale Verwaltung installierter SW\n\nupdate aller Programme mit einem Befehl\n\n2 Manager: systemweit, Python\napp stores keine Innovation\ngehört auch zur Arbeitsorganisation"
  },
  {
    "objectID": "lectures/md/KED2023_03.html#open-source-is-a-mindset",
    "href": "lectures/md/KED2023_03.html#open-source-is-a-mindset",
    "title": "The ABC of Computational Text Analysis",
    "section": "Open-Source is a Mindset",
    "text": "Open-Source is a Mindset\n\nStanding on the shoulders of giants\n\nfree + open software\ncross-platform portability\ncollaboration\n\nshare + reuse\n\nhelpful community\n\n\n\nZusammen erreicht man mehr -> Abhängigkeiten\nAbhängigkeiten funktionieren am besten, wenn offen\n\nRückmeldungen -> gemeinsame Verbesserung\nschnelle Weiterentwicklung\n\nCS offenste Disziplin\n\nPrivate + Firmen\noft unentgeltlich"
  },
  {
    "objectID": "lectures/md/KED2023_03.html#resources-everyone-is-using",
    "href": "lectures/md/KED2023_03.html#resources-everyone-is-using",
    "title": "The ABC of Computational Text Analysis",
    "section": "Resources everyone is using",
    "text": "Resources everyone is using\n\nStack Overflow\n\nwhere you get answers to technical questions\n\nGitHub\n\nwhere you find open software\n\n\n\n\nKollaboration nicht nur für SW, auch für Fragenbeantwortung\nallermeiste Fragen/Probleme nicht neu, schon beantwortet\n\nbash commands auf stackoverflow\ninstallationsprobleme\n\nGithub\n\nsource code + anleitung für Millionen von Programmen (klein und gross)\nrepository"
  },
  {
    "objectID": "lectures/md/KED2023_03.html#learning-by-doing-doing-by-googleing.-woman_cartwheelingman_cartwheeling",
    "href": "lectures/md/KED2023_03.html#learning-by-doing-doing-by-googleing.-woman_cartwheelingman_cartwheeling",
    "title": "The ABC of Computational Text Analysis",
    "section": "Learning by doing, doing by Googleing. :woman_cartwheeling::man_cartwheeling:",
    "text": "Learning by doing, doing by Googleing. :woman_cartwheeling::man_cartwheeling:\n\n\nTechnical problems are normal + solutions around the corner\n\nFehlermeldung lesen + googlen\n\nWithout the internet, you are a nobody\nInstallation is sometimes harder and much more poorly documented than mere usage"
  },
  {
    "objectID": "lectures/md/KED2023_03.html#backup-japanese_ogre",
    "href": "lectures/md/KED2023_03.html#backup-japanese_ogre",
    "title": "The ABC of Computational Text Analysis",
    "section": "Backup :japanese_ogre:",
    "text": "Backup :japanese_ogre:\nYou don’t need it until you desperately need it!"
  },
  {
    "objectID": "lectures/md/KED2023_03.html#setting-up-your-development-environment",
    "href": "lectures/md/KED2023_03.html#setting-up-your-development-environment",
    "title": "The ABC of Computational Text Analysis",
    "section": "Setting up your Development Environment",
    "text": "Setting up your Development Environment\n\nInstallation\n\nPython 3.8\n\npackages for NLP + visualization\n\nTesseract\nvarious Bash tools\nVS Code Editor\n\n:point_right: Follow the installation guide for your OS.\n\n\nverschiedene Installationsmöglichkeiten\n\nIdee: plattformübergreifend, relativ einfach, uneingeschränkt\n\nWindows Leute installieren Ubuntu für Bash\nInstallations Guide folgen\n\nVerstehen aktuell egal\nFragen/Verbesserungsvorschläge willkommen\nAblauf verfolgen & auf Fehler achten"
  },
  {
    "objectID": "lectures/md/KED2023_03.html#vs-code-editor",
    "href": "lectures/md/KED2023_03.html#vs-code-editor",
    "title": "The ABC of Computational Text Analysis",
    "section": "VS Code Editor",
    "text": "VS Code Editor\n\n\n\nThe Microsoft Word for coding\n\npowerful integrated development environment (IDE)\ncross-platform\ninteractive analysis\n\n\n\n\n\n\nWrite your first Python script in VS Code\n\n\n\n\n\n\nin-class demonstrieren\nVisual Studio: Code, Erklärung, Output an selbem Ort"
  },
  {
    "objectID": "lectures/md/KED2023_03.html#first-steps-in-python",
    "href": "lectures/md/KED2023_03.html#first-steps-in-python",
    "title": "The ABC of Computational Text Analysis",
    "section": "First Steps in Python",
    "text": "First Steps in Python\nLearn Python with interactive tutorials\n\nPython Principles\nLearnPython\n\n\n\nPython individuell vertiefen"
  },
  {
    "objectID": "lectures/md/KED2023_03.html#readings",
    "href": "lectures/md/KED2023_03.html#readings",
    "title": "The ABC of Computational Text Analysis",
    "section": "Readings",
    "text": "Readings\n\noptional: pimp your workflow\nHealy, Kieran. 2019. “The Plain Person’s Guide to Plain Text Social Science.” online."
  },
  {
    "objectID": "lectures/md/KED2023_03.html#references",
    "href": "lectures/md/KED2023_03.html#references",
    "title": "The ABC of Computational Text Analysis",
    "section": "References",
    "text": "References"
  },
  {
    "objectID": "lectures/md/KED2023_06.html",
    "href": "lectures/md/KED2023_06.html",
    "title": "The ABC of Computational Text Analysis",
    "section": "",
    "text": "well-solved assignment #1 :confetti_ball:\n\nexample solution\n\ncounting words :1234:\n\nparticular words or entire vocabulary\n\npreprocessing ​and cleaning :soap:\n\n\n\nÜbung\n\nFragen?\nBearbeitungszeit unterschiedlich (1.5h - 8h)\nBeispiellösungen aufgeschaltet, andere Wege möglich\nungewollte Hürde: locate nicht standardmässig installiert\n\nFrequenzanalysen\n\nÜbersicht gewinnen: Wo liegen Schwerpunkte?\nkomparative Vergleich über Zeit oder Akteure (z.B. Partei)\nvergleichbar mit Ngram-Viewer"
  },
  {
    "objectID": "lectures/md/KED2023_06.html#outline",
    "href": "lectures/md/KED2023_06.html#outline",
    "title": "The ABC of Computational Text Analysis",
    "section": "Outline",
    "text": "Outline\n\nintroducing regular expression :sparkles:\npracticing the writing of patterns :roller_coaster:\n\n\n\nHalbzeit von Semester, langsam gehts ans Eingemachte\nSimpler Plan, RegEx allerdings mühsam\nuralt, aber unumgänglich für Data Cleaning\nje nach Zeit, nächstes Mal nochmals RegEx + Übungszeit"
  },
  {
    "objectID": "lectures/md/KED2023_06.html#formal-search-patterns",
    "href": "lectures/md/KED2023_06.html#formal-search-patterns",
    "title": "The ABC of Computational Text Analysis",
    "section": "Formal Search Patterns",
    "text": "Formal Search Patterns\n\nHow to extract all email addresses in a text collection?\nPlease contact us via info@organization.org.\n---\nFor specific questions ask Mrs. Green (a.green@mail.com).\n---\nReach out to support@me.ch\n\n:point_right: Solution: Write a single pattern to match any valid email adress\n[A-Z0-9._%+-]+@[A-Z0-9.-]+\\.[A-Z]{2,}   # match any email address (case-insensitive)\n\n\nWas meint Text als Pattern?\n\nam einfachsten an Problemstellung zu sehen\nEmail-Adressen sind immer nach dem gleichen Muster aufgebaut\nganze Sprache ist voller Muster –> Grammatik\n\nFrage an Studis: Wie macht ihr das?\n\nBsp. Marketing-Analyse oder Wistleblower Korpus\n\nallen bekannt: Suche in Text\n\nSuche nach @ findet alle Adressen\nwie aber extrahieren und welche Teile gehören genau dazu?\n\nkryptisch + hässlich, aber beliebig expressive Beschreibungssprache"
  },
  {
    "objectID": "lectures/md/KED2023_06.html#what-are-patterns-for",
    "href": "lectures/md/KED2023_06.html#what-are-patterns-for",
    "title": "The ABC of Computational Text Analysis",
    "section": "What are patterns for?",
    "text": "What are patterns for?\n\nfinding :mag_right:\nextracting :hammer_and_wrench:\nremoving/cleaning :wastebasket:\nreplacing :repeat:\n\n… specific parts in texts\n\n\nRegEx mit breiter Anwendung\n\nfür Preprocessing Textanalysen unverzichtbar\nData Cleaning\n\nfunktioniert genau gleich in Python, R und anderen Programmiersprachen"
  },
  {
    "objectID": "lectures/md/KED2023_06.html#data-cleaning-is-paramount",
    "href": "lectures/md/KED2023_06.html#data-cleaning-is-paramount",
    "title": "The ABC of Computational Text Analysis",
    "section": "Data Cleaning is paramount!",
    "text": "Data Cleaning is paramount!\n\n\n\nAufbereitung braucht viel Zeit\neinfaches Modell mit ein paar Zeilen Code, Bereinigung immer spezifisch für Datenquelle\n\nsrc"
  },
  {
    "objectID": "lectures/md/KED2023_06.html#what-are-regular-expressions-regex",
    "href": "lectures/md/KED2023_06.html#what-are-regular-expressions-regex",
    "title": "The ABC of Computational Text Analysis",
    "section": "What are Regular Expressions (RegEx)?",
    "text": "What are Regular Expressions (RegEx)?\n\nRegEx builds on two classes of symbols\n\nliteral characters and strings\n\nletters, digits, words, phrases, dates etc.\n\nmeta expressions with special meaning\n\ne.g., \\w represents alphanumeric characters\n[Cc]o+l → Col, col, Cool, coool …\n\nakin to regular languages\n\n\n\nRegex = Muster = generalisierende Beschreibung\nErklären von String = Zeichensequenz\nzwei Arten von Zeichen\nLiterale = Zeichen steht für tatsächliches Zeichen (buchstabentreue Repräsentation)\n\nwie letztes Mal\n\nMeta-Zeichen = Zeichen mit spezieller Bedeutung\n\nanfänglich verwirrend\nThema heutiger Sitzung\n\ngenaue mathematische Definition hier nicht Thema"
  },
  {
    "objectID": "lectures/md/KED2023_06.html#finding-extracting",
    "href": "lectures/md/KED2023_06.html#finding-extracting",
    "title": "The ABC of Computational Text Analysis",
    "section": "Finding + Extracting",
    "text": "Finding + Extracting\n\nextended globally search for regular expression and print (egrep)\n\ntool to filter/keep matching lines only\n\n# check a regular expression quickly\necho \"check this pattern\" | egrep \"pattern\" \n\negrep \"yes\" file.txt        # search in a specific file\negrep -r \"yes\" folder       # search recursively within folder\n\negrep \"yes\" *.txt           # keep lines containing pattern (yes) across txt-files\negrep -i \"yes\" *.txt        # dito, ignore casing (Yes, yes, YES ...)\negrep -v \"noisy\" *.txt      # do NOT keep lines containing noisy\n\n# extract raw match only to allow for subsequent counting\negrep -o \"only\" *.txt       # print match only instead of entire line\negrep -h \"only\" *.txt       # suppress file name\n\n\nEmpfehlung: egrep benutzen statt grep"
  },
  {
    "objectID": "lectures/md/KED2023_06.html#quantifiers",
    "href": "lectures/md/KED2023_06.html#quantifiers",
    "title": "The ABC of Computational Text Analysis",
    "section": "Quantifiers",
    "text": "Quantifiers\n\nrepeat preceding character X times\n\n? zero or one\n+ one or more\n* zero or any number\n{n}, {m,n} a specified number of times\n\negrep -r \"Bundesrath?es\"        # match old and new spelling\negrep -r \"a+\"                   # match one or more \"a\"\negrep -r \"e{2}\"                 # match sequence of two \"e\"\n:warning: Do not confuse regex with Bash wildcards!\n\n\nerste Klasse von Meta-Symbolen: Quantifikatoren\ndefinieren Anzahl von vorangehendem Zeichen\nin Regex beziehen sich Operatoren auf vorderes Zeichen, in Wildcard nicht"
  },
  {
    "objectID": "lectures/md/KED2023_06.html#character-sets",
    "href": "lectures/md/KED2023_06.html#character-sets",
    "title": "The ABC of Computational Text Analysis",
    "section": "Character Sets",
    "text": "Character Sets\n\n[...] any of the characters between brackets\n\nany vowel: [auoei]\nany digit: [0-9]\nany letter: [A-Za-z]\n\n[^...] any character but none of these (negation)\n\nanything but the vowels: [^auoei]\n\n\n# match the capitalized and non-capitalized form\negrep -r \"[Gg]rüne\"\n\n# match sequences of 3 vowels\negrep -r [aeiou]{3}\n\n# extract all bigrams (sequence of two words)\negrep -rohi \"[a-z]+ [a-z]+\""
  },
  {
    "objectID": "lectures/md/KED2023_06.html#special-symbols",
    "href": "lectures/md/KED2023_06.html#special-symbols",
    "title": "The ABC of Computational Text Analysis",
    "section": "Special Symbols",
    "text": "Special Symbols\n\n. matches any character (excl. newline)\n\\ escapes to match literal\n\n\\. means the literal . instead of “any symbol”\n\n\\w matches any alpha-numeric character\n\nsame as [A-Za-z0-9_]\n\n\\s matches any whitespace (space, newline, tab)\n\nsame as [ \\t\\n]\n\n\n# match anything between brackets\negrep -r \"\\(.*\\)\"\n\n\nKlammern sind auch Metasymbole"
  },
  {
    "objectID": "lectures/md/KED2023_06.html#the-power-of-.",
    "href": "lectures/md/KED2023_06.html#the-power-of-.",
    "title": "The ABC of Computational Text Analysis",
    "section": "The power of .* …",
    "text": "The power of .* …\nmatches any character any times"
  },
  {
    "objectID": "lectures/md/KED2023_06.html#more-complex-examples",
    "href": "lectures/md/KED2023_06.html#more-complex-examples",
    "title": "The ABC of Computational Text Analysis",
    "section": "More Complex Examples",
    "text": "More Complex Examples\n# extract basename of URLs\negrep -ro \"www\\.\\w+\\.[a-z]{2,}\"\n\n# extract valid email adresses (case-insensitive)\negrep -iro \"[A-Z0-9._%+-]+@[A-Z0-9.-]+\\.[A-Z]{2,}\" **/*.txt\n\n\nbei Erstellung von Online-Accounts prüfen RegEx Validität von Email"
  },
  {
    "objectID": "lectures/md/KED2023_06.html#combining-regex-with-frequency-analysis",
    "href": "lectures/md/KED2023_06.html#combining-regex-with-frequency-analysis",
    "title": "The ABC of Computational Text Analysis",
    "section": "Combining RegEx with Frequency Analysis",
    "text": "Combining RegEx with Frequency Analysis\n\nsomething actually useful\n# count political areas by looking up words ending with \"politik\"\negrep -rioh \"\\w*politik\" **/*.txt | sort | uniq -c | sort -h\n\n# count ideologies/concepts by looking up words ending with \"ismus\"\negrep -rioh \"\\w*ismus\" **/*.txt | sort | uniq -c | sort -h\n\n\nbis jetzt Spielerei, um RegEx zu lernen\nGrundlage für Seminararbeit\n\nsystematisches Suchen, quantifizieren und analysieren von Begriffsverwendung"
  },
  {
    "objectID": "lectures/md/KED2023_06.html#start-simple-add-complexity-subsequently.",
    "href": "lectures/md/KED2023_06.html#start-simple-add-complexity-subsequently.",
    "title": "The ABC of Computational Text Analysis",
    "section": "Start simple, add complexity subsequently.",
    "text": "Start simple, add complexity subsequently."
  },
  {
    "objectID": "lectures/md/KED2023_06.html#in-class-exercise",
    "href": "lectures/md/KED2023_06.html#in-class-exercise",
    "title": "The ABC of Computational Text Analysis",
    "section": "In-class: Exercise",
    "text": "In-class: Exercise\n\nUse the command line to navigate to the local copy of the Github repository KED2023 and make sure it is up-to-date with git pull. Change in to the directory materials/data/swiss_party_programmes/txt.\nUse egrep to extract all uppercased words like UNO, OECD, SP and count their frequency.\nUse egrep to extract all plural nouns with female endings e.g. Schweizerinnen (starting with an uppercase letter, ending with innen, and any letter in between). Do the same for the male forms. Is there a qualitative or a quantitative difference between the gendered forms?\n\n# Some not so random hints \npiping with |\nsort\nuniq -c\negrep -roh **/*.txt\n\n\negrep -roh \"[A-Z]{2,}\" **/*.txt | sort | uniq -c | sort -h\nPause\nCTRL+C um Befehl abzubrechen (falls länger als eine Sekunde dauert, ist etwas falsch)\nStart mit einfachem grep-Befehl, dann schauen, was gematcht wird und dann auszählen"
  },
  {
    "objectID": "lectures/md/KED2023_06.html#in-class-solution",
    "href": "lectures/md/KED2023_06.html#in-class-solution",
    "title": "The ABC of Computational Text Analysis",
    "section": "In-class: Solution",
    "text": "In-class: Solution\n\nUse egrep to extract all uppercased words like UNO, OECD, SP and count their frequency.\n\negrep -roh \"[A-Z]{2,}\" **/*.txt | sort | uniq -c | sort -h\n\nUse egrep to extract all plural nouns with female endings e.g. Schweizerinnen (starting with an uppercase letter, ending with innen, and any letter in between). Do the same for the male forms. Is there a qualitative or a quantitative difference between the gendered forms?\n\negrep -roh \"[A-Z][a-z]+innen\\b\" **/*.txt | sort | uniq -c | sort -h\negrep -roh \"[A-Z][a-z]+er\\b\" **/*.txt | sort | uniq -c | sort -h (there is no way with regular expression to extract only nouns of the male form but not Wasser and the like. For this, you have to use some kind of machine learning.)"
  },
  {
    "objectID": "lectures/md/KED2023_06.html#replacing-removing",
    "href": "lectures/md/KED2023_06.html#replacing-removing",
    "title": "The ABC of Computational Text Analysis",
    "section": "Replacing + Removing",
    "text": "Replacing + Removing\n\nstream editor (sed)\n\nadvanced find + replace using regex\n\nsed \"s/WHAT/WITH/g\" file.txt\n\nsed replaces any sequence, tr only single symbols\n\necho \"hello\" | sed \"s/llo/y/g\"      # replace \"llo\" with a \"y\"\n\n# by setting the g flag in \"s/llo/y/g\",\n# sed replaces all occurences, not only the first one\n\n\negrep für Extraktion, sed für Manipulation\n\nwichtig um Daten aufzubereiten\n\nwie Suchen-Ersetzen-Funktion von Word, nur mächtiger dank Regex\nLöschen = Ersetzen mit leeren Sequenz\nflag “global”\nDemonstration mit \necho \"hello hell\" | sed \"s/l\\b/lo/g\""
  },
  {
    "objectID": "lectures/md/KED2023_06.html#contextual-replacing",
    "href": "lectures/md/KED2023_06.html#contextual-replacing",
    "title": "The ABC of Computational Text Analysis",
    "section": "Contextual Replacing",
    "text": "Contextual Replacing\n\nreuse match with grouping\n\ndefine a group with parentheses (group_pattern)\n\\1 equals the expression inside first pair of parentheses\n\\2 expression of second pair\n…\n\n# swap order of name (last first -> first last)\necho \"Lastname Firstname\" | sed -E \"s/(.+) (.+)/\\2 \\1/\"\n\n# matching also supports grouping\n# match any pair of two identical digits\negrep -r \"([0-9])\\1\"\n\n\nTeilausdruck gruppieren zur Wiederverwendung\nKlammern sind ebenfalls Metazeichen"
  },
  {
    "objectID": "lectures/md/KED2023_06.html#more-meta-symbols",
    "href": "lectures/md/KED2023_06.html#more-meta-symbols",
    "title": "The ABC of Computational Text Analysis",
    "section": "More Meta-Symbols",
    "text": "More Meta-Symbols\n\n\\b matches word boundary\n\nword\\b does not match words\n\n^ matches begin of line and $ end of line\n\n^A matches only A at line start\n\n| is a disjunction (OR)\n\n(Mr|Mrs|Mr\\.|Mrs\\.) Green matches alternatives\n\n\n\n\ndiese Symbole sind leer, sie matchen keine Zeichen\nspezifizieren Positon von regulärem Ausdruck\nline start hilfreich für übung"
  },
  {
    "objectID": "lectures/md/KED2023_06.html#greediness-trap",
    "href": "lectures/md/KED2023_06.html#greediness-trap",
    "title": "The ABC of Computational Text Analysis",
    "section": "Greediness Trap",
    "text": "Greediness Trap\n\ngreedy ~ match the longest string possible\nquantifiers * or + are greedy\nnon-greedy by excluding some symbols\n\n[^EXCLUDE_SYMBOLS] instead of .*\n\n\n# greedy: an apple, other apple\necho \"an apple, other apple\" | egrep \"a.*apple\"\n\n# non-greedy: an apple\necho \"an apple, other apple\" | egrep \"a[^,]*apple\"\n\n\n.* = jegliche Zeichen, beliebige Länge"
  },
  {
    "objectID": "lectures/md/KED2023_06.html#assignment-2-writing_hand",
    "href": "lectures/md/KED2023_06.html#assignment-2-writing_hand",
    "title": "The ABC of Computational Text Analysis",
    "section": "Assignment #2 :writing_hand:",
    "text": "Assignment #2 :writing_hand:\n\nget/submit via OLAT\n\nstarting tomorrow\ndeadline 15 April 2023, 23:59\n\nuse forum on OLAT\n\nsubscribe to get notifications\n\nask friends for support, not solutions"
  },
  {
    "objectID": "lectures/md/KED2023_06.html#in-class-exercises-i",
    "href": "lectures/md/KED2023_06.html#in-class-exercises-i",
    "title": "The ABC of Computational Text Analysis",
    "section": "In-class: Exercises I",
    "text": "In-class: Exercises I\n\nUse egrep to extract capitalized words and count them. What are the most frequent nouns?\nUse egrep to extract words following any of these strings: der die das. Hint: Use a disjunction.\nDo the self-check on the next slide.\nUse sed -E to remove the table of content, the footer and the page number in the programme of the Green Party. Check the corresponding PDF to get a visual impression and test your regular expression with egrep first to see if you match the correct parts in the document."
  },
  {
    "objectID": "lectures/md/KED2023_06.html#in-class-solution-i",
    "href": "lectures/md/KED2023_06.html#in-class-solution-i",
    "title": "The ABC of Computational Text Analysis",
    "section": "In-class: Solution I",
    "text": "In-class: Solution I\n\nUse egrep to extract capitalized words and count them. What are the most frequent nouns?\n\negrep -roh \"[A-Z][a-z]+\" **/*.txt | sort | uniq -c | sort -h\n\nUse egrep to extract words following any of these strings: der die das. Hint: Use a disjunction.\n\negrep -roh \"(der|die|das) \\w+\" **/*.txt\n\nUse sed -E to remove the table of content, the footer and the page number in the programme of the Green Party. Check the corresponding PDF to get a visual impression and test your regular expression with egrep first to see if you match the correct parts in the document.\n\ncat gruene_programme_2019.txt | sed \"1,192d\" | sed -E \"s/^Wahlplattform.*2023$//g\" | sed -E \"s/^[0-9]+$//g\""
  },
  {
    "objectID": "lectures/md/KED2023_06.html#in-class-self-check",
    "href": "lectures/md/KED2023_06.html#in-class-self-check",
    "title": "The ABC of Computational Text Analysis",
    "section": "In-class: Self-Check",
    "text": "In-class: Self-Check\n\nequivalent patterns\na+ == aa*               # \"a\" once or more than once\na? == (a|_)             # \"a\" once or nothing\na{3} == aaa             # three \"a\"\na{2,3} == (aa|aaa)      # two or three \"a\"\n[ab] == (a|b)           # \"a\" or \"b\"\n[0-9] == (0|1|2|3|4|5|6|7|8|9)  #any digit"
  },
  {
    "objectID": "lectures/md/KED2023_06.html#in-class-exercise-ii",
    "href": "lectures/md/KED2023_06.html#in-class-exercise-ii",
    "title": "The ABC of Computational Text Analysis",
    "section": "In-class: Exercise II",
    "text": "In-class: Exercise II\n\nCount all the bigrams (sequence of two words) using character sets and quantifiers. What about trigrams (three words)?\nExtract the words following numbers (also consider numbers like: 1'000, 1,000 or 5%). Then, count all the words while excluding the numbers themselves. Hint: Pipe another grep to remove the digits.\nYou are ready to come up with your own patterns…"
  },
  {
    "objectID": "lectures/md/KED2023_06.html#in-class-solution-ii",
    "href": "lectures/md/KED2023_06.html#in-class-solution-ii",
    "title": "The ABC of Computational Text Analysis",
    "section": "In-class: Solution II",
    "text": "In-class: Solution II\n\nCount all the bigrams (sequence of two words) using character sets and quantifiers. What about trigrams (three words)?\n\negrep -hoir \"\\b[a-z]+ [a-z]+\\b\" | sort | uniq -c | sort -h\negrep -hoir \"\\b[a-z]+ [a-z]+ [a-z]+\\b\" | sort | uniq -c | sort -h\n\nExtract the words following numbers (also consider numbers like: 1'000, 1,000 or 5%). Then, count all the words while excluding the numbers themselves. Hint: Pipe another grep to remove the digits.\n\negrep -rhoi \"[0-9][0-9,'%]+ [a-z]+\" | egrep -io \"[a-z]+\" | sort | uniq -c | sort -h\nAlternative: egrep -rhoi \"[0-9][0-9,'%]+ [a-z]+\" | sed -E \"s/[0-9][0-9,'%]+//g\" | sort | uniq -c | sort -h"
  },
  {
    "objectID": "lectures/md/KED2023_06.html#in-class-exercise-iii",
    "href": "lectures/md/KED2023_06.html#in-class-exercise-iii",
    "title": "The ABC of Computational Text Analysis",
    "section": "In-class: Exercise III",
    "text": "In-class: Exercise III\n\nSince you know about RegEx, we can use a more sophisticated tokenizer to split a text into words. What is the difference between the old and new approach? Test it and check the helper page with man.\n# new, improved approach\ncat text.txt | tr -sc \"[a-zäöüA-ZÄÖÜ0-9-]\" \"\\n\"\n\n# old approach\ncat text.txt | tr \" \" \"\\n\""
  },
  {
    "objectID": "lectures/md/KED2023_06.html#more-resources",
    "href": "lectures/md/KED2023_06.html#more-resources",
    "title": "The ABC of Computational Text Analysis",
    "section": "More Resources",
    "text": "More Resources\n\nrequired\n\nBen Schmidt. 2019. Regular Expressions.\nCheatsheet of this course\n\n\n\n\nhighly recommended\n\nNikolaj Lindberg. egrep for Linguists.\n\n\n\n\nonline regular expression editor\n\nregex101 to write and check patterns"
  },
  {
    "objectID": "lectures/md/KED2023_06.html#questions",
    "href": "lectures/md/KED2023_06.html#questions",
    "title": "The ABC of Computational Text Analysis",
    "section": "Questions?",
    "text": "Questions?"
  },
  {
    "objectID": "lectures/md/KED2023_08.html",
    "href": "lectures/md/KED2023_08.html",
    "title": "The ABC of Computational Text Analysis",
    "section": "",
    "text": "assignment 2 accomplished :white_check_mark:\nan abundance of data sources\n\nJSTOR, Nexis, few datasets\n\ncreating your own dataset\n\nconvert any data to .txt\n\nprocessing a batch of files\n\nperform tasks in for-loop\n\n\n\n\nAssignment\n\nposititv: gut gelöst, für manche Leute sehr viel einfacher, für manche schwieriger\nTradeoff: generalisierung vs. spezifizität\n\nKunst ist: so generell wie möglich, so spezifisch wie nötig\n\n\ninteressante Quellen für Sozialwissenschaften\n\nUnmenge intressanter Daten, wenig Datensätze\nKonversion, da Dokument nicht in maschinenlesbarer Form\n\ngerüstet um mit (fast) allen Textdaten zu arbeiten"
  },
  {
    "objectID": "lectures/md/KED2023_08.html#outline",
    "href": "lectures/md/KED2023_08.html#outline",
    "title": "The ABC of Computational Text Analysis",
    "section": "Outline",
    "text": "Outline\n\nethics is everywhere :see_no_evil::hear_no_evil::speak_no_evil:\n\n… and your responsibility\n\nunderstand the development of modern NLP :rocket:\n\n… or how to put words into computers\n\n\n\n\nheute ein Sprung, der über praktischen Teil von Seminar hinausgeht\n\nMix aus Ethik und Entwicklung NLP\nmoderne NLP leistungsfähiger als je zuvor, aber mit Problemen\n\nAI ist Werkzeug, erstmal weder gut noch schlecht\n\nVergleich: Motor für Krankenwagen oder Panzer (Verteidigung/Angriff).\nInternet für Wikipedia oder für Kinderpornographie\n\nDie Frage ist: Wer profitiert genau von was? Wer verliert dabei?"
  },
  {
    "objectID": "lectures/md/KED2023_08.html#you-are-applying-for-a-job-at-a-big-company.",
    "href": "lectures/md/KED2023_08.html#you-are-applying-for-a-job-at-a-big-company.",
    "title": "The ABC of Computational Text Analysis",
    "section": "You are applying for a job at a big company.",
    "text": "You are applying for a job at a big company.\n\n\nEthik ist nicht nur abstrakt und gehört nicht nur in Philosophie\n\nnicht Begriff ist wichtig, sondern Denkart\nnachdenken über Ausgangslage + Konsequenzen\n\nAnekdoten aus eigenem Bewerbungsprozess\n\nals Bewerber\nfür Jobs, die Tools zur automatischen CV-Verarbeitung machen"
  },
  {
    "objectID": "lectures/md/KED2023_08.html#does-your-cv-pass-the-automatic-pre-filtering",
    "href": "lectures/md/KED2023_08.html#does-your-cv-pass-the-automatic-pre-filtering",
    "title": "The ABC of Computational Text Analysis",
    "section": "Does your CV pass the automatic pre-filtering?",
    "text": "Does your CV pass the automatic pre-filtering?\n:red_circle: :green_circle:\n\n\n:thinking: For what reasons?\n\n\nautomatische Vorselektion Bewerbungen\nbestenfalls: naiv, schlechtensfalls: anti-liberal/diskriminierend"
  },
  {
    "objectID": "lectures/md/KED2023_08.html#your-interview-is-recorded.-sunglasses-hot_face-what-personal-traits-are-inferred-from-that",
    "href": "lectures/md/KED2023_08.html#your-interview-is-recorded.-sunglasses-hot_face-what-personal-traits-are-inferred-from-that",
    "title": "The ABC of Computational Text Analysis",
    "section": "Your interview is recorded. :sunglasses: :hot_face: What personal traits are inferred from that? ",
    "text": "Your interview is recorded. :sunglasses: :hot_face: What personal traits are inferred from that? \n\n\n:thinking: Is it a good reflection of your personality?\n\n\n\nFace impressions as perceived by a model by [@Peterson2022]"
  },
  {
    "objectID": "lectures/md/KED2023_08.html#dont-worry-about-the-future",
    "href": "lectures/md/KED2023_08.html#dont-worry-about-the-future",
    "title": "The ABC of Computational Text Analysis",
    "section": "Don’t worry about the future …",
    "text": "Don’t worry about the future …\n\n… worry about the present.\n\nAI is persuasive in everyday’s life\n\nassessing risks and performances (credits, job, crimes, terrorism etc.)\n\nAI is extremely capable\nAI is not so smart and often poorly evaluated\n\n:bulb: What is going on behind the scene?\n\n\nje mehr die Systeme können, desto mehr werden sie eingesetzt, desto unsichtbarer wird, was sie nicht können\nModerne AI lernt Muster aus Daten. Gilt auch für NLP.\n\ngeneralisiert blind oder eben genau so wie gelernt\n\nUngleichheit wird reproduziert, gar verstärkt durch Systematik\n\nGeschlecht, Ethnie, sozioökonomisch\n\nProblematik ist Reichweite und Intransparenz\n\nMenschen auch fallibel, aber mit grösser Varietät\n\nAnwendungen für tabelarische Daten, Text, Bild, Video\n\nself-driving cars (walking pedestrians vs wheel chair pedestrians)\n\nEntwicklung NLP und Ethik hängen zusammen\n\nquasi: je leistungsfähiger NLP, desto mehr Bias wird mitgelernt im aktuellen Paradigma\nbesseres Verständnis = bessere Data Science"
  },
  {
    "objectID": "lectures/md/KED2023_08.html#algorithmic-managment-of-labour-force",
    "href": "lectures/md/KED2023_08.html#algorithmic-managment-of-labour-force",
    "title": "The ABC of Computational Text Analysis",
    "section": "Algorithmic Managment of Labour Force",
    "text": "Algorithmic Managment of Labour Force\n\n\n\nChatGPT may be used to communicate difficult desicions\n\n\n\n\nAkkordarbeit und bei ungenügender Leistung automatische Entlassung?"
  },
  {
    "objectID": "lectures/md/KED2023_08.html#from-bag-of-words-to-embeddings",
    "href": "lectures/md/KED2023_08.html#from-bag-of-words-to-embeddings",
    "title": "The ABC of Computational Text Analysis",
    "section": "From Bag of Words to Embeddings",
    "text": "From Bag of Words to Embeddings\n\nPutting Words into Computers [@Smith2020; @Church2021]\n\nfrom coarse, static to fine, contextual meaning\nhow to measure similarity of words\n\nstring-based\nsyntactic (e.g., part-of-speech)\nsemantic (e.g., animate)\nembedding as abstract representations\n\nfrom counting to learning representations\n\n\n\nEin Sprung, wie diese Technologie gewachsen ist\nProbleme\n\nBank kann Kreditinstitut bedeuten, in anderem Satz Parkbank\nHaus und Gebäude sehr ähnlich, aber nicht reflektiert in Oberflächenform\n\nSyntax und Semantik um Wörter zu gruppieren"
  },
  {
    "objectID": "lectures/md/KED2023_08.html#bag-of-words",
    "href": "lectures/md/KED2023_08.html#bag-of-words",
    "title": "The ABC of Computational Text Analysis",
    "section": "Bag of Words",
    "text": "Bag of Words\n\nword as arbitrary, discrete numbers\n\nKing = 1, Queen = 2, Man = 3, Woman = 4\n\nintrinsic meaning\nhow are these words similar?\n\n\n\n\nDiscrete, symbolic words [@Colyer2016]\n\n\n\n\nVektorrepräsentation für ganzes Vokubular\njedes Wort ist anderes als jedes andere, in unvergleichbarer Weise\nBoW lange Zeit Standard. Ergänzt durch zusätzliche Information wie POS"
  },
  {
    "objectID": "lectures/md/KED2023_08.html#representing-a-corpus",
    "href": "lectures/md/KED2023_08.html#representing-a-corpus",
    "title": "The ABC of Computational Text Analysis",
    "section": "Representing a Corpus",
    "text": "Representing a Corpus\n\n\n\nCollection of Documents\n\n\nNLP is great. I love NLP.\nI understand NLP.\nNLP, NLP, NLP.\n\n\n\n\n\nDocument Term Matrix\n\n\n\n\nNLP\nI\nis\nterm\n\n\n\n\nDoc 1\n2\n1\n1\n…\n\n\nDoc 2\n1\n1\n0\n…\n\n\nDoc 3\n3\n0\n0\n…\n\n\nDoc ID\n…\n…\n…\nterm frequency\n\n\n\n\n\n\n\n\nfür den Computer müssen Daten tabularisiert werden für weitere Verarbeitung"
  },
  {
    "objectID": "lectures/md/KED2023_08.html#i-eat-a-hot-___-for-lunch.",
    "href": "lectures/md/KED2023_08.html#i-eat-a-hot-___-for-lunch.",
    "title": "The ABC of Computational Text Analysis",
    "section": "“I eat a hot ___ for lunch.”",
    "text": "“I eat a hot ___ for lunch.”\n\n\nWörter können aber auch anders definiert werden und das will ich hier illustrieren\nFrage an Studis"
  },
  {
    "objectID": "lectures/md/KED2023_08.html#section",
    "href": "lectures/md/KED2023_08.html#section",
    "title": "The ABC of Computational Text Analysis",
    "section": "",
    "text": "You shall know a word by the company it keeps!\n@Firth1957\n\n\n\nkontextuelle Bedeutung statt intrinschische Definition\nSaussure: Zeichen nur definiert durch andere Zeichen\nrelationale Bedeutung: Objekt ist definiert durch Kontext\nlange ohne technische Implementation"
  },
  {
    "objectID": "lectures/md/KED2023_08.html#word-embeddings",
    "href": "lectures/md/KED2023_08.html#word-embeddings",
    "title": "The ABC of Computational Text Analysis",
    "section": "Word Embeddings",
    "text": "Word Embeddings\n\nword2vec [@Mikolov2013]\n\nwords as continuous vectors\n\naccounting for similarity between words\n\nsemantic similarity\n\nKing – Man + Woman = Queen\nFrance / Paris = Switzerland / Bern\n\n\n\n\n\n\nSingle continuous vector per word [@Colyer2016]\n\n\n\n\n\nWords as points in a semantic space [@Colyer2016]\n\n\n\n\n\nDoing arithmetics with words [@Colyer2016]\n\n\n\n\n\nSeit 2013 hat sich alles verändert\nvector = list of numbers -> point in Euclidean space\nIdee: wenn Wort genau gleich gebraucht wird, dann selbe Stelle\nSynonyme, Analogien finden\nalles noch globale Information. Ein Wort hat genau ein Vektor\n\nWas passiert mit mehrdeutigen Wörter (z.B. Bank)?\n\nFrage wie diese Repräsentationen genau gelernt wir nach Pause"
  },
  {
    "objectID": "lectures/md/KED2023_08.html#contextualized-word-embeddings",
    "href": "lectures/md/KED2023_08.html#contextualized-word-embeddings",
    "title": "The ABC of Computational Text Analysis",
    "section": "Contextualized Word Embeddings",
    "text": "Contextualized Word Embeddings\n\nBERT [@Devlin2019]\n\nrecontextualize static word embedding\n\ndifferent embeddings in different contexts\naccounting for ambiguity (e.g., bank)\n\nacquire linguistic knowledge from language models (LM)\n\nLM predict next/missing word\npre-trained on massive data (> 300 billions words)\n\n\n\n:boom: embeddings are the cornerstone of modern NLP\n\n\nalles lässt sich embedden (Wörter, Sätze, Paragraphen, Dokumente)"
  },
  {
    "objectID": "lectures/md/KED2023_08.html#learning-associations-from-data",
    "href": "lectures/md/KED2023_08.html#learning-associations-from-data",
    "title": "The ABC of Computational Text Analysis",
    "section": "Learning Associations from Data",
    "text": "Learning Associations from Data\n\n\n«___ becomes a doctor.»\n\n\n\n\n\nGender bias of the commonly used language model BERT [@Devlin2019]\n\n\n\n\nBERT wird in Google Search gebraucht"
  },
  {
    "objectID": "lectures/md/KED2023_08.html#cultural-associations-in-training-data",
    "href": "lectures/md/KED2023_08.html#cultural-associations-in-training-data",
    "title": "The ABC of Computational Text Analysis",
    "section": "Cultural Associations in Training Data",
    "text": "Cultural Associations in Training Data\n\n\n\nGender bias of the commonly used language model BERT [@Devlin2019]\n\n\n\n\nAnalyse umkehren: Nicht nach Pronomen fragen, sondern nach Tätigkeiten\nModel trained on Wikipedia and Books (not Reddit)"
  },
  {
    "objectID": "lectures/md/KED2023_08.html#word-embeddings-are-biased",
    "href": "lectures/md/KED2023_08.html#word-embeddings-are-biased",
    "title": "The ABC of Computational Text Analysis",
    "section": "Word Embeddings are biased …",
    "text": "Word Embeddings are biased …\n\n… because our data is we are biased. [@Bender2021]\n\n\nTimnit Gebru (Google Ethics Lead) gefeuert für dieses Paper Ende 2020\nDaten sind nicht besser als wir und Gesellschaft trägt extreme Diskriminierungen mit sich\nPause"
  },
  {
    "objectID": "lectures/md/KED2023_08.html#in-class-exercises-i",
    "href": "lectures/md/KED2023_08.html#in-class-exercises-i",
    "title": "The ABC of Computational Text Analysis",
    "section": "In-class: Exercises I",
    "text": "In-class: Exercises I\n\nOpen the following website in your browser: https://pair.withgoogle.com/explorables/fill-in-the-blank/\nRead the the article and play around with the interactive demo.\nWhat works surprisingly well? What is flawed by societal bias? Where do you see limits of large language models?"
  },
  {
    "objectID": "lectures/md/KED2023_08.html#how-does-deep-learning-work",
    "href": "lectures/md/KED2023_08.html#how-does-deep-learning-work",
    "title": "The ABC of Computational Text Analysis",
    "section": "How does Deep Learning work?",
    "text": "How does Deep Learning work?\n\nDeep Learning works like a huge bureaucracy\n\nstart with random prediction\nblame units for contributing to wrong predictions\nadjust units based on the accounted blame\nrepeat the cycle\n\n\n:nerd_face: train with gradient descent, a series of small steps taken to minimize an error function"
  },
  {
    "objectID": "lectures/md/KED2023_08.html#limitations-of-data-driven-deep-learning",
    "href": "lectures/md/KED2023_08.html#limitations-of-data-driven-deep-learning",
    "title": "The ABC of Computational Text Analysis",
    "section": "Limitations of data-driven Deep Learning",
    "text": "Limitations of data-driven Deep Learning\n\n\n„This sentence contains 32 characters.“ „Dieser Satz enthält 32 Buchstaben.“  \n\n\n\nohne Trainingsdaten aktuell nicht zu lösen"
  },
  {
    "objectID": "lectures/md/KED2023_08.html#current-state-of-deep-learning",
    "href": "lectures/md/KED2023_08.html#current-state-of-deep-learning",
    "title": "The ABC of Computational Text Analysis",
    "section": "Current State of Deep Learning",
    "text": "Current State of Deep Learning\n\nExtremely powerful but … [@Bengio2021]\n\ngreat at learning patterns, yet reasoning in its infancy\nrequires tons of data due to inefficient learning\ngeneralizes poorly\n\n\n\nout of domain (schwarze vs weisse Menschen, anderes Textgenre)"
  },
  {
    "objectID": "lectures/md/KED2023_08.html#data-digital-traces-social-artifacts",
    "href": "lectures/md/KED2023_08.html#data-digital-traces-social-artifacts",
    "title": "The ABC of Computational Text Analysis",
    "section": "Data = Digital Traces = Social Artifacts",
    "text": "Data = Digital Traces = Social Artifacts\n\ncollecting, curating, preserving traces\ndata is imperfect, always\n\nsocial bias, noise, lack of data etc.\n\ndata is more a tool to refine questions rather than a reflection of the world\n\n\n\nWas sind Daten?\n\nDaten sind kein Abbild der Welt, nichts natürliches.\n\nAnalog zu Romanos: Massenmedien sind nicht die Welt\n\nDaten sind auch nicht die Welt\n\nDaten liegen nicht einfach herum, sondern gemacht (siehe Schritte)"
  },
  {
    "objectID": "lectures/md/KED2023_08.html#data-vs.-capta",
    "href": "lectures/md/KED2023_08.html#data-vs.-capta",
    "title": "The ABC of Computational Text Analysis",
    "section": "Data vs. Capta",
    "text": "Data vs. Capta\n\nDifferences in the etymological roots of the terms data and capta make the distinction between constructivist and realist approaches clear. Capta is “taken” actively while data is assumed to be a “given” able to be recorded and observed.\n\n\nHumanistic inquiry acknowledges the situated, partial, and constitutive character of knowledge production, the recognition that knowledge is constructed, taken, not simply given as a natural representation of pre-existing fact.\n@Drucker2011\n\n\n\nviel Konflikt geht auf diese Realismus/Konstruktivismus Perspektive zurück\nKonstruktivmus heisst nur, Fragen zu stellen, wieso die Dinge sind, wie sie ausschauen"
  },
  {
    "objectID": "lectures/md/KED2023_08.html#section-1",
    "href": "lectures/md/KED2023_08.html#section-1",
    "title": "The ABC of Computational Text Analysis",
    "section": "",
    "text": "Raw data is an oxymoron.\n@Gitelman2013"
  },
  {
    "objectID": "lectures/md/KED2023_08.html#two-sides-of-the-ai-coin",
    "href": "lectures/md/KED2023_08.html#two-sides-of-the-ai-coin",
    "title": "The ABC of Computational Text Analysis",
    "section": "Two Sides of the AI Coin",
    "text": "Two Sides of the AI Coin\n\nExplaining vs. Solving\n\nconduct research to understand matters in science\nautomate matters in business using applied AI"
  },
  {
    "objectID": "lectures/md/KED2023_08.html#still-doubts-about-practical-implications",
    "href": "lectures/md/KED2023_08.html#still-doubts-about-practical-implications",
    "title": "The ABC of Computational Text Analysis",
    "section": "Still doubts about practical implications?",
    "text": "Still doubts about practical implications?\n\n\n\nGender bias in Google Translate\n\n\n\n\nUngarische Pronomen sind nich gendered"
  },
  {
    "objectID": "lectures/md/KED2023_08.html#and-it-goes-on",
    "href": "lectures/md/KED2023_08.html#and-it-goes-on",
    "title": "The ABC of Computational Text Analysis",
    "section": "And it goes on …",
    "text": "And it goes on …\n\n\n\nGender bias in Google Translate"
  },
  {
    "objectID": "lectures/md/KED2023_08.html#fair-is-a-fad",
    "href": "lectures/md/KED2023_08.html#fair-is-a-fad",
    "title": "The ABC of Computational Text Analysis",
    "section": "Fair is a Fad",
    "text": "Fair is a Fad\n\ncompanies also engage in fair AI to avoid regulation\nFair and good – but to whom? [@Kalluri2020 ]\nlacking democratic legitimacy\n\n\n\nFair kann ziemlich vieles bedeuten, solange man es selbst definieren kann\n\ndemokratische Legitimität fehlt für all diese Systeme\n\nlooking beyond data\n\ninvading privacy\neconomic monopolies\n(unpaid) AI-trainers and click-workers\nenvironmental costs"
  },
  {
    "objectID": "lectures/md/KED2023_08.html#section-2",
    "href": "lectures/md/KED2023_08.html#section-2",
    "title": "The ABC of Computational Text Analysis",
    "section": "",
    "text": "Don’t ask if artificial intelligence is good or fair, ask how it shifts power.\n@Kalluri2020"
  },
  {
    "objectID": "lectures/md/KED2023_08.html#data-represents-real-life.",
    "href": "lectures/md/KED2023_08.html#data-represents-real-life.",
    "title": "The ABC of Computational Text Analysis",
    "section": "Data represents real life.",
    "text": "Data represents real life.\nDon’t be a fool. Be wise, think twice."
  },
  {
    "objectID": "lectures/md/KED2023_08.html#references",
    "href": "lectures/md/KED2023_08.html#references",
    "title": "The ABC of Computational Text Analysis",
    "section": "References",
    "text": "References"
  },
  {
    "objectID": "lectures/md/KED2023_05.html",
    "href": "lectures/md/KED2023_05.html",
    "title": "The ABC of Computational Text Analysis",
    "section": "",
    "text": "perform shell commands :joystick:\n\nnavigate filesystem\ncreate/copy/move/remove files\n\ncomplete assignment :writing_hand:\n\n\n\nEinstieg in Shell\n\nVerzeichnisbaum, Erstellen von Files/Ordner\nPiping für komplexere Operationen\n\nÜbungen ok? technische Fragen?\nletztes Mal inhaltliche Zumutung, heute erste inhaltlich interessante Analysen\nähnliches Tempo, dafür mehr Zeit zum Üben"
  },
  {
    "objectID": "lectures/md/KED2023_05.html#get-around-in-your-filesystem-evergreen_tree",
    "href": "lectures/md/KED2023_05.html#get-around-in-your-filesystem-evergreen_tree",
    "title": "The ABC of Computational Text Analysis",
    "section": "Get around in your filesystem :evergreen_tree:",
    "text": "Get around in your filesystem :evergreen_tree:\n.\n├── README.md\n└── lectures\n    ├── images\n    │   └── ai.jpg\n    └── md\n        ├── KED2023_01.md\n        └── KED2023_02.md\nExample location of the course material: /home/alex/KED2023\n\npwd get the path to the current directory\ncd .. go one folder up\ncd FOLDERNAME go one folder down into FOLDERNAME\nls -l see the content of the current folder"
  },
  {
    "objectID": "lectures/md/KED2023_05.html#outline",
    "href": "lectures/md/KED2023_05.html#outline",
    "title": "The ABC of Computational Text Analysis",
    "section": "Outline",
    "text": "Outline\n\ncorpus linguistic using the shell​ :knife:\n\ncounting, finding, comparing​​\n\nanalyzing programmes of Swiss parties :bar_chart:\n\n\n\nFrequenzanalysen = Schweizer Taschenmesser\n\näusserst effektiv\n\nZiel: mehr Übungszeit\nSyntax nicht merken, Wichtiges werdet ihr schlussendlich erinnern"
  },
  {
    "objectID": "lectures/md/KED2023_05.html#when-politics-changes-language-changes.",
    "href": "lectures/md/KED2023_05.html#when-politics-changes-language-changes.",
    "title": "The ABC of Computational Text Analysis",
    "section": "When politics changes, language changes.",
    "text": "When politics changes, language changes.\n\n\n\nhistorical development of Swiss party politics (Tagesanzeiger)\n\n\n\n\nPositionierung Parteien im politischen Raum über Zeit\nGleiche Parteien, neue Ziele. Also doch nicht so gleich!\nWie erkenne ich semantische Veränderungen?\n\nhier: Abstimmungsparolen von Parteien ausgewertet\nWelche Ziele/Ideologien stehen dahinter? –> Texte fundamental\n\nWenn Politik ändert, ändert sich Sprache\n\noder gerade umgekehrtes zeitliches Verhältnis\nin Politik werden Narrative erprobt"
  },
  {
    "objectID": "lectures/md/KED2023_05.html#processing-a-text-collection",
    "href": "lectures/md/KED2023_05.html#processing-a-text-collection",
    "title": "The ABC of Computational Text Analysis",
    "section": "Processing a Text Collection",
    "text": "Processing a Text Collection\n\neach document as individual file ​(​​.​t​x​t​​)​\n\nuse shell for quick analysis\n\na dataset of documents (.csv, .tsv, .xml)\n\nuse Python for indepth analysis\n\n\n\n\n\nProcessing a collection of documents (src)\n\n\n\n\nStart sehr oft Kommandozeile (z.B. Datenextraktion), dann Auswertung in Python\ntxt-files erste Stufe bei Datensatzerstellung\nDaten existieren viele, Datensätze eher wenige\nbei Datensatz\n\nPython praktischer\nDokument in Zelle in tsv/csv-file\n\nvorerst arbeiten wir nur mit txt files"
  },
  {
    "objectID": "lectures/md/KED2023_05.html#frequency-analysis",
    "href": "lectures/md/KED2023_05.html#frequency-analysis",
    "title": "The ABC of Computational Text Analysis",
    "section": "Frequency Analysis",
    "text": "Frequency Analysis\n\n\n\nfrequency ~ measure of relevance\nbag of words approach\nsimple\npowerful\n\n\n\n\n\ntext as a bag of words (src)\n\n\n\n\n\n\nHäufigkeit indiziert Form von Relevanz\nin Häufigkeitsanalyse sind Worte kontextlos\n\nBoW = Sack mit Wörtern\nApproach schmerzt aus sozialwissenschaftlicher Perspektive\nVerlust Ambiguitäten = Nachteil // radikale Vereinfachung (einfaches Zählen) = grösster Vorteil\n\ntheoetische Übersicht von Approaches später im Seminar\n\nKontrolle, was dahinter steht\n\nähnlich wie Google Ngram, aber eigene Daten"
  },
  {
    "objectID": "lectures/md/KED2023_05.html#key-figures-of-texts",
    "href": "lectures/md/KED2023_05.html#key-figures-of-texts",
    "title": "The ABC of Computational Text Analysis",
    "section": "Key Figures of Texts",
    "text": "Key Figures of Texts\nwc *.txt    # count number of lines, words, characters\n\n\nzuerst Charakterisierung Datenquelle, nicht nur Inhalt\nZahlen für einzelne Dokumente und aggregiert auf Sammlung"
  },
  {
    "objectID": "lectures/md/KED2023_05.html#word-occurrences",
    "href": "lectures/md/KED2023_05.html#word-occurrences",
    "title": "The ABC of Computational Text Analysis",
    "section": "Word Occurrences",
    "text": "Word Occurrences\n\nshow in context\negrep -ir \"computational\" folder/       # search in all files in folder, ignore case\n\n# common egrep options:\n# -i            search case-insensitive\n# -r            search recursively in all subfolders\n# --colour      highlight matches\n# --context 2   show 2 lines above/below match\n\n\ncount words\negrep -ic \"big data\" *.txt      # count across all txt-files, ignore case\n\n\noptions\n\nignore case\nrecursive / specific files\n\nDateinamen als Filter benutzen\n\nQuelle/Jahr\negrep -ir ” daten” *svp*.txt\n\nwc als Alternative\nzeige in Kurs-Repo\n\negrep -irc –colour –context 3 “data” lectures/md | sort\n\n\ncd /home/alex/KED2023/materials/data/swiss_party_programmes\negrep -irc “ökologisch” ."
  },
  {
    "objectID": "lectures/md/KED2023_05.html#word-frequencies",
    "href": "lectures/md/KED2023_05.html#word-frequencies",
    "title": "The ABC of Computational Text Analysis",
    "section": "Word Frequencies",
    "text": "Word Frequencies\n\nsteps of the algorithm\n\nsplit text into one word per line (tokenize)\nsort words alphabetically\ncount how often each word appears\n\n# piping steps to get word frequencies\ncat text.txt | tr \" \" \"\\n\" | sort | uniq -c | sort -h > wordfreq.txt\n\n# explanation of individual steps:\ntr \" \" \"\\n\"     # replace spaces with newline \nsort -h         # sort lines alphanumerically\nuniq -c         # count repeated lines\n\n\nZweck: Häufigkeiten aller Wörter\nkein direkter Befehl -> Kombinieren von Befehlen (modular)\nBefehle erklären\n\nZusammenfassen gleicher Zeilen mit uniq\n\nNewline Character\nAggregation extrem flexibel\n\nanderer Text, alle Texte (*)\n\nFrage an Klasse: häufigstes Wort SVP?\n\nSchweiz, Bürger etc.: national, männlich\ncat materials/data/swiss_party_programmes/txt/svp_programmes/*txt |  tr \" \" \"\\n\" | sort | uniq -c | sort -h"
  },
  {
    "objectID": "lectures/md/KED2023_05.html#word-frequencies-1",
    "href": "lectures/md/KED2023_05.html#word-frequencies-1",
    "title": "The ABC of Computational Text Analysis",
    "section": "Word Frequencies",
    "text": "Word Frequencies\n\nabsolute frequency\nrelative frequency\n\n= n_occurrences / n_total_words\nindependent of size\n\nstatistical validation of variation\n\nsignificance tests between corpora\n\n\n\n\nKorpus = Textsammlung\nabsolut nur, wenn grösserer Output (z.B. mehr Flyers) mitgemessen werden soll"
  },
  {
    "objectID": "lectures/md/KED2023_05.html#convert-stats-into-dataset",
    "href": "lectures/md/KED2023_05.html#convert-stats-into-dataset",
    "title": "The ABC of Computational Text Analysis",
    "section": "Convert Stats into Dataset",
    "text": "Convert Stats into Dataset\n\nconvert to .tsv file\nuseful for further processing\n\ne.g., import in Excel\n\n\n# convert word frequencies into tsv-file\n# additional step: replace a sequence of spaces with a tabulator\ncat text.txt | tr \" \" \"\\n\" | sort | uniq -c  | sort -h | \\\ntr -s \" \" \"\\t\"  > test.tsv  \n\n\n-s alle Leerschläge durch Tabulator ersetzen\nrelative frequency in Excel"
  },
  {
    "objectID": "lectures/md/KED2023_05.html#in-class-matching-and-counting",
    "href": "lectures/md/KED2023_05.html#in-class-matching-and-counting",
    "title": "The ABC of Computational Text Analysis",
    "section": "In-class: Matching and counting",
    "text": "In-class: Matching and counting\n\nPrint the following sentence in your command line using echo.\necho \"There are a few related fields: NLP, computational linguistics, and computational text analysis.\"\nHow many words are in this sentence? Use the pipe operator to combine the command above with wc.\nMatch the words computational and colorize its occurences in the sentence using egrep.\nGet the frequencies of each word in this sentence using tr and other commands.\n\n\nPause"
  },
  {
    "objectID": "lectures/md/KED2023_05.html#common-preprocessing",
    "href": "lectures/md/KED2023_05.html#common-preprocessing",
    "title": "The ABC of Computational Text Analysis",
    "section": "Common Preprocessing",
    "text": "Common Preprocessing\n\nRefining results with\n\nlowercasing\nreplace symbols\njoin lines\ntrimming header + footer\nsplitting into multiple files\nusing patterns to remove/extract parts :date:\n\n\n\nPreprocessing für bessere Resultate\nRegex nächste Woche"
  },
  {
    "objectID": "lectures/md/KED2023_05.html#lowercasing",
    "href": "lectures/md/KED2023_05.html#lowercasing",
    "title": "The ABC of Computational Text Analysis",
    "section": "Lowercasing",
    "text": "Lowercasing\n\nreduce word forms\necho \"ÜBER\" | tr \"A-ZÄÖÜ\" \"a-zäöü\"  # fold text to lowercase\n\n\nGrossschreibung Satzanfang"
  },
  {
    "objectID": "lectures/md/KED2023_05.html#removing-and-replacing-symbols",
    "href": "lectures/md/KED2023_05.html#removing-and-replacing-symbols",
    "title": "The ABC of Computational Text Analysis",
    "section": "Removing and Replacing Symbols",
    "text": "Removing and Replacing Symbols\necho \"3x3\" | tr -d \"[:digit:]\"      # remove all digits \ncat text.txt | tr -d \"[:punct:]\"    # remove punctuation like .,:;?!- \n\ntr \"Y\" \"Z\"                          # replace any Y with Z\n\n\nEs gibt Zeichenklassen für Buchstaben, Zahlen und Interpunktion\nlöscht alle Einzelzeichen in Text (keine Sequenzen)\nInterpunktion wird sehr oft entfernt, da sowieso Kontext verloren geht in BoW"
  },
  {
    "objectID": "lectures/md/KED2023_05.html#standard-preprocessing",
    "href": "lectures/md/KED2023_05.html#standard-preprocessing",
    "title": "The ABC of Computational Text Analysis",
    "section": "Standard Preprocessing",
    "text": "Standard Preprocessing\n\nsave a preprocessed document\n# lowercase, no punctuation, no digits\ncat speech.txt | tr \"A-ZÄÖÜ\" \"a-zäöü\" | \\\ntr -d \"[:punct:]\" | tr -d \"[:digit:]\" > speech_clean.txt\n\n\nKleinschreibung , keine Interpunktion, keine Zahlen\nstandardmässige Repräsentation in BoW (hier noch mit Reihenfolge)"
  },
  {
    "objectID": "lectures/md/KED2023_05.html#join-lines",
    "href": "lectures/md/KED2023_05.html#join-lines",
    "title": "The ABC of Computational Text Analysis",
    "section": "Join Lines",
    "text": "Join Lines\ncat test.txt | tr -s \"\\n\" \" \"   # replace newlines with spaces\n\n\nharte Zeilenumbrüche entfernen\nsqueeze repeated newline and replace with a single whitespace"
  },
  {
    "objectID": "lectures/md/KED2023_05.html#trim-lines",
    "href": "lectures/md/KED2023_05.html#trim-lines",
    "title": "The ABC of Computational Text Analysis",
    "section": "Trim Lines",
    "text": "Trim Lines\ncat -n text.txt         # show line numbers\nsed \"1,10d\" text.txt    # remove lines 1 to 10"
  },
  {
    "objectID": "lectures/md/KED2023_05.html#splitting-files",
    "href": "lectures/md/KED2023_05.html#splitting-files",
    "title": "The ABC of Computational Text Analysis",
    "section": "Splitting Files",
    "text": "Splitting Files\n# splits file at every delimiter into a stand-alone file\ncsplit huge_text.txt  \"/delimiter/\" {*}"
  },
  {
    "objectID": "lectures/md/KED2023_05.html#check-differences-between-files",
    "href": "lectures/md/KED2023_05.html#check-differences-between-files",
    "title": "The ABC of Computational Text Analysis",
    "section": "Check Differences between Files",
    "text": "Check Differences between Files\n\nsanity check after modification\n# show differences side-by-side and only differing lines\ndiff -y --suppress-common-lines text_raw.txt text_proc.txt"
  },
  {
    "objectID": "lectures/md/KED2023_05.html#where-there-is-a-shell-there-is-a-way.-thumbsup",
    "href": "lectures/md/KED2023_05.html#where-there-is-a-shell-there-is-a-way.-thumbsup",
    "title": "The ABC of Computational Text Analysis",
    "section": "Where there is a shell,there is a way. :thumbsup:",
    "text": "Where there is a shell,there is a way. :thumbsup:\n\n\nZusammenfassung\n\nNach Filesystem, nun auch Bearbeiten, Zählen\n\nShell = flexibles + mächtiges Werkzeug durch Kombinieren von mehreren Commands\nStackoverflow liefert Antworten auf ein Problem"
  },
  {
    "objectID": "lectures/md/KED2023_05.html#organizing-code",
    "href": "lectures/md/KED2023_05.html#organizing-code",
    "title": "The ABC of Computational Text Analysis",
    "section": "Organizing Code",
    "text": "Organizing Code\n\nGit tracks file changes and allows for version management\nGitHub is a popular hosting platform based on Git\n\nshare code and collaborate\nrepository = project folder\n\n\n:nerd_face: Published code and data are parts of the endeavour of open science.\n\n\nVersion Managment Software\n\nähnlich Änderungsmodus in Word\n\nNutzen\n\nfür moderne Software-Entwicklung nicht wegzudenken\nneuerdings für Tracking wissenschaftlicher Arbeiten\n\nRepository = Ablage"
  },
  {
    "objectID": "lectures/md/KED2023_05.html#in-class-getting-ready",
    "href": "lectures/md/KED2023_05.html#in-class-getting-ready",
    "title": "The ABC of Computational Text Analysis",
    "section": "In-class: Getting ready",
    "text": "In-class: Getting ready\n\nChange into your local copy of the GitHub course repository KED2023 and update it with git pull. When you haven’t cloned the repository, follow section 5 of the installation guide .\nYou find some party programmes (Grüne, SP, SVP) in materials/data/swiss_party_programmes/txt. The programmes are provided in plain text which I have extracted from the publicly available PDFs.\nHave a look at the content of some of these text files using more."
  },
  {
    "objectID": "lectures/md/KED2023_05.html#in-class-analyzing-swiss-party-programmes-i",
    "href": "lectures/md/KED2023_05.html#in-class-analyzing-swiss-party-programmes-i",
    "title": "The ABC of Computational Text Analysis",
    "section": "In-class: Analyzing Swiss Party Programmes I",
    "text": "In-class: Analyzing Swiss Party Programmes I\n\nCompare the absolute frequencies of single terms or multi-word expressions of your choice (e.g., Ökologie, Sicherheit, Schweiz)…\n\nacross parties\nhistorically within a party\n\nUse the file names as filter to get various aggregation of the word counts.\nPick terms of your interest and look at their contextual use by extracting relevant passages. Does the usage differ across parties or time?\n\nShare your insights with the class using Etherpad."
  },
  {
    "objectID": "lectures/md/KED2023_05.html#in-class-analyzing-swiss-party-programmes-ii",
    "href": "lectures/md/KED2023_05.html#in-class-analyzing-swiss-party-programmes-ii",
    "title": "The ABC of Computational Text Analysis",
    "section": "In-class: Analyzing Swiss Party Programmes II",
    "text": "In-class: Analyzing Swiss Party Programmes II\n\nConvert the word frequencies per party into a tsv dataset. Compute the relative word frequency instead of the absolute frequency using any spreadsheet software (e.g. Excel). Are your conclusions still valid after accounting for the size?\nCan you refine the results with further preprocessing of the data?\nWhat is the size of the vocabulary of this data collection (number of unique words)?\n\nPro Tip :nerd_face:: Use egrep to look up commands in the .md course slides"
  },
  {
    "objectID": "lectures/md/KED2023_05.html#additional-resources",
    "href": "lectures/md/KED2023_05.html#additional-resources",
    "title": "The ABC of Computational Text Analysis",
    "section": "Additional Resources",
    "text": "Additional Resources\nWhen you look for useful primers on Bash, consider the following resources:\n\nTutorial Basic Text Analysis by W. Turkel\nTutorial Pattern Matching + KWIC by W. Turkel"
  },
  {
    "objectID": "lectures/md/KED2023_04.html",
    "href": "lectures/md/KED2023_04.html",
    "title": "The ABC of Computational Text Analysis",
    "section": "",
    "text": "Successful installation? :white_check_mark:\n\nScripting :dart:\n\nautomate, document, reproduce\n\nAny questions?\n\n\n\nInstallation\n\nInstallation erfolgreich?\nunklare Dinge im Guide?\nVoraussetzung für weitere Sessions\n\nProjektorganisation zentral\nScripting bedeutet Organisation\n\niterative Entwicklung\nReproduktion\nChaos ist nachvollziehbar!"
  },
  {
    "objectID": "lectures/md/KED2023_04.html#outline",
    "href": "lectures/md/KED2023_04.html#outline",
    "title": "The ABC of Computational Text Analysis",
    "section": "Outline",
    "text": "Outline\n\nlearn principles of the shell :classical_building:\nperform shell commands :arrow_forward:\nget practice by solving exercises :building_construction:\n\n\n\nheute Einführung in Shell, nächstes Mal tatsächlich nützliche Dinge\nCommands statt Klicks\nWieso Shell (und nicht Python?)\n\nwichtig für alle Datei-Operationen, skalierbar für Tausende Dateien\nmanche Programme lassen sich nur über Shell starten\neinfacher und effizienter als Python, Denken/Konzepte sehr ähnlich\n\nShell ist Cockpit bis zu Osterferien\nMetaziel\n\nsanfter Einstieg ins Programmieren\nVerstehen was wichtig ist in Computerinteraktion"
  },
  {
    "objectID": "lectures/md/KED2023_04.html#how-to-get-started",
    "href": "lectures/md/KED2023_04.html#how-to-get-started",
    "title": "The ABC of Computational Text Analysis",
    "section": "How to get started",
    "text": "How to get started\n\nOpen a Shell\n\n\n\nmacOS\n\nopen Terminal\nshell type: zsh\n\n\n\n\nWindows\n\nopen Ubuntu 20.04 LTS\nshell type: Bash\nopen Windows Command Prompt\n\n\n\n\n\n\ninteraktiv: Studis sollen Bash auch öffnen\n\nrasch durchgehen, damit mehr Übungszeit bleibt\n\nShell Commands platformübergreifend\nShell zeigen: Erscheinungsbild je System unterschiedlich"
  },
  {
    "objectID": "lectures/md/KED2023_04.html#bourne-again-shell",
    "href": "lectures/md/KED2023_04.html#bourne-again-shell",
    "title": "The ABC of Computational Text Analysis",
    "section": "Bourne-again Shell",
    "text": "Bourne-again Shell\n\n\n\nBash\n\noffers many built-in tools\nshell prompt\n\nUSER@HOSTNAME:~$\n\nhome directory\n\n~ refers to /home/USER\n\ncase-sensitive\nno feedback\n\nunless there is an issue\n\n\n\n\n\n\n\n\n\nComputersteuerung komplett über CLI möglich\n\nunnütze Verdoppelung zu GUI?\nermöglicht Bearbeitung Tausender Dateien\nschnelle Inspektion von Daten\n\nEmpfehlung: konsequente Nutzung CLI\n\nVertrauen, Geschwindigkeit\nAnfang braucht Gewöhnung\nVorteilen von beiden nutzen\n\nno need to memorize syntax\n\nslides, google, stackoverflow\nwiederkehrendes auto-erinnern\n\nEigenschaften\n\nextremely fast\nkompakter Code -> komplexe Prozessierung"
  },
  {
    "objectID": "lectures/md/KED2023_04.html#unix-philosophy",
    "href": "lectures/md/KED2023_04.html#unix-philosophy",
    "title": "The ABC of Computational Text Analysis",
    "section": "Unix Philosophy",
    "text": "Unix Philosophy\nBuild small programs that do one thing and do it well. :nerd_face:"
  },
  {
    "objectID": "lectures/md/KED2023_04.html#basic-commands-in-shell",
    "href": "lectures/md/KED2023_04.html#basic-commands-in-shell",
    "title": "The ABC of Computational Text Analysis",
    "section": "Basic commands in Shell",
    "text": "Basic commands in Shell\n\nexample components of a command\ncommand -a --long_argument FILE     # non-working example command\n\n\nrun command + help\necho \"hello world\"      # print some text\nman echo                # get help for any command (e.g., echo)\n\n\nCLI: Eingabe + Ausgabe gleiches Fenster\nTradition von hello world\nquotes to preserve formatting\nHilfeseiten sehr umfangreich\n\ngooglen oft schneller –> stackoverflow"
  },
  {
    "objectID": "lectures/md/KED2023_04.html#and-how-to-find-them",
    "href": "lectures/md/KED2023_04.html#and-how-to-find-them",
    "title": "The ABC of Computational Text Analysis",
    "section": "… and how to find them",
    "text": "… and how to find them\n\n\n\nhierarchical filesystem :evergreen_tree:\n\nfolders/directories\nfiles with a suffix\n\nabsolute path starting from top-level directory\n\ne.g. /home/alex/KED2023/slides/KED2023_01.html\n\nrelative path looking from current directory\n\ne.g. KED2023/slides/KED2023_01.html\n\n\n\n.\n├── README.md\n└── lectures\n    ├── images\n    │   └── ai.jpg\n    ├── html\n    │   ├── KED2023_01.html\n    │   └── KED2023_02.html\n    └── md\n        ├── KED2023_01.md\n        └── KED2023_02.md\n\n\n\n:thumbsup: Only relative paths work across systems\n\n\nSehr wichtig und hat in Vergangenheit Probleme bereitet (“Cloud/App-Abstraktion”)\nverschachtelte, hierarchische Struktur\n\nwie Aktenschrank mit Ordner\n\nNavigation in diesem Baum/Aktenschrank (tiefer/höher)\n/ für Angabe von Pfad (Verzeichnis/Unterverzeichnis)\nabsoluter Pfad beginnt mit Slash, relativer ohne Slash\nOrdner / Dateien –> Endungen\n\nkeine technische Notwendigkeit"
  },
  {
    "objectID": "lectures/md/KED2023_04.html#important-places-in-your-filesystem",
    "href": "lectures/md/KED2023_04.html#important-places-in-your-filesystem",
    "title": "The ABC of Computational Text Analysis",
    "section": "Important Places in your Filesystem",
    "text": "Important Places in your Filesystem\n\nshortcut names of directories\n\n. current dir\n.. parent dir\n~ home dir (e.g. /home/alex)\n\nfind your files on Windows\n\n/mnt/c/Users/YOUR_USERNAME/\nshortcut with documents\n\n\n\n\nWindows-Leute hinweisen auf Pfad innerhalb Ubuntu"
  },
  {
    "objectID": "lectures/md/KED2023_04.html#navigating-in-a-file-system",
    "href": "lectures/md/KED2023_04.html#navigating-in-a-file-system",
    "title": "The ABC of Computational Text Analysis",
    "section": "Navigating in a File System",
    "text": "Navigating in a File System\npwd                 # show absolute path of current directory\n\nls                  # list content of current directory\nls -lh              # list with more information\nls dirname          # list content of directory dirname\n\ncd ..               # change directory to go folder up\ncd dir/subdir       # go to folder dir/subdir (two folders down)\n\nwhen you are lost, open in file manager (GUI)\nopen .          # open path in finder (macOS)\nexplorer.exe .  # open Windows Explorer in WSL Ubuntu (Windows)\n\n\nBefehle sind Abkürzungen\n\ncd = change directory\nls = list\n\ndiskutieren von ls output\n\nBerechtigung/Eigentümer/Grösse/Mod.-datum/Name\nd für directory\n\nbeliebige Verzeichnisse springen\n\noberstes Verzeichnis: C: oder /\ncd zurück ins home\n\nKommentare mit #"
  },
  {
    "objectID": "lectures/md/KED2023_04.html#open-files",
    "href": "lectures/md/KED2023_04.html#open-files",
    "title": "The ABC of Computational Text Analysis",
    "section": "Open Files",
    "text": "Open Files\n\nshow within Shell\nmore text.txt           # print content (space to scroll)\n\nhead text.txt           # print first 10 lines of file      \ntail -5 text.txt        # print last 5 lines of file\n\n\nshow with default application (GUI)\nopen text.txt           # macOS\nwslview text.txt        # WSL Ubuntu (Windows)"
  },
  {
    "objectID": "lectures/md/KED2023_04.html#useful-key-actions",
    "href": "lectures/md/KED2023_04.html#useful-key-actions",
    "title": "The ABC of Computational Text Analysis",
    "section": "Useful Key Actions",
    "text": "Useful Key Actions\n\nautocompletion: TAB\nget last command: :arrow_up:\nscrolling: SPACE\ncancel CTRL + C\nquit: q or CTRL + D\n\n\n\nlange Dateinamen, Programme nur halb erinnert\nq bspw. in Hilfe-Seite man"
  },
  {
    "objectID": "lectures/md/KED2023_04.html#creating-moving-and-copying",
    "href": "lectures/md/KED2023_04.html#creating-moving-and-copying",
    "title": "The ABC of Computational Text Analysis",
    "section": "Creating, Moving and Copying",
    "text": "Creating, Moving and Copying\n\ncreate files and directories\ntouch test.txt      # create a new file\n\nmkdir data          # make a new directory\nmkdir -p data/1999  # make a new directory with a subfolder\n\n\ncopy and move files\ncp test.txt other/.             # copy file into other folder, keep its name            \nmv test.txt other/new_name.txt  # move or rename a file\n\n\ntouch abängig, ob Datei existiert\nmkdir -p für Subdirs"
  },
  {
    "objectID": "lectures/md/KED2023_04.html#removing-files",
    "href": "lectures/md/KED2023_04.html#removing-files",
    "title": "The ABC of Computational Text Analysis",
    "section": "Removing Files",
    "text": "Removing Files\nWatch out, there is no recycle bin. No way back!\nrm old.txt          # remove a file\nrm -r old_data      # remove a folder with all its files"
  },
  {
    "objectID": "lectures/md/KED2023_04.html#in-class-exercises-i",
    "href": "lectures/md/KED2023_04.html#in-class-exercises-i",
    "title": "The ABC of Computational Text Analysis",
    "section": "In-class: Exercises I",
    "text": "In-class: Exercises I\n\nCreate a new directory called tmp.\nChange into that directory using cd and print its absolute path using pwd.\nUse touch to create a new file called magic.txt in tmp.\nRename the file from magic.txt to easy_as_pie.txt.\nCheck out the helper page of mv command.\nLook around in the filesystem using cd and ls.\n\n\nPause"
  },
  {
    "objectID": "lectures/md/KED2023_04.html#wildcards",
    "href": "lectures/md/KED2023_04.html#wildcards",
    "title": "The ABC of Computational Text Analysis",
    "section": "Wildcards",
    "text": "Wildcards\n\nplaceholders to match …\n\nany single character: ?\nany sequence of characters: *\n\nmv data/*.txt new_data/.    # move txt-files from to another subfolder\ncp *.txt files/.            # copy all txt-files in a single folder\n\n\nbei Unkenntnis oder Zusammenfassung von Dateinamen\nbatch operation\nOrdner muss existieren\n\nzeige Fehlermeldung"
  },
  {
    "objectID": "lectures/md/KED2023_04.html#searching",
    "href": "lectures/md/KED2023_04.html#searching",
    "title": "The ABC of Computational Text Analysis",
    "section": "Searching",
    "text": "Searching\n\ncollect certain files only\nls *.txt        # list all files with the suffix .txt (in current directory)\n\n\nfind specific files\n# search on filename\nfind /path/to/dir -name \"*speech*\"  # find files in specific directory\nlocate -i pattern_1 pattern_2       # global search of files/folders\n\n# search on content\ngrep -r \"Europe\" /path/to/dir       # find all files containing X in a directory \n\n\ngrep ist mächtig, häufig brauchen\ngrep nur für rohe Textformate (txt, html, csv etc.)\narg -i case-insensitiveness"
  },
  {
    "objectID": "lectures/md/KED2023_04.html#expansion",
    "href": "lectures/md/KED2023_04.html#expansion",
    "title": "The ABC of Computational Text Analysis",
    "section": "Expansion",
    "text": "Expansion\n\nbatch processing with expansion\ntouch text_{a..c}.txt   \n# is equivalent to\ntouch text_a.txt text_b.txt text_c.txt\n\nmkdir {2000..2005}{a..c}\n# is equivalent to\nmkdir 2000a 2000b 2000c 2001a 2001b 2001c ...\n\n\nhilfreich um Dateien zu ordnen"
  },
  {
    "objectID": "lectures/md/KED2023_04.html#operators",
    "href": "lectures/md/KED2023_04.html#operators",
    "title": "The ABC of Computational Text Analysis",
    "section": "Operators",
    "text": "Operators\n\n\n\nOperators machten Probleme\nmodulares Zusammenbauen von Commands\n“Leim” zum Übergeben von Resultaten\nFiles als Zwischenprodukte umgehen\nUnix Philosophy"
  },
  {
    "objectID": "lectures/md/KED2023_04.html#combining-commands",
    "href": "lectures/md/KED2023_04.html#combining-commands",
    "title": "The ABC of Computational Text Analysis",
    "section": "Combining Commands",
    "text": "Combining Commands\n\nuse shell operators to …\n\nredirect output into file (overwrite): >\nappend to existing file: >>\nstream to next command: | (pipe)\n\necho 'line 1' > test.txt    # write into file\nmore test.txt | tail -1     # pass output to next command    \n\nLearn more about operators\n\n\ncheck output first, then overwrite file as a second step"
  },
  {
    "objectID": "lectures/md/KED2023_04.html#merging-files",
    "href": "lectures/md/KED2023_04.html#merging-files",
    "title": "The ABC of Computational Text Analysis",
    "section": "Merging Files",
    "text": "Merging Files\ncat part_1.txt part_2.txt       # concatenate multiple files\ncat *.txt > all_text.txt        # merge all txt into a single one"
  },
  {
    "objectID": "lectures/md/KED2023_04.html#conventions-pray",
    "href": "lectures/md/KED2023_04.html#conventions-pray",
    "title": "The ABC of Computational Text Analysis",
    "section": "Conventions :pray:",
    "text": "Conventions :pray:\n\nno spaces/umlauts in names\n\nalphanumeric, underscore, hyphen, dot\n\nfiles have a suffix, folders don’t\n\ntext_1.txt vs. texts\n\ndescriptive file names\n\nSOURCE/YEAR/speech_party_X.txt\n\ndon’t modify the raw data\n\n\n\nKonventionen helfen Fehler vermindern, Verständnis verbessern"
  },
  {
    "objectID": "lectures/md/KED2023_04.html#writing-a-runnable-script",
    "href": "lectures/md/KED2023_04.html#writing-a-runnable-script",
    "title": "The ABC of Computational Text Analysis",
    "section": "Writing a runnable Script",
    "text": "Writing a runnable Script\n\nExample script: find_all_pdf.sh\n#!/bin/sh\n\necho \"This is a list of all PDFs on my computer:\"\nlocate -i /home/*.pdf\n\nfile with suffix .sh\n\none command per row\n# precedes comments\n\nstart script with Shebang #!/bin/sh\nexecute with bash SCRIPTNAME.sh\n\n\n\nAutomation = Ablauf von Instruktionen\nVorteil: nicht nötig Befehlsabfolge zu merken\nÜbung ebenfalls als Skript abgeben\nno prompt, just commands"
  },
  {
    "objectID": "lectures/md/KED2023_04.html#the-beauty-of-scripting-is-automation.-zap",
    "href": "lectures/md/KED2023_04.html#the-beauty-of-scripting-is-automation.-zap",
    "title": "The ABC of Computational Text Analysis",
    "section": "The beauty of scripting is automation. :zap:",
    "text": "The beauty of scripting is automation. :zap:"
  },
  {
    "objectID": "lectures/md/KED2023_04.html#assignment-1-writing_hand",
    "href": "lectures/md/KED2023_04.html#assignment-1-writing_hand",
    "title": "The ABC of Computational Text Analysis",
    "section": "Assignment #1 :writing_hand:",
    "text": "Assignment #1 :writing_hand:\n\nget/submit via OLAT\n\nstarting tonight\ndeadline: 31 March 2023, 23:59\n\ndiscuss issues on OLAT forum\nask friends for support, not solutions\n\n\n\nkleine Übung\nVertrauen entwickeln\nnicht bis zum Schluss warten"
  },
  {
    "objectID": "lectures/md/KED2023_04.html#in-class-exercises-ii",
    "href": "lectures/md/KED2023_04.html#in-class-exercises-ii",
    "title": "The ABC of Computational Text Analysis",
    "section": "In-class: Exercises II",
    "text": "In-class: Exercises II\n\nCreate a new file with touch.\nWrite the following content into that file, one line at a time using the append operator:\nHow about making programming a little more accessible? Like:\nfrom human_knowledge import solution\nMake sure that the content was written into that file using more."
  },
  {
    "objectID": "lectures/md/KED2023_04.html#in-class-exercises-iii",
    "href": "lectures/md/KED2023_04.html#in-class-exercises-iii",
    "title": "The ABC of Computational Text Analysis",
    "section": "In-class: Exercises III",
    "text": "In-class: Exercises III\n\nNavigate up and down in in your filesystem using cd and list the respective files per directory with ls. Where can you find your personal documents? Print the absolute path with pwd.\nA hint to Windows users as they are working in a Ubuntu subsystem, have a look at: /mnt/c/Users\nRead man ls and write an ls command that lists your documents ordered\n\nby recency (time)\nby size\n\nUse the | and > operators to write the 3 “last modified” files in your documents folder into a file called last-modified.txt on your desktop (desktop is also a directory). It is a single command performing multiple operations, one after another."
  },
  {
    "objectID": "lectures/md/KED2023_04.html#additional-resources",
    "href": "lectures/md/KED2023_04.html#additional-resources",
    "title": "The ABC of Computational Text Analysis",
    "section": "Additional Resources",
    "text": "Additional Resources\n\nuseful primers on Bash\n\nCheatsheet for this course\nThe Programming Historian\nDigitalOcean"
  },
  {
    "objectID": "lectures/md/KED2023_01.html",
    "href": "lectures/md/KED2023_01.html",
    "title": "The ABC of Computational Text Analysis",
    "section": "",
    "text": "digital revolution or hype?\nabout us\ngoals of this course\n\n\n\nSRF-Archivperle Computer (1982)\nDreiteilig\n\nHeranführung, Austausch, Kursorganisation\n\nbrandaktuelles Thema, öffentlicher Diskurs\nExkurs: Computer als Zaubermaschine vs. Werkzeug\n\nWas ist künstliche Intelligenz?\n\nErst dann Textanalyse im eigentlichen Sinne"
  },
  {
    "objectID": "lectures/md/KED2023_01.html#the-world-has-changed-hasnt-it",
    "href": "lectures/md/KED2023_01.html#the-world-has-changed-hasnt-it",
    "title": "The ABC of Computational Text Analysis",
    "section": "The world has changed, hasn’t it?",
    "text": "The world has changed, hasn’t it?\n\n\n\n\n\n\n\nWelt im Wandel\nEinfache Google Suche nach “AI”\nWer kennt solche Bilder nicht?\n\nRoboter, Hybride, Hirne\nBlauton\n\npopulärwissenschaftliche Bild transportiert\nAI ist hip: Technologiefirmen, Forschung, Konsumenten"
  },
  {
    "objectID": "lectures/md/KED2023_01.html#group-discussion",
    "href": "lectures/md/KED2023_01.html#group-discussion",
    "title": "The ABC of Computational Text Analysis",
    "section": "Group Discussion",
    "text": "Group Discussion\n\nWhat makes a computer looking intelligent?\n\n\nAI is a moving target with respect to …\n\nhuman capabilities\ntechnological abilities\n\n\n\n\nDiskussion bevor ich meine Perspektive einführe (5min)\n\nVorstellung von Studis?\nWer braucht Siri/Alexa & Co.?\n\nAltersdifferenz spielt wohl eine Rolle in IT\nIntelligenz nichts Absolutes, relativ zu Menschen\n\nvon Schachspielen zu Lernen\nGeneralisierbarkeit\n\nGewöhnungseffekt: “AI is whatever hasn’t been done yet.” D. Hofstadter\nEindrückliche Beispiele, Stand Forschung"
  },
  {
    "objectID": "lectures/md/KED2023_01.html#transfer-of-human-intelligence",
    "href": "lectures/md/KED2023_01.html#transfer-of-human-intelligence",
    "title": "The ABC of Computational Text Analysis",
    "section": "Transfer of Human Intelligence",
    "text": "Transfer of Human Intelligence\n\nfrom static machines to more flexible devices\n\nmimicking intelligent behavior\n\nreading + seeing + hearing\nspeaking + writing + drawing\n\na sense of contextual perception\nmany degrees of freedom\n\n\n\nReferenzfolie Mensch\n\nImitieren von menschlichen Sinnen und Ausdrücken\nSprechen/Sehen/Hören/Zeichnen\nStatische maschine\n\nif this then that\n\nKontextabhängiges Handeln\n\nInteraktion initiieren\nSituationsangepasstheit\n\n\nEindrückliche Beispiele, Stand Forschung"
  },
  {
    "objectID": "lectures/md/KED2023_01.html#seeing-like-a-human",
    "href": "lectures/md/KED2023_01.html#seeing-like-a-human",
    "title": "The ABC of Computational Text Analysis",
    "section": "Seeing like a Human?",
    "text": "Seeing like a Human?\n\n\n\nAn image segmentation with Facebook’s Detectron2 [@Wu2019]\n\n\n\n\nUnbeschriftete Dinge? → ontologische Frage\n\nMensch nur als Ganzes, ohne Erkennung von Kleidern\nPerson ohne Geschlecht?\nTeekanne falsch/nicht erkannt\n\nKeine naturgegebene Ordnung oder technologische Notwendigkeit, sondern in Code/Daten gegossene Entscheidungen"
  },
  {
    "objectID": "lectures/md/KED2023_01.html#speaking-like-a-human",
    "href": "lectures/md/KED2023_01.html#speaking-like-a-human",
    "title": "The ABC of Computational Text Analysis",
    "section": "Speaking like a Human?",
    "text": "Speaking like a Human?\n\nSpeech-to-Text (STT)\nRecognizing speech regardless of language, accent, speed, noise etc.\nCheck out samples of Whisper [@Radford2022]\n\n\nText-to-Speech Synthesis (TTS)\nPersonalizing voice given an audio sample of 3s\nCheck out samples of VALL-E [@Wang2023]"
  },
  {
    "objectID": "lectures/md/KED2023_01.html#outsmarting-humans",
    "href": "lectures/md/KED2023_01.html#outsmarting-humans",
    "title": "The ABC of Computational Text Analysis",
    "section": "Outsmarting Humans?",
    "text": "Outsmarting Humans?\n\n\n\n\n\n\n\nUniLu Suchmaschine\nSchweizerdeutsch\nEssay + Reim\nBullshit"
  },
  {
    "objectID": "lectures/md/KED2023_01.html#chatgpt-is-amazing-but",
    "href": "lectures/md/KED2023_01.html#chatgpt-is-amazing-but",
    "title": "The ABC of Computational Text Analysis",
    "section": "ChatGPT is amazing but …",
    "text": "ChatGPT is amazing but …\n\n… it is also a stochastic parrot. :parrot:\n\n\n[@Bender2021]"
  },
  {
    "objectID": "lectures/md/KED2023_01.html#can-you-disenchant-chatgpt",
    "href": "lectures/md/KED2023_01.html#can-you-disenchant-chatgpt",
    "title": "The ABC of Computational Text Analysis",
    "section": "Can you disenchant ChatGPT?",
    "text": "Can you disenchant ChatGPT?\n\nExperiment with ChatGPT\n\nWhat works (surprisingly) well?\nWhere does it fail?"
  },
  {
    "objectID": "lectures/md/KED2023_01.html#beyond-perception-and-unimodality",
    "href": "lectures/md/KED2023_01.html#beyond-perception-and-unimodality",
    "title": "The ABC of Computational Text Analysis",
    "section": "Beyond Perception and Unimodality",
    "text": "Beyond Perception and Unimodality\n\n\npassive vs. aktive Rolle von AI\nChat verbindet Wahrnehmen + Generieren\nintensive Forschung Generieren von Text, Bild, Video\nZusammenbringen von mehreren Medien\nAuswirkung auf Sozialwelt"
  },
  {
    "objectID": "lectures/md/KED2023_01.html#section",
    "href": "lectures/md/KED2023_01.html#section",
    "title": "The ABC of Computational Text Analysis",
    "section": "",
    "text": "Generated Images by a Neural Network\nhttps://thisxdoesnotexist.com/\nGive me more!\n\n\n\nbeliebiges Generieren photorealer Gesichter durch Computer\nProblem: Es gibt kein manipulationssicheres Medium mehr. Es kann alles generiert werden: Bilder, Video, Texte"
  },
  {
    "objectID": "lectures/md/KED2023_01.html#trend-towards-multimodality",
    "href": "lectures/md/KED2023_01.html#trend-towards-multimodality",
    "title": "The ABC of Computational Text Analysis",
    "section": "Trend towards Multimodality",
    "text": "Trend towards Multimodality\n\n\n\nBreakthrough by combining language processing and image generation with Muse [@Chang2023]\n\n\n\n\nzielgerichtete Generierung über Multimodalität\n\nmit Sprache neue, sehr eigenwillige (!) Bild generieren lassen\n\n3 Monate altes Paper"
  },
  {
    "objectID": "lectures/md/KED2023_01.html#deepfakes-it-is-real",
    "href": "lectures/md/KED2023_01.html#deepfakes-it-is-real",
    "title": "The ABC of Computational Text Analysis",
    "section": "Deepfakes? It is real!",
    "text": "Deepfakes? It is real!\n\n\n\nEditing pictures with Muse using natural language [@Chang2023]\n\n\n\n\nNicht nur generieren, sondern auch verändern\nDeepfakes\n\nBildmanipulation gab es schon zu Zeiten Stalin, aber “Photoshop” wird einfacher\nnicht nur technologisch interessant, sondern auch gesellschaftliche Auswirkungen\n\nProblematisch für\n\nPersönlichkeitsrechte\nJournalismus und historische Forschung\n\nBusiness-Möglichkeiten\n\nZalando-Kleider virtuell anprobieren\nvirtueller Ikea Einrichtungskatalog"
  },
  {
    "objectID": "lectures/md/KED2023_01.html#but-videos-are-still-real",
    "href": "lectures/md/KED2023_01.html#but-videos-are-still-real",
    "title": "The ABC of Computational Text Analysis",
    "section": "But videos are still real?",
    "text": "But videos are still real?\nCheckout this demo trailer for authentic dubbing.\n:movie_camera:\n\nhttps://www.latimes.com/entertainment-arts/business/story/2022-12-19/the-next-frontier-in-moviemaking-ai-edits"
  },
  {
    "objectID": "lectures/md/KED2023_01.html#artificial-intelligence",
    "href": "lectures/md/KED2023_01.html#artificial-intelligence",
    "title": "The ABC of Computational Text Analysis",
    "section": "Artificial Intelligence",
    "text": "Artificial Intelligence\n\nSubfields\n\nNatural Language Processing (NLP)\nComputer Vision (CV)\nRobotics\n\n\n\nVon der Anwendungsseite zum technisch-wissenschaftlichen Teil\nin Wissenschaft primär drei Felder\nsehr viel gemeinsam in Methodik: Lernen von Daten"
  },
  {
    "objectID": "lectures/md/KED2023_01.html#how-does-computer-intelligence-work",
    "href": "lectures/md/KED2023_01.html#how-does-computer-intelligence-work",
    "title": "The ABC of Computational Text Analysis",
    "section": "How does Computer Intelligence work?",
    "text": "How does Computer Intelligence work?\n\ninterchangeably (?) used concepts\n\nArtificial Intelligence (AI), Machine Learning (ML), Deep Learning (DL)\n\nlearn patterns from lots of data\n\nmore recycling than genuine intelligence\ntheory agnostically\n\nsupervised training is the most popular\n\nlearn relation between input and output\n\n\n\n\n\nWie funktionieren diese Systeme?\nAI-Paradigma: Logik vs. Lernen\nDL = dominantes ML-Modell\n\nschichtweise Abstraktion\nunzureichende Metapher Hirn (Neuron trägt zur Konfusion bei)\n\nLernen von Unmengen Daten\n\nInput-Antwort-Beziehung\n\nRegression = ML\n\nRelated concepts (src)"
  },
  {
    "objectID": "lectures/md/KED2023_01.html#ai-hype-in-a-nutshell",
    "href": "lectures/md/KED2023_01.html#ai-hype-in-a-nutshell",
    "title": "The ABC of Computational Text Analysis",
    "section": "AI Hype in a Nutshell",
    "text": "AI Hype in a Nutshell\nAI = from humankind import solution\n\n\nmanche Dinge für Computer schwierig, für Menschen einfach (und umgekehrt)\nSchach einfach für Computer, Unterschied Katze/Hund lange schwierig\nAlgo für Katze/Hund-Bestimmung erkennt kein Auto, weil zu spezifisch\n\nkeine Angst vor Computer, die Welt übernehmen"
  },
  {
    "objectID": "lectures/md/KED2023_01.html#this-is-how-current-ai-looks-like",
    "href": "lectures/md/KED2023_01.html#this-is-how-current-ai-looks-like",
    "title": "The ABC of Computational Text Analysis",
    "section": "This is how current AI looks like",
    "text": "This is how current AI looks like\n\n\nFalls jemand noch immer überzeugt ist von der Intelligenz dieser Systeme, dies soll Zweifel hervorrufen"
  },
  {
    "objectID": "lectures/md/KED2023_01.html#computational-social-science",
    "href": "lectures/md/KED2023_01.html#computational-social-science",
    "title": "The ABC of Computational Text Analysis",
    "section": "Computational Social Science",
    "text": "Computational Social Science\n\ndata-driven research\n\ncomputational social science [@Lazer2009]\n\nDigital Humanities, Computational History, Data Science\n\nhighly interdisciplinary\nearly computational history already in 1960s [@Graham2015]\n\n\n\nWieso zeige ich all das in sozialwissenschaftlichem Kurs?\n\nAI Spitze von Eisberg\nPointe: Data-driven applications + research (!)\nPaper CSS Manifesto\n\nDaten\n\nModelle sind nur die halbe Miete, Daten sind der Zauberstoff.\nResearch: Nicht neuste ML, sorgfältiges aggregieren\n\nSozialwissenschaften müssen sich bewegen\n\nCSS löst Problem sinkender response-rate in Survey\nalternative Datenquellen\nAuswirkungen Forschung, Fragestellung, Ausbildung\n\nCSS mit langer Vorgeschichte, nie Mainstream"
  },
  {
    "objectID": "lectures/md/KED2023_01.html#group-discussion-1",
    "href": "lectures/md/KED2023_01.html#group-discussion-1",
    "title": "The ABC of Computational Text Analysis",
    "section": "Group Discussion",
    "text": "Group Discussion\n\nWhat kind of data is there?\n\n\nWhat data is relevant for social science?\n\n\n\ndata as traces of social behaviour\n\ntabular, text, image\n\ndatafication\n\nsensors of smartphone, digital communication\n\nmuch of human knowledge compiled as text\n\n\n\n\nalles sind Daten\nje mehr digital, desto einfacher für Wissenschaft\nadvent of cheap computational resources as well as the mass digitization of libraries and archives"
  },
  {
    "objectID": "lectures/md/KED2023_01.html#about-the-mystery-of-coding",
    "href": "lectures/md/KED2023_01.html#about-the-mystery-of-coding",
    "title": "The ABC of Computational Text Analysis",
    "section": "About the Mystery of Coding",
    "text": "About the Mystery of Coding\n\ncoding is like…\n\ncooking with recipes\nsuperpowers\n\n\n\nmoderne Datenauswertung braucht Coding-Skills\n2 Metapher für Abarbeiten von Befehlen\n\nCode wie Kochrezept\nSuperkraft: Küchenmaschine kommt quasi gleich mit"
  },
  {
    "objectID": "lectures/md/KED2023_01.html#women-have-coding-powers-too",
    "href": "lectures/md/KED2023_01.html#women-have-coding-powers-too",
    "title": "The ABC of Computational Text Analysis",
    "section": "Women have coding powers too!",
    "text": "Women have coding powers too!\n\n\nBild von Superman im Kopf\nweniger Frauen in IT ist trauriger Fakt\ngewachsen auf historischen Stereotypen"
  },
  {
    "objectID": "lectures/md/KED2023_01.html#where-the-actual-revolution-is",
    "href": "lectures/md/KED2023_01.html#where-the-actual-revolution-is",
    "title": "The ABC of Computational Text Analysis",
    "section": "Where the actual Revolution is",
    "text": "Where the actual Revolution is\nCoding is a superpower …\n\nflexible\nreusable\nreproducible\ninspectable\ncollaborative\n\n… to tackle complex problems on scale\n\n\nCoding ermöglicht Verarbeitung von komplexen, nicht standardisierten Daten\nVorteile\n\nalles explizit und nachvollziehbar, kritisier- und verbesserbar\nFehler im Code möglich → beheben → erneut ausführen (Fortschritt)\nleicht andere Fragestellung → Code schnell adaptieren\n\nPause"
  },
  {
    "objectID": "lectures/md/KED2023_01.html#personal-example",
    "href": "lectures/md/KED2023_01.html#personal-example",
    "title": "The ABC of Computational Text Analysis",
    "section": "Personal Example",
    "text": "Personal Example\ndirected country mentions in UN speeches\n\n\nUN-Debatte: Wer erwähnt wen in Rede?\n\nAufmerksamkeiten\nExtrahiert aus Texten\n\nFarben/Ordnung nach Kontinent\nRolle der USA (Regenbogen)\nErkenntnisse\n\nRegionale Aufmerksamkeit statt Globalität\nAllianzen und Feindschaften"
  },
  {
    "objectID": "lectures/md/KED2023_01.html#what-you-learn",
    "href": "lectures/md/KED2023_01.html#what-you-learn",
    "title": "The ABC of Computational Text Analysis",
    "section": "What you learn",
    "text": "What you learn\n\ncomputationally analyze, interpret, and visualize texts\n\ncommand line + Python\n\ndigital literacy + scholarship\nproblem-solving capacity\n\n\n\nText als Datenform → Textanalyse\ntechnisch: CL + Python Sprachen\nTools/Arbeitsweisen für Alltag/Forschen/Arbeit\n\nLeben vereinfachen\nDinge, die ich spät gelernt habe\n\nProblemlöseverhalten\n\nNachlesen & Ausprobieren"
  },
  {
    "objectID": "lectures/md/KED2023_01.html#learnings-from-previous-courses",
    "href": "lectures/md/KED2023_01.html#learnings-from-previous-courses",
    "title": "The ABC of Computational Text Analysis",
    "section": "Learnings from previous Courses",
    "text": "Learnings from previous Courses\n\ntoo much content, too little practice\nprogramming can be overwhelming\nlearning by doing, doing by googling\n\n\n\neigene Erfahrung als Studi und Tutor\nZu ambitioniert + an Bedürfnissen vorbei\nzu einseitig Programmieren\nbraucht viel Übung\nKein Einzelfallwissen, sondern Selbsthilfe\nlearnbyexample\n\nDoing is often better than thinking of doing"
  },
  {
    "objectID": "lectures/md/KED2023_01.html#levels-of-proficiency",
    "href": "lectures/md/KED2023_01.html#levels-of-proficiency",
    "title": "The ABC of Computational Text Analysis",
    "section": "Levels of Proficiency",
    "text": "Levels of Proficiency\n\nawareness of today’s computational potential\nanalyzing existing datasets\ncreating + analyzing new datasets\napplying advanced machine learning\n\n\n\nKompetenzstufen\nComputer nicht nur Word + Youtube, sondern auch Werkzeug\nZiel: Stufe 3"
  },
  {
    "objectID": "lectures/md/KED2023_01.html#what-i-teach",
    "href": "lectures/md/KED2023_01.html#what-i-teach",
    "title": "The ABC of Computational Text Analysis",
    "section": "What I teach",
    "text": "What I teach\n\ncomputational practises\ncritical perspective on technology\nlecture-style introductions\nhands-on coding sessions\ndiscussions + experiments in groups\n\n\n\nAufbau Unterricht\nSeminar heisst interaktiv\nMix zwischen Vorlesung, Diskutieren & Experimentieren"
  },
  {
    "objectID": "lectures/md/KED2023_01.html#topics",
    "href": "lectures/md/KED2023_01.html#topics",
    "title": "The ABC of Computational Text Analysis",
    "section": "Topics",
    "text": "Topics\n\n\n\nTechniques\n\ntext processing\nextracting and aggregating information\ncreating simple visualizations\noptical character recognition (OCR)\nscraping files\n\n\n\n\nData\n\nusing existing datasets\ncreating new datasets\n\n\n\n\n\n\n:nerd_face: inputs are more than welcome!\n\n\n\nTechniken & Resourcen\nText zu Daten machen und auswerten\n\nganzer Arbeitsprozess von PDF bis zur Visualisierung\n\nInputs"
  },
  {
    "objectID": "lectures/md/KED2023_01.html#provisional-schedule",
    "href": "lectures/md/KED2023_01.html#provisional-schedule",
    "title": "The ABC of Computational Text Analysis",
    "section": "Provisional Schedule",
    "text": "Provisional Schedule\n\n\n\nDate\nTopic\n\n\n\n\n03 March 2023\nIntroduction + Where is the digital revolution?\n\n\n10 March 2023\nText as Data\n\n\n17 March 2023\nSetting up your Development Environment\n\n\n24 March 2023\nIntroduction to the Command-line\n\n\n31 March 2023\nBasic NLP with Command-line\n\n\n07 April 2023\nLearning Regular Expressions\n\n\n14 April 2023\nWorking with (your own) Data\n\n\n21 April 2023\nno lecture (Osterpause)\n\n\n28 April 2023\nEthics and the Evolution of NLP\n\n\n05 May 2023\nIntroduction to Python\n\n\n12 May 2023\nNLP with Python\n\n\n19 May 2023\nNLP with Python + Working Session\n\n\n26 May 2023\nno lecture (Christi Himmelfahrt)\n\n\n02 June 2023\nMini-Project Presentations + Discussion\n\n\n\n\n\n12 Sitzungen, vorläufiger Plan\nÜberblicksitzungen\nGemeinsame Installation → immer wieder Probleme\nKommandozeile\nDaten\nSitzung mit aktueller NLP und Ethik\nPython\nMini-Projekt"
  },
  {
    "objectID": "lectures/md/KED2023_01.html#tldr-rocket",
    "href": "lectures/md/KED2023_01.html#tldr-rocket",
    "title": "The ABC of Computational Text Analysis",
    "section": "TL;DR :rocket:",
    "text": "TL;DR :rocket:\nYou will be tech-savvy…\n…yet no programmer applying fancy machine learning\n\n\nNicht ML, aber solide Analyse\nEmpirische Arbeiten schreiben\nbereits sehr effektiv und toll"
  },
  {
    "objectID": "lectures/md/KED2023_01.html#requirements",
    "href": "lectures/md/KED2023_01.html#requirements",
    "title": "The ABC of Computational Text Analysis",
    "section": "Requirements",
    "text": "Requirements\n\nno technical skills required :white_check_mark:\n\nself-contained course\n\nlaptop (macOS, Win10, Linux) :computer:\n\nupdate system\nfree up at least 15GB storage\nbackup files"
  },
  {
    "objectID": "lectures/md/KED2023_01.html#grading-writing_hand",
    "href": "lectures/md/KED2023_01.html#grading-writing_hand",
    "title": "The ABC of Computational Text Analysis",
    "section": "Grading :writing_hand:",
    "text": "Grading :writing_hand:\n\n3 exercises during semester\n\nno grades (pass/fail)\n\nmini-project with presentation\n\nbackup claims with numbers\nwork in teams\ndata of your interest\n\noptional: writing a seminar paper\n\nin cooperation with Prof. Sophie Mützel\n\n\n\n\nSeminar üblicherweise mit Vorträgen\n\nHier: 3 Übungen + Mini-Projekt\n\nDenkt über Daten nach → mehr Spass, wenn Interesse daran\nAuseinandersetzung fördern, keine harte Beurteilung\nTeamarbeit\nKollaboration Mützel"
  },
  {
    "objectID": "lectures/md/KED2023_01.html#organization",
    "href": "lectures/md/KED2023_01.html#organization",
    "title": "The ABC of Computational Text Analysis",
    "section": "Organization",
    "text": "Organization\n\nseminar on Thursday from 2.15pm - 4.00pm\ncourse website KED2023 with slides + information\nreadings on OLAT\ncommunication on OLAT Forum\n\nforum for everything except personal\nsubscribe to notifications\ndirect: alex.flueckiger@doz.unilu.ch\n\n\n\n\nPräsentation/Daten über Kurs-Website\n\nzeigen\n\nalte Sessions sind über noch verfügbar über SWITCHtube\nForum für Fragen, Benachrichtigung einrichten\nPapers OLAT\nAbmelden vom Seminar"
  },
  {
    "objectID": "lectures/md/KED2023_01.html#who-are-you",
    "href": "lectures/md/KED2023_01.html#who-are-you",
    "title": "The ABC of Computational Text Analysis",
    "section": "Who are you?",
    "text": "Who are you?\nPlease fill out this questionnaire\n\n:memo:\n\n\nWebsite lesen\nReading\nFragen"
  },
  {
    "objectID": "lectures/md/KED2023_01.html#reading",
    "href": "lectures/md/KED2023_01.html#reading",
    "title": "The ABC of Computational Text Analysis",
    "section": "Reading",
    "text": "Reading\n\nRequired\nLazer, David, Alex Pentland, Lada Adamic, Sinan Aral, Albert-László Barabási, Devon Brewer, Nicholas Christakis, Noshir Contractor, James Fowler, Myron Gutmann, Tony Jebara, Gary King, Michael Macy, Deb Roy, and Marshall Van Alstyne. 2009. “Computational Social Science.” Science 323(5915):721–23.\n(via OLAT)\n\n\nOptional\nGraham, Shawn, Ian Milligan, and Scott Weingart. 2015. Exploring Big Historical Data: The Historian’s Macroscope. Open Draft Version. Under contract with Imperial College Press.\nonline"
  },
  {
    "objectID": "lectures/md/KED2023_01.html#references",
    "href": "lectures/md/KED2023_01.html#references",
    "title": "The ABC of Computational Text Analysis",
    "section": "References",
    "text": "References"
  },
  {
    "objectID": "lectures/md/KED2023_10.html",
    "href": "lectures/md/KED2023_10.html",
    "title": "The ABC of Computational Text Analysis",
    "section": "",
    "text": "working with VS Code Editor\nlearning programming concepts & syntax\n\ndata types, loops, indexing…"
  },
  {
    "objectID": "lectures/md/KED2023_10.html#outline",
    "href": "lectures/md/KED2023_10.html#outline",
    "title": "The ABC of Computational Text Analysis",
    "section": "Outline",
    "text": "Outline\n\nget the organizational stuff done\n\nevaluation, mini-project, assignment #3\n\nlet’s do serious NLP! :sparkles:\ncode interactively\n\ninterrupt, ask, and complement\n\n\n\n\nFragen sehr wichtig, da gewaltiger Sprung zu letzter Session\nStellschrauben kennen lernen, Gefühl für die Sprache entwickeln\n\nDetails vorerst unwichtig\n\ndritt letzte Sitzung, neue Inhalte nur noch heute und nächstes Mal\nstream via Zoom"
  },
  {
    "objectID": "lectures/md/KED2023_10.html#course-evaluation",
    "href": "lectures/md/KED2023_10.html#course-evaluation",
    "title": "The ABC of Computational Text Analysis",
    "section": "Course Evaluation",
    "text": "Course Evaluation"
  },
  {
    "objectID": "lectures/md/KED2023_10.html#tell-me-mega",
    "href": "lectures/md/KED2023_10.html#tell-me-mega",
    "title": "The ABC of Computational Text Analysis",
    "section": "Tell me… :mega:",
    "text": "Tell me… :mega:\n\nPlease follow the link in the email\n\nreceived on 9 May 2023 (or similar)\nby the University of Lucerne, Faculty of Humanities and Social Sciences\n\n\n\n\nThanks for any constructive feedback,  be it sweet or sour! :pray:\n\n\noffene Kommentare nutzen, statt nur Kreuze\n5min Zeit zum ausfüllen\nandere Code anschauen\nTell me…\n\n… what you disliked\n… what you missed\n… what you learned"
  },
  {
    "objectID": "lectures/md/KED2023_10.html#assignment-3-writing_hand",
    "href": "lectures/md/KED2023_10.html#assignment-3-writing_hand",
    "title": "The ABC of Computational Text Analysis",
    "section": "Assignment #3 :writing_hand:",
    "text": "Assignment #3 :writing_hand:\n\nget/submit via OLAT\n\nstarting tomorrow\ndeadline 20 May 2023, 23:59\n\nuse the OLAT forum\n\nsubscribe to get notifications\n\nask friends for support, not solutions"
  },
  {
    "objectID": "lectures/md/KED2023_10.html#requirements-of-mini-project",
    "href": "lectures/md/KED2023_10.html#requirements-of-mini-project",
    "title": "The ABC of Computational Text Analysis",
    "section": "Requirements of Mini-Project",
    "text": "Requirements of Mini-Project\n\npresent project on 2 June 2023\n\nanalyze any collection of documents\n\ncompare historically\ncompare between actors\n\napply quantitative measures + interpretation\n\nexecutable script\nmultiple documents\n\nform groups of 2-4 people\n\n:exclamation: share your project idea here by 19 May 2023\n\n\nmini project online stellen?\nnoch nicht alle in Liste?\n“Forschungsfrage” überlegen\nscript ist gemachte arbeit zur wiederverwendung\nrelative frequency :thumbsup:\nabsolute frequency :thumbsdown:"
  },
  {
    "objectID": "lectures/md/KED2023_10.html#optional-seminar-paper",
    "href": "lectures/md/KED2023_10.html#optional-seminar-paper",
    "title": "The ABC of Computational Text Analysis",
    "section": "Optional Seminar Paper",
    "text": "Optional Seminar Paper\n\nwriting a seminar paper (6 ECTS)\nget in touch to discuss your idea"
  },
  {
    "objectID": "lectures/md/KED2023_10.html#what-is-a-word",
    "href": "lectures/md/KED2023_10.html#what-is-a-word",
    "title": "The ABC of Computational Text Analysis",
    "section": "What is a Word?",
    "text": "What is a Word?\n\nwords ~ segments between whitespace\nyet, there are …\n\ncontractions: U.S., don't\ncollocations: New York"
  },
  {
    "objectID": "lectures/md/KED2023_10.html#token",
    "href": "lectures/md/KED2023_10.html#token",
    "title": "The ABC of Computational Text Analysis",
    "section": "Token",
    "text": "Token\n\n\n\ntoken ~ computational unit\n\nrepresentation of words\n\nlemma ~ base form of a word\n\ntexts → text\ngoes → go\n\nstop words ~ functional words\n\nlacking deeper meaning\nthe, a, on, and …\n\n\n\n\n\n\nTokenizing a sentence (Medium)\n\n\n\n\n\nLet's tokenize this sentence! Isn't is easy? :nerd_face:\n\n\nText wird in seine Teile gesplittet\nLemma ~ Form in Duden"
  },
  {
    "objectID": "lectures/md/KED2023_10.html#common-processing-steps-in-nlp",
    "href": "lectures/md/KED2023_10.html#common-processing-steps-in-nlp",
    "title": "The ABC of Computational Text Analysis",
    "section": "Common Processing Steps in NLP",
    "text": "Common Processing Steps in NLP\n\n\n\nTokenizing\n\nsegmenting text into words, punctuations etc.\n\nTagging part-of-speech (POS)\n\nassigning word types (e.g. verb, noun)\n\nParsing\n\ndescribing syntactic relations\n\nNamed Entity Recognition (NER)\n\norganizations, persons, locations, time etc.\n\n\n:nerd_face: Catch up on NLP with @Jurafskyforthcominga\n\n\n\n\n\n\nAbfolge von Prozessierungsschritten\nalles sprachabhängig\nTagging: ca. 20-30 POS tags\nParsing\n\nVerb ist Head von Satz\nSubjekt-Verb-Objekt"
  },
  {
    "objectID": "lectures/md/KED2023_10.html#modulespackages",
    "href": "lectures/md/KED2023_10.html#modulespackages",
    "title": "The ABC of Computational Text Analysis",
    "section": "Modules/Packages",
    "text": "Modules/Packages\n\nNo programming from scratch :tada:\n\npackages provide specific functionalities\npackages need to be installed first\n\n\n\nStanding on the shoulders of giants\nalles auf GitHub"
  },
  {
    "objectID": "lectures/md/KED2023_10.html#nlp-packages",
    "href": "lectures/md/KED2023_10.html#nlp-packages",
    "title": "The ABC of Computational Text Analysis",
    "section": "NLP Packages",
    "text": "NLP Packages\n\nspaCy\n\nindustrial-strength Natural Language Processing (NLP)\n\ntextaCy\n\nNLP, before and after spaCy\n\nscattertext\n\nbeautiful visualizations of how language differs across corpora"
  },
  {
    "objectID": "lectures/md/KED2023_10.html#deep-dive-into-nlp-for-social-science",
    "href": "lectures/md/KED2023_10.html#deep-dive-into-nlp-for-social-science",
    "title": "The ABC of Computational Text Analysis",
    "section": "Deep Dive into NLP for Social Science​​",
    "text": "Deep Dive into NLP for Social Science​​\n\ncheck code on GitHub\nrun code on Binder"
  },
  {
    "objectID": "lectures/md/KED2023_10.html#resources",
    "href": "lectures/md/KED2023_10.html#resources",
    "title": "The ABC of Computational Text Analysis",
    "section": "Resources",
    "text": "Resources\n\ntutorials on spaCy\n\nofficial spaCy 101\nofficial online course spaCy\nHitchhiker’s Guide to NLP in spaCy"
  },
  {
    "objectID": "lectures/md/KED2023_10.html#references",
    "href": "lectures/md/KED2023_10.html#references",
    "title": "The ABC of Computational Text Analysis",
    "section": "References",
    "text": "References"
  },
  {
    "objectID": "lectures/md/KED2023_09.html",
    "href": "lectures/md/KED2023_09.html",
    "title": "The ABC of Computational Text Analysis",
    "section": "",
    "text": "from words to embeddings\n\nrecontextualized word meaning\n\ndata-driven NLP is both powerful and biased\ndata is never raw but depends on many decisions\n\n\n\nDaten haben eine Geschichte\n\nGeschichte der Gesellschaft"
  },
  {
    "objectID": "lectures/md/KED2023_09.html#outline",
    "href": "lectures/md/KED2023_09.html#outline",
    "title": "The ABC of Computational Text Analysis",
    "section": "Outline",
    "text": "Outline\n\nenter the shiny world of Python :sunglasses:\n\nprogramming basics\ndevelopment editor\n\nthink about mini-project\n\n\n\nziemlich trockener Einstieg in Python. Notwendig für"
  },
  {
    "objectID": "lectures/md/KED2023_09.html#python-is",
    "href": "lectures/md/KED2023_09.html#python-is",
    "title": "The ABC of Computational Text Analysis",
    "section": "Python is …",
    "text": "Python is …\n\n\n\n\na programming language that is …\n\ngeneral-purpose\n\nnot specific to any domain\n\ninterpreted\n\nno compiling\n\nstandard language in data science\n\n\n\n\n\n\nPopular programming languages src\n\n\n\n\n\n\nPython vs. R\n\nPython breiter einsetzbar\nmeiste Tools für beide Sprachen"
  },
  {
    "objectID": "lectures/md/KED2023_09.html#how-to-learn-programming",
    "href": "lectures/md/KED2023_09.html#how-to-learn-programming",
    "title": "The ABC of Computational Text Analysis",
    "section": "How to learn programming?",
    "text": "How to learn programming?\n\nThree inconvenient truths :cold_sweat:\n\nprogramming cannot be learnt in a course\n\nI try to make the start as easy as possible!\n\nfrustration is part of learning\n\nfight your way!\n\nthe Python ecosystem is huge\n\ngrow skills by step-by-step\n\n\n\nProgramming can be absolutely captivating! :v:\n\n\nEs wäre gelogen zu sagen, Programmieren sei einfach\nWelt des Programmierens ist riesig\n\nes gibt mehr möglichkeiten/einschränkungen als ich zeigen kann\n\nvieles wird nicht unmittelbar klar\n\nlernen aus Fehlern, wenn sie passieren\n\nProgrammbeispiele sind möglichst kurz / einfach geschrieben\nEs gibt auch Positives\n\nunmittelbares feedback\ntolles Gefühl, wenns klappt\nselbständiges Lernen is gut möglich"
  },
  {
    "objectID": "lectures/md/KED2023_09.html#variables",
    "href": "lectures/md/KED2023_09.html#variables",
    "title": "The ABC of Computational Text Analysis",
    "section": "Variables",
    "text": "Variables\n\nVariables are kind of storage boxes\n# define variables\nx = \"at your service\"\ny = 2\nz = \", most of the time.\"\n\n# combine variables\nint_combo = y * y       # for numbers any mathematical operation\nstr_combo = x + z       # for text only concatenation with +\n\n# show content of variable\nprint(str_combo)\n\n\nVariablen sind wie Schachteln oder ein Tupperware\n\netwas (Objekte) reinstecken\nbei Gebrauch hervorholen\nZuweisung/Zugriff\n\nNamenskonvention von Variablen: Kleinschreibung, English, underscore statt space"
  },
  {
    "objectID": "lectures/md/KED2023_09.html#data-types",
    "href": "lectures/md/KED2023_09.html#data-types",
    "title": "The ABC of Computational Text Analysis",
    "section": "Data Types",
    "text": "Data Types\n\nThe type defines the object’s properties\n\n\n\n\n\n\n\n\n\nName\nWhat for?\nType\nExamples\n\n\n\n\nString\nText\nstr\n\"Hi!\"\n\n\nInteger, Float\nNumbers\nint, float\n20, 4.5\n\n\nBoolean\nTruth values\nbool\nTrue, False\n\n\n⋮\n⋮\n⋮\n⋮\n\n\nList\nList of items (ordered, mutable)\nlist\n[\"Good\", \"Afternoon\", \"Everybody\"]\n\n\nTuple\nList of items (ordered, immutable)\ntuple\n(1, 2)\n\n\nDictionary\nRelations of items (unordered, mutable)\ndict\n{\"a\":1, \"b\": 2, \"c\": 3}\n\n\n\n\n\nes gibt verschiedene Aufbewahrungsboxen\nNamen unabhängig von Programmiersprache\nje nach Typ andere Eigenschaften\nder Typ ist immer implizit (dynamisch), nicht angeben bei Zuweisung\nListe ist eine grosse Box, die kleine Boxen aufnehmen kann"
  },
  {
    "objectID": "lectures/md/KED2023_09.html#data-type-conversion",
    "href": "lectures/md/KED2023_09.html#data-type-conversion",
    "title": "The ABC of Computational Text Analysis",
    "section": "Data Type Conversion",
    "text": "Data Type Conversion\n\nCombine variables of the same type only\n# check the type\ntype(YOUR_VARIABLE)\n\n# convert types (similar for other types)\nint('100')  # convert to integer\nstr(100)    # convert to string\n\n# easiest way to use a number in a text\nx = 3\nmixed = f\"x has the value: {x}\"\nprint(mixed)\n\n\nAnalogie: nur gleiche Schachteln können kombiniert werden"
  },
  {
    "objectID": "lectures/md/KED2023_09.html#confusing-equal-sign",
    "href": "lectures/md/KED2023_09.html#confusing-equal-sign",
    "title": "The ABC of Computational Text Analysis",
    "section": "Confusing Equal-Sign",
    "text": "Confusing Equal-Sign\n\n= vs. == contradicts the intuition\n# assign a value to a variable\nx = 1\nword = \"Test\"\n\n# compare two values if they are identical\n1 == 2              # False\nword == \"Test\"      # True\n\n\n= mit ungewohnter Funktion\n= Zuweisungsoperator\n\nin Schule gelernt: Entsprechung linker + rechter Seite\nR nutzt <-\n\n== Vergleichsoperator"
  },
  {
    "objectID": "lectures/md/KED2023_09.html#comments",
    "href": "lectures/md/KED2023_09.html#comments",
    "title": "The ABC of Computational Text Analysis",
    "section": "Comments",
    "text": "Comments\n\nlines ignored by Python\nwrite comments, it helps you …\n\nto learn initially\nto understand later\n\n\n# single line comment\n\n\"\"\"\ncomment across \nmultiple \nlines\n\"\"\""
  },
  {
    "objectID": "lectures/md/KED2023_09.html#visual-studio-code",
    "href": "lectures/md/KED2023_09.html#visual-studio-code",
    "title": "The ABC of Computational Text Analysis",
    "section": "Visual Studio Code",
    "text": "Visual Studio Code\n\nThe (best) editor to program in Python\n\n\n\nintegrated development environment (IDE)\n\ninteractive development\nsimilar to RStudio\n\n3 views in editor\n\nprogramming (left)\noutput (right)\nadditional information (bottom)\n\nuse tab for autocompletion\n\n\n\n\n\nVisual Studio Code\n\n\n\n\n\n\ndemonstration\n\nscript\nvariable explorer\ntab for completion\ncheck for WSL and tabnine support"
  },
  {
    "objectID": "lectures/md/KED2023_09.html#in-class-run-your-first-python-program-i",
    "href": "lectures/md/KED2023_09.html#in-class-run-your-first-python-program-i",
    "title": "The ABC of Computational Text Analysis",
    "section": "In-class: Run your first Python Program I",
    "text": "In-class: Run your first Python Program I\n\nMake sure that your local copy of the Github repository KED2023 is up-to-date with git pull.\nOpen the Visual Studio Editor.\nWindows User only: Make sure that you are connected to WSL: Ubuntu (green badge lower-left corner, see image on the next slide). If not, click on the badge and select New WSL Window.\nCreate a new file with the following content, save it as hello_world.py. Then, execute it by a right click on the code and select Run current file in interactive window.\n# print out a message\nmsg = \"Hello World!\"\nprint(msg)\nDoes the output looks like the screenshot on the next slide? If the execution doesn’t work as expected, ask me or your neighbour. There might be a technical issue."
  },
  {
    "objectID": "lectures/md/KED2023_09.html#in-class-run-your-first-python-program-i-1",
    "href": "lectures/md/KED2023_09.html#in-class-run-your-first-python-program-i-1",
    "title": "The ABC of Computational Text Analysis",
    "section": "In-class: Run your first Python Program I",
    "text": "In-class: Run your first Python Program I"
  },
  {
    "objectID": "lectures/md/KED2023_09.html#iterations",
    "href": "lectures/md/KED2023_09.html#iterations",
    "title": "The ABC of Computational Text Analysis",
    "section": "Iterations",
    "text": "Iterations\n\nfor-loop\ndo something with each element of a collection\nsentence = ['This', 'is', 'a', 'sentence']\n\n# iterate over each element\nfor token in sentence:\n    \n    # do something with the element\n    print(token)    \n\n\nAnwendungsfall\n\nfür viele Objekte das gleiche machen\n\nObjekt innerhalb loop bearbeiten\nsentence + token sind Variablen\n\ntoken ist temporäre Variable (Inhalt ändert in loop)"
  },
  {
    "objectID": "lectures/md/KED2023_09.html#conditionals",
    "href": "lectures/md/KED2023_09.html#conditionals",
    "title": "The ABC of Computational Text Analysis",
    "section": "Conditionals",
    "text": "Conditionals\n\nif-else statement\ncondition action on variable content\nsentence = ['This', 'is', 'a', 'sentence']\n\nif len(sentence) < 3:\n    print('This sentence is shorter than 3 tokens')\nelif len(sentence) == 3:\n    print('This sentence has exactly 3 tokens')\nelse:\n    print('This sentence is longer than 3 tokens')\n\n\nAnwendungsfall\n\nAktion abhängig machen von Variableninhalt\n\nFrage: Was wird ausgegeben?\nFunktionen für Länge und Print"
  },
  {
    "objectID": "lectures/md/KED2023_09.html#indentation-matters",
    "href": "lectures/md/KED2023_09.html#indentation-matters",
    "title": "The ABC of Computational Text Analysis",
    "section": "Indentation matters!",
    "text": "Indentation matters!\n\nintend code within code blocks\n\nloops, if-statements etc.\n\npress tab to intend\n\n\n\n\n:white_check_mark:\nif 5 > 2:\n    print('5 is greater than 2')\n\n:x:\nif 5 > 2:\nprint('5 is greater than 2')\n\n\n\n\nshift + tab für unintend\ntab wird umgewandelt in 4 spaces"
  },
  {
    "objectID": "lectures/md/KED2023_09.html#methods",
    "href": "lectures/md/KED2023_09.html#methods",
    "title": "The ABC of Computational Text Analysis",
    "section": "Methods",
    "text": "Methods\n\nDo somethin with an object\nsentence = 'This is a sentence'\n\n# split at whitespace\ntokens = sentence.split(' ')    \n\n# check the variables\nprint(sentence, type(sentence), tokens, type(tokens))           \n\n# add something to a list\ntokens.append('.')\n\n# concatenate elements to string\ntokens = ' '.join(tokens)\nprint(tokens, type(tokens))\n\n\njedes Objekt/Variable stellt Methoden zur Verfügung"
  },
  {
    "objectID": "lectures/md/KED2023_09.html#functions-and-arguments",
    "href": "lectures/md/KED2023_09.html#functions-and-arguments",
    "title": "The ABC of Computational Text Analysis",
    "section": "Functions and Arguments",
    "text": "Functions and Arguments\n\nDRY: Don’t Repeat Yourself\n\nfunctions have a name and optional arguments\n\nfunction_name(arg1, ..., argn)\n\n\n# define a new function\ndef get_word_properties(word):\n    \"\"\"\n    My first function to print word properties.\n    It takes any string as argument (variable: word).\n    \"\"\"\n    \n    # print(), len() and sorted() work also as functions  \n    length = len(word)\n    sorted_letters = sorted(word, reverse=True)\n    print(word, 'length:', length, 'letters:', sorted_letters)\n\nget_word_properties('computer') # call function with any word\n\n\nAnwendungsfall\n\nCode strukturieren, Redundanzen vermeiden\n\nzwei Teile: definieren + aufrufen\nFunktion mit vordefinierten Argumenten"
  },
  {
    "objectID": "lectures/md/KED2023_09.html#indexing",
    "href": "lectures/md/KED2023_09.html#indexing",
    "title": "The ABC of Computational Text Analysis",
    "section": "Indexing",
    "text": "Indexing\n\nComputers start counting from zero! :dizzy_face:\nsentence = ['This', 'is', 'a', 'sentence']\n\n# element at position X\nfirst_tok = sentence[0]     # 'This'\n\n# elements of subsequence [start:end]\nsub_seq = sentence[0:3]     # ['This', 'is', 'a']\n\n# elements of subsequence backwards\nsub_seq_back = sentence[-2:]        # ['a', 'sentence']\n\n\nAnwendungsfall\n\nnur ein Teil der Objekte auswählen"
  },
  {
    "objectID": "lectures/md/KED2023_09.html#errors",
    "href": "lectures/md/KED2023_09.html#errors",
    "title": "The ABC of Computational Text Analysis",
    "section": "Errors",
    "text": "Errors\n\nA myriad of things can go wrong\n\n\n\nread the message\nfind the source of the error\n\nscript name + line number\n\npaste message into Google\n\n\n\n\n\n\nLearning by doing, doing by googling\n\n\n\n\n\n\nproduce NameError, TypeError"
  },
  {
    "objectID": "lectures/md/KED2023_09.html#modulespackages",
    "href": "lectures/md/KED2023_09.html#modulespackages",
    "title": "The ABC of Computational Text Analysis",
    "section": "Modules/Packages",
    "text": "Modules/Packages\n\nNo programming from scratch :tada:\n\npackages provide specific functionalities\npackages need to be installed first\n\n\n\nStanding on the shoulders of giants\nalles auf GitHub"
  },
  {
    "objectID": "lectures/md/KED2023_09.html#nlp-packages",
    "href": "lectures/md/KED2023_09.html#nlp-packages",
    "title": "The ABC of Computational Text Analysis",
    "section": "NLP Packages",
    "text": "NLP Packages\n\nspaCy\n\nindustrial-strength Natural Language Processing (NLP)\n\ntextaCy\n\nNLP, before and after spaCy\n\nscattertext\n\nbeautiful visualizations of how language differs across corpora"
  },
  {
    "objectID": "lectures/md/KED2023_09.html#mini-project",
    "href": "lectures/md/KED2023_09.html#mini-project",
    "title": "The ABC of Computational Text Analysis",
    "section": "Mini-Project",
    "text": "Mini-Project\n\npresent project on 2 June 2023\n\nanalyze any collection of documents\napply quantitative measures + interpretation\n\ncompare historically\ncompare between actors\n\nform groups of 2-4 people\n\n\n\nwelche Daten das ist das wichtigste\nstart on 19 May in class"
  },
  {
    "objectID": "lectures/md/KED2023_09.html#in-class-exercises-i",
    "href": "lectures/md/KED2023_09.html#in-class-exercises-i",
    "title": "The ABC of Computational Text Analysis",
    "section": "In-class: Exercises I",
    "text": "In-class: Exercises I\n\nOpen the script with the basics of Python in your Visual Studio Editor:\nmaterials/code/python_basics.ipynb\nTry to understand the code in each cell and run them by clicking the play symbol left to them. Check the output. Modify some code as well as data and see how the output changes. Initially, this try-and-error is good strategy to learn. Some ideas:\n\nCombine a string and an integer variable without converting it. What error do you get? How can you avoid it?\nSelect is a from the list using the right index."
  },
  {
    "objectID": "lectures/md/KED2023_09.html#in-class-exercises-ii",
    "href": "lectures/md/KED2023_09.html#in-class-exercises-ii",
    "title": "The ABC of Computational Text Analysis",
    "section": "In-class: Exercises II",
    "text": "In-class: Exercises II\n\nWrite a Python script that\n\ntakes text (a string)\nsplits it into words (a list)\niterates over all the tokens and print all tokens that are longer than 5 characters\nBonus: wrap your code in a function.\n\nGo to the next slide. Start with some of the great interactive exercises out there in the web."
  },
  {
    "objectID": "lectures/md/KED2023_09.html#resources",
    "href": "lectures/md/KED2023_09.html#resources",
    "title": "The ABC of Computational Text Analysis",
    "section": "Resources",
    "text": "Resources\n\nlearn basics interactively\n\nPython Principles\nLearnPython\n\n\n\nofficial Python introduction\n\nPython introduction"
  },
  {
    "objectID": "lectures/md/KED2023_07.html",
    "href": "lectures/md/KED2023_07.html",
    "title": "The ABC of Computational Text Analysis",
    "section": "",
    "text": "describe text as pattern using RegEx\nextract + replace textual parts\n\nliteral: abc\nmeta: \\w \\s [^abc] *\npower of .*\n\n\n\n\nRegex für Extraktion + Säubern\n\nman muss nur ungefähr wissen wonach suchen\ngeneralisierte Form = Muster\n\nLiterale = Zeichen steht für tatsächliches Zeichen (buchstabentreu)\nMeta-Zeichen = Zeichen mit spezieller Bedeutung\nFragen zu RegEx oder Übung?"
  },
  {
    "objectID": "lectures/md/KED2023_07.html#outline",
    "href": "lectures/md/KED2023_07.html#outline",
    "title": "The ABC of Computational Text Analysis",
    "section": "Outline",
    "text": "Outline\n\nlearn about available data resources\nuse your own textual data\n\nany text :heavy_check_mark:\n“any” format :heavy_check_mark:\nfrom anywhere :heavy_check_mark:\n\n\n\n\nheute letzte Sitzung zu Kommandozeile\nzweiteilige Sitzung mit wenig Technischem\n\nexistierende Daten, eigene Daten\n\ninteressannte Datasets für die Sozialwissenschaften\n\nes gibt allerdings nicht viele\nzumeist eigene Daten präparieren\n\neigene Textdaten nutzen unabhängig von\n\nFormaten\nhistorischem Kontext (digital native)\n\nÜberlegen, was für eine Analyse in MiniProject"
  },
  {
    "objectID": "lectures/md/KED2023_07.html#what-data-sources-are-there",
    "href": "lectures/md/KED2023_07.html#what-data-sources-are-there",
    "title": "The ABC of Computational Text Analysis",
    "section": "What Data Sources are there?",
    "text": "What Data Sources are there?\n\nbroadly social\n\nnewspapers + magazines\nwebsites + social media\nreports by NGOs/GOs\n\nscientific articles\neconomic\n\nbusiness plans/reports\ncontracts\npatents\n\n\n:point_right: basically, any textual documents…\n\n\nunheimliche Vielfalt an Dokumenten, alles neue digital\nTextkollektion ist bereits vorhanden im Gegensatz zu Survey\n\ngiven data statt created data\nDatenbereinigungen sind nötig\n\nhier nur einen Bruchteil vorgestellt, soll als Inspiration dienen\n\nZeit reicht nicht, um auf alle Quellen einzugehen\n\n\nImage Credits"
  },
  {
    "objectID": "lectures/md/KED2023_07.html#interesting-publishers",
    "href": "lectures/md/KED2023_07.html#interesting-publishers",
    "title": "The ABC of Computational Text Analysis",
    "section": "Interesting Publishers",
    "text": "Interesting Publishers\n\nNexis Uni\n\nnewspaper, business + legal reports (international)\nlicensed by the university\n\nConstellate\n\nscientific articles of JSTOR across disciplines\nprovides an easy dataset builder\n\nHathiTrust and Project Gutenberg\n\nmassive collection of books (international)\nopen, HathiTrust requires agreement\n\n\n:point_right: check out other resources licensed by ZHB\n\n\nRessourcen gelistet auf ZHBLuzern\n\nZugang tlw. über ezproxy\n\nNexis vielleicht spannendste Quelle für Analyse soziale Probleme\nWieso Literatur? –> Zeitgeist\n\ngenderspezifische Sprache, Verweise Natur/Umwelt\n\nConstellate\n\nkurze Demo von Constellate\nbrandneue Platform\neinfache Zusammenstellung von JSTOR Artikeln\nsehr gute Metadaten\nauch gut für schnelle Recherchen ohne Download"
  },
  {
    "objectID": "lectures/md/KED2023_07.html#dataset-search",
    "href": "lectures/md/KED2023_07.html#dataset-search",
    "title": "The ABC of Computational Text Analysis",
    "section": "Dataset Search",
    "text": "Dataset Search\n\nHarvard Dataverse\n\nopen scientific data repository\n\nGoogle Dataset Search\n\nGoogle for datasets basically\n\ncorpora by the Department of Computational Linguistics @ UZH\n\n\n:point_right: search for a topic followed by corpus, text collection or text as data\n\n\nmoderne Wissenschaft veröffentlicht nicht nur Papers, sondern auch Daten\n\ncomputergestützte Textanalyse ist aber immer noch Nische\n\nSuchmaschinen für Datensätze\n\nallerlei Datensätze, primär aus Wissenschaft\n\nUZH hat Institut Computerlinguistik\n\nverschiedene Korpora\nCredit Suisse PDF Bulletin Corpus"
  },
  {
    "objectID": "lectures/md/KED2023_07.html#some-great-historical-corpora",
    "href": "lectures/md/KED2023_07.html#some-great-historical-corpora",
    "title": "The ABC of Computational Text Analysis",
    "section": "Some great historical Corpora",
    "text": "Some great historical Corpora\n\nready off-the-shelf, machine-readable\n\n1 August speeches by Swiss Federal Councilors\n\nprovided via course repo\n\nHuman Rights Reports by various NGOs\nUnited Nations General Debate Corpus\n\n\n\n:sweat: There are still not many.\n\n\nsehr wenige standardisierte Datasets\nnicht wie bei Survey-Forschung, numerischer Daten aus Politik und Ökonomie"
  },
  {
    "objectID": "lectures/md/KED2023_07.html#online-computational-text-analysis",
    "href": "lectures/md/KED2023_07.html#online-computational-text-analysis",
    "title": "The ABC of Computational Text Analysis",
    "section": "Online Computational Text Analysis",
    "text": "Online Computational Text Analysis\n\nImpresso\n\nmany historical newspapers + magazines (CH, LU)\nfree, requires account\n\nbookworm HathiTrust\n\ngreat filtering by metadata\ncredible scientific source\n\nGoogle Ngram Viewer\n\nno filtering option\nuseful for quick analysis\n\n\n\n\nDatenanalysen online durchführen\nAbsicherung über andere Quellen\nImpresso: Complete re-digitization of the NZZ (together with the Zurich Central Library and Swiss National Library)"
  },
  {
    "objectID": "lectures/md/KED2023_07.html#search-techniques",
    "href": "lectures/md/KED2023_07.html#search-techniques",
    "title": "The ABC of Computational Text Analysis",
    "section": "Search Techniques",
    "text": "Search Techniques\nMake your web search more efficient by using dedicated tags. Examples:\n\n\"computational social science\"\nnature OR environment\nsite:nytimes.com\n\n\n\nQuotes für Wörter die zusammen gehören\nBoolean Search\n\nOR / AND"
  },
  {
    "objectID": "lectures/md/KED2023_07.html#data-is-property-no_entry_sign",
    "href": "lectures/md/KED2023_07.html#data-is-property-no_entry_sign",
    "title": "The ABC of Computational Text Analysis",
    "section": "Data is property :no_entry_sign:",
    "text": "Data is property :no_entry_sign:\n\n… and has rights too\n\ncopyright may further limit access to high quality data\ncheck the rights before processing the data\n\n\n\n\nCopyrighs may restrict some data use (src)\n\n\n\n\nZugang zu Daten nicht immer einfach\n\nopen data unterschiedlich unterstützt\n\nDatenbereitstellung oftmals Teil von Geschäftsmodell\n\ndann restriktiv\n\noftmals ist Verwendung nicht geregelt\n\nnutzt Graubereich"
  },
  {
    "objectID": "lectures/md/KED2023_07.html#a-world-for-humans-and-a-jungle-of-file-formats.",
    "href": "lectures/md/KED2023_07.html#a-world-for-humans-and-a-jungle-of-file-formats.",
    "title": "The ABC of Computational Text Analysis",
    "section": "A world for humans …… and a jungle of file formats.",
    "text": "A world for humans …… and a jungle of file formats.\n\n\n\nextrem viele File-Typen\nmühsam, aber es gibt einfache Tools für Umwandlung"
  },
  {
    "objectID": "lectures/md/KED2023_07.html#common-conversions",
    "href": "lectures/md/KED2023_07.html#common-conversions",
    "title": "The ABC of Computational Text Analysis",
    "section": "Common Conversions",
    "text": "Common Conversions\n\nnews, press releases, reports from organizations\n\n\n:arrow_down:\ndigital native documents .pdf, .docx, .html\n:arrow_down:\nconvert to .txt\n\n:arrow_down:\nscans of (old) documents .pdf, .jpg, .png\n:arrow_down:\nOptical Character Recognition (OCR)\n\n\n\nmachine-readable :white_check_mark:\n\n\nPDF ist Publikationsstandard\n\nneue (digital) vs. alte (scans)\nKriterium: Suche möglich?\n\nanschliessend Schritte zur Umwandlung der wichtigsten Formate\nKeine Konzepte lernen, wie bei RegEx\n\nnur welches Tool, für welche Umwandlung\nmehr oder weniger copy-paste\n\nPause (etwas früher)"
  },
  {
    "objectID": "lectures/md/KED2023_07.html#conversion-of-docx",
    "href": "lectures/md/KED2023_07.html#conversion-of-docx",
    "title": "The ABC of Computational Text Analysis",
    "section": "Conversion of DOCX",
    "text": "Conversion of DOCX\n\nuse case: news articles from Nexis\n\npandoc to convert many file formats\ndownload as single articles in .docx on Nexis\n\n# convert docx to txt\npandoc infile.docx -o outfile.txt\n\n### Install first with\nbrew install pandoc     # macOS\nsudo apt install pandoc # Ubuntu\n\n\npandoc ist ein fast-alles Könner für Dokumentkonversion\n\nkann auch html konvertieren: pandoc slides/KED2023_01.html -t plain\n\nzusätzliche Installation\nNexis = News-Datenbank\n\nfreier Zugang ezproxy\nkennen ezproxy alle?"
  },
  {
    "objectID": "lectures/md/KED2023_07.html#conversion-of-native-pdf",
    "href": "lectures/md/KED2023_07.html#conversion-of-native-pdf",
    "title": "The ABC of Computational Text Analysis",
    "section": "Conversion of native PDF",
    "text": "Conversion of native PDF\n\nuse case: Swiss party programmes\n\npdftotext extracts text from non-scanned PDF\n\n# convert native pdf to txt\npdftotext -nopgbrk -eol unix infile.pdf\n\n### Install first with\nbrew install poppler            # macOS\nsudo apt install poppler-utils  # Ubuntu\n\n\npdftotext: Name ist Programm\n\nOutputfilename kann nicht spezifiziert werden\n\ndieselben Parteiprogramme, die wir schon analysiert haben\nLayout kann Extraktion erschweren\n\nSpalten/Tabelle\n\nHäufigkeitsanalysen von Wörter sind robust, Struktur egal"
  },
  {
    "objectID": "lectures/md/KED2023_07.html#optical-character-recognition-ocr",
    "href": "lectures/md/KED2023_07.html#optical-character-recognition-ocr",
    "title": "The ABC of Computational Text Analysis",
    "section": "Optical Character Recognition (OCR)",
    "text": "Optical Character Recognition (OCR)\n\n\n\nOCR ~ convert images into text\n\nextract text from scans/images\n\ntesseract performs OCR\n\nlanguage-specific models\nsupports handwriting + Fraktur texts\n\nimage quality is crucial\n\n\n\n\n\nsteps when performing OCR (Wikipedia)\n\n\n\n\n\n\ntatsächlicher Buchstabe, nicht nur Bild davon\nZwischenschritt Verbesserung Kontrast, B/W\ntechnisch Deep-Learning, nicht weiter von Bedeutung\nfrüher teure Programme, heute sogar iPhone\n\nfür viele Dokumente jedoch nicht geeignet"
  },
  {
    "objectID": "lectures/md/KED2023_07.html#conversion-of-digitalized-pdf",
    "href": "lectures/md/KED2023_07.html#conversion-of-digitalized-pdf",
    "title": "The ABC of Computational Text Analysis",
    "section": "Conversion of digitalized PDF",
    "text": "Conversion of digitalized PDF\n\nuse-case: historical party programmes\n\nextract image from PDF + improve contrast\nrun optical character recognition (OCR) on the image\n\n# convert scanned pdf to tiff, control quality with parameters\nconvert -density 300 -depth 8 -strip -background white -alpha off \\\ninfile.pdf temp.tiff\n\n# run OCR for German (\"eng\" for English, \"fra\" for French etc.)\ntesseract -l deu temp.tiff file_out\n\n### Install first with\nbrew install imagemagick            # macOS\nsudo apt-get install imagemagick    # Ubuntu\n\n\nZwei Schritte: Bildumwandlung + OCR\ntesseract funktioniert für viele Bildformate\n\nnicht direkt für PDF\n\nBeispiel: Kassenbon fotografieren & mit Regex parsen\n\nWirtschaftswissenschaften: indexierter Warenkorb"
  },
  {
    "objectID": "lectures/md/KED2023_07.html#configure-imagemagick-properly",
    "href": "lectures/md/KED2023_07.html#configure-imagemagick-properly",
    "title": "The ABC of Computational Text Analysis",
    "section": "Configure ImageMagick properly",
    "text": "Configure ImageMagick properly\n\nWindows Ubuntu users need to execute the following\n# disable security policy for Windows\nsudo sed -i '/<policy domain=\"coder\" rights=\"none\" pattern=\"PDF\"/d' /etc/ImageMagick-6/policy.xml\n\n# increase memory limits\nsudo sed -i -E 's/name=\"memory\" value=\".+\"/name=\"memory\" value=\"8GiB\"/g' /etc/ImageMagick-6/policy.xml\nsudo sed -i -E 's/name=\"map\" value=\".+\"/name=\"map\" value=\"8GiB\"/g' /etc/ImageMagick-6/policy.xml\nsudo sed -i -E 's/name=\"area\" value=\".+\"/name=\"area\" value=\"8GiB\"/g' /etc/ImageMagick-6/policy.xml\nsudo sed -i -E 's/name=\"disk\" value=\".+\"/name=\"disk\" value=\"8GiB\"/g' /etc/ImageMagick-6/policy.xml"
  },
  {
    "objectID": "lectures/md/KED2023_07.html#lifehack-make-a-pdf-searchable",
    "href": "lectures/md/KED2023_07.html#lifehack-make-a-pdf-searchable",
    "title": "The ABC of Computational Text Analysis",
    "section": "#LifeHack: Make a PDF searchable",
    "text": "#LifeHack: Make a PDF searchable\n\nuse case: scanned book chapters\n# output searchable pdf instead of txt\nconvert -density 300 -depth 8 -strip -background white -alpha off -compress group4 \\\nfile_in.pdf temp.tiff\n\ntesseract -l deu temp.tiff file_out pdf\n\n\nOutput als PDF statt Text\nfür Suchen/Zitate rauskopieren\nconvert hier mit Kompression, da PDFs zu gross werden ansonsten"
  },
  {
    "objectID": "lectures/md/KED2023_07.html#scraping-pdf-from-websites",
    "href": "lectures/md/KED2023_07.html#scraping-pdf-from-websites",
    "title": "The ABC of Computational Text Analysis",
    "section": "Scraping PDF from Websites",
    "text": "Scraping PDF from Websites\n\nuse case: Swiss voting booklet\n\nwget to download any files from the internet\n\n# get a single file\nwget EXACT_URL\n\n# get all linked pdf from a single webpage\nwget --recursive --accept pdf -nH --cut-dirs=5 \\\n--ignore-case --wait 1 --level 1 --directory-prefix=data \\\nhttps://www.bk.admin.ch/bk/de/home/dokumentation/abstimmungsbuechlein.html\n\n# --accept FORMAT_OF_YOUR_INTEREST\n# --directory-prefix YOUR_OUTPUT_DIRECTORY\n\n\nbis hierher: Wie Daten in txt Format bringen\njetzt Download automatisieren\n\nVorteil: schneller systematischer Download & Dokumentation von Quellen\n\nHaupt-URL angeben, wo PDF verlinkt sind\nScraping von Blogs möglich über Python\n\nnicht Teil von Seminar\n\nnicht auf alle Argumente eingehen"
  },
  {
    "objectID": "lectures/md/KED2023_07.html#interesting-resources",
    "href": "lectures/md/KED2023_07.html#interesting-resources",
    "title": "The ABC of Computational Text Analysis",
    "section": "Interesting Resources",
    "text": "Interesting Resources\n\nParty Programmes across Europe\n\ncovers over 1000 parties from 1920 until today in over 50 countries\n\nSwiss voting booklets\n\nfrom 1978 until today\n\n1 August speeches by Swiss Federal Councillors\nNestlé Annual Reports\n… any organization of your interest :thumbsup:"
  },
  {
    "objectID": "lectures/md/KED2023_07.html#basics-of-batch-processing",
    "href": "lectures/md/KED2023_07.html#basics-of-batch-processing",
    "title": "The ABC of Computational Text Analysis",
    "section": "Basics of Batch Processing",
    "text": "Basics of Batch Processing\n\nperform the same operation on many files\n# loop over all txt files\nfor file in *.txt; do\n\n    # indent all commands in loop with a tab\n\n    # rename each file\n    # e.g. a.txt -> new_a.txt\n    mv $file new_$file\n\ndone\n\n\nBatch Processing = gleiche Operation durchführen für alle Files\nErklären von Loop/Schleife und Variable\n\nWildcard zur Selektion > Liste von Files > Variable\n\nfor-loop wichtiges Programmierkonzept\nTabulator fürs Einrücken"
  },
  {
    "objectID": "lectures/md/KED2023_07.html#perform-ocr-for-many-pdf",
    "href": "lectures/md/KED2023_07.html#perform-ocr-for-many-pdf",
    "title": "The ABC of Computational Text Analysis",
    "section": "Perform OCR for many PDF",
    "text": "Perform OCR for many PDF\nfor FILEPATH in *.pdf; do\n    # convert pdf to image\n    convert -density 300 $FILEPATH -depth 8 -strip \\\n    -background white -alpha off temp.tiff\n    \n    # define output name (remove .pdf from input)\n    OUTFILE=${FILEPATH%.pdf} \n    \n    # perform OCR on the tiff image\n    tesseract -l deu temp.tiff $OUTFILE\n    \n    # remove the intermediate tiff image\n    rm temp.tiff\n\ndone\n\n\nsehr ähnlich wie vorher, nur für jedes einzelne File jetzt"
  },
  {
    "objectID": "lectures/md/KED2023_07.html#preprocessing-regex",
    "href": "lectures/md/KED2023_07.html#preprocessing-regex",
    "title": "The ABC of Computational Text Analysis",
    "section": "Preprocessing → RegEx",
    "text": "Preprocessing → RegEx\n\n\n\nAufbereitung unterschiedlich aufwendig\nfür schnelle Analyse nicht notwendig\nnun alles da für Mini-Project, ausser wenn Lösung in Python"
  },
  {
    "objectID": "lectures/md/KED2023_07.html#in-class-exercises-i",
    "href": "lectures/md/KED2023_07.html#in-class-exercises-i",
    "title": "The ABC of Computational Text Analysis",
    "section": "In-class: Exercises I",
    "text": "In-class: Exercises I\n\nMake sure that your local copy of the Github repository KED2023 is up-to-date with git pull. Check out the data samples in materials/data and the scripts to extract their text in materials/code.\nInstall the missing tools with the commands given on the respective slides: pandoc, imagemagick, poppler.\nApply the commands to reproduce on the given data. Test them on your own data. Check the resources. Ask questions. Think about your mini-project."
  },
  {
    "objectID": "lectures/md/KED2023_07.html#in-class-exercises-ii",
    "href": "lectures/md/KED2023_07.html#in-class-exercises-ii",
    "title": "The ABC of Computational Text Analysis",
    "section": "In-class: Exercises II",
    "text": "In-class: Exercises II\n\nUse wget to download cogito and its predecessor uniluAKTUELL issues (PDF files) from the UniLu website. Start with downloading one issue first and then try to automatize the process to download all the listed issued using arguments for the wget command.\nConvert the cogito and uniluAKTUELL PDF files into TXT files using tesseract. Try with a single issue first and then write a loop to batch process all of them.\nWhat is the University of Lucerne talking about in its issues? Use the commands of the previous lectures to count the vocabulary.\nDo the same as in 3.), yet analyze the vocabulary of cogito and uniluAKTUELL issues separately. Does the language and topics differ between the two magazines?"
  },
  {
    "objectID": "lectures/md/KED2023_07.html#in-class-exercises-iii",
    "href": "lectures/md/KED2023_07.html#in-class-exercises-iii",
    "title": "The ABC of Computational Text Analysis",
    "section": "In-class: Exercises III",
    "text": "In-class: Exercises III\n\nUse wget to download a book from Project Gutenberg and count some things (e.g., good/bad, joy/sad).\nwget is a powerful tool. Have a look at its arguments and search for more examples in tutorials on the web."
  },
  {
    "objectID": "lectures/md/KED2023_07.html#resources",
    "href": "lectures/md/KED2023_07.html#resources",
    "title": "The ABC of Computational Text Analysis",
    "section": "Resources",
    "text": "Resources\n\nMake a more sophisticated script for PDF-to-TXT conversion\n\nErick Peirson. 2015. Tutorial: Text Extraction and OCR with Tesseract and ImageMagick - Methods in Digital and Computational Humanities - DigInG Confluence. online"
  },
  {
    "objectID": "lectures/md/KED2023_07.html#have-a-nice-easter-break",
    "href": "lectures/md/KED2023_07.html#have-a-nice-easter-break",
    "title": "The ABC of Computational Text Analysis",
    "section": "Have a nice Easter break!",
    "text": "Have a nice Easter break!"
  },
  {
    "objectID": "lectures/md/KED2023_11.html",
    "href": "lectures/md/KED2023_11.html",
    "title": "The ABC of Computational Text Analysis",
    "section": "",
    "text": "perform NLP with spaCy :abc:\n\nNamed Entities, linguistic information …\n\ncreate a textacy corpus from text files :bookmark_tabs:\nexport basic statistics :abacus:\n\n\n\nExport Vocabulary"
  },
  {
    "objectID": "lectures/md/KED2023_11.html#assignment-3",
    "href": "lectures/md/KED2023_11.html#assignment-3",
    "title": "The ABC of Computational Text Analysis",
    "section": "Assignment #3",
    "text": "Assignment #3\n:mega: When you cannot import the packages or load the model, reach out!"
  },
  {
    "objectID": "lectures/md/KED2023_11.html#outline",
    "href": "lectures/md/KED2023_11.html#outline",
    "title": "The ABC of Computational Text Analysis",
    "section": "Outline",
    "text": "Outline\n\nexport your own dataset as .csv :package:\nexplore + visualize :art:\n\nexplore corpus interactively\nvisualize word frequencies\n\nwork on mini-project\n\n\n\nKrönung endlich ein bisschen Farbe in die grauen Zahlen\n\nVisualisierung kein Selbstzweck: effektivere Analyse\n\n1 Lektion Theorie\nFragestunde / Projektunterstützung"
  },
  {
    "objectID": "lectures/md/KED2023_11.html#mini-project-presentations",
    "href": "lectures/md/KED2023_11.html#mini-project-presentations",
    "title": "The ABC of Computational Text Analysis",
    "section": "Mini-Project Presentations",
    "text": "Mini-Project Presentations\n\n~8 projects\npresentations on 2 June 2023\n\n8 minutes per group\n+1 min preparation\n+1 min feedback\n\n\n\n\nhartes Zeitmaximum\nnicht viel, daher kurz und prägnant"
  },
  {
    "objectID": "lectures/md/KED2023_11.html#prepare-presentation",
    "href": "lectures/md/KED2023_11.html#prepare-presentation",
    "title": "The ABC of Computational Text Analysis",
    "section": "Prepare Presentation",
    "text": "Prepare Presentation\n\ntalk, slides, and plots …\n\nMotivation: What did you explore?\nData & Methods: What data and methods did you use?\nResults & Discussion: What did you find out?\n\nconnect your laptop with projector\n\n\n:chart_with_upwards_trend: visualize to drive your point home\n\n\nProjekt online stellen?"
  },
  {
    "objectID": "lectures/md/KED2023_11.html#continue-our-journey-on-nlp-for-social-science",
    "href": "lectures/md/KED2023_11.html#continue-our-journey-on-nlp-for-social-science",
    "title": "The ABC of Computational Text Analysis",
    "section": "Continue our Journey on NLP for Social Science",
    "text": "Continue our Journey on NLP for Social Science\n\ncheck code on GitHub\nrun code on Binder \n\n\n\nvergleichende Analysen\nAnwendungsfall scattertext\n\nWie unterscheidet sich die Sprache zwischen zwei Gruppen von Dokumenten?\nbedeutende Wörter in Korpus finden\nzeige interaktiv\n\nAnwendungsfall word frequencies\n\nWörter + Themen haben Konjunkturzyklen\nFluktuation grösser, je weniger Dokumente (Zufallsgründe)\n\nMetapher zu vectorizer\n\nDinge in Säcke abfüllen (Reis, Mais)\nInstruktionen zum Wägen/Zählen für vectorizer\nfit() startet Abfüllprozess\n\nunterschiedliche Funktionen zum Einlesen\n\ncsv dataset (Metadaten fix)\neinzelne txt files in Ordner"
  },
  {
    "objectID": "lectures/md/KED2023_02.html",
    "href": "lectures/md/KED2023_02.html",
    "title": "The ABC of Computational Text Analysis",
    "section": "",
    "text": "recap + reading\nmethodological foundation :grimacing:\nfirst computational text analysis\n\n\n\nkurzfristige Umstellung auf Zoom\nDiskussion Survey Seminarerwartungen\n\n3/4 mit R-Erfahrung, wenige mit Python und Kommandozeile\nSoCom Leute mit Inhaltsanalyse, keine Daten in Aussicht\nEinführungskurs, aber komplementäres Wissen und Pointers für Fortgeschrittene\nSkills für Seminar/BA-Arbeiten\nallgemeine Programmier- und Computerkenntnisse\n\nFragen zu Inhalt/Website?\nDiskussion letzte Sitzung + Paper\nHauptteil: Bedeutung/Grundlage von Textanalyse\n\nAuf welcher methodischen Grundlage steht das Feld?\nQualitativer Anteil gegenüber letzten Sitzung herausheben\n\nzweite Lektion: erste Textanalyse\n\neinfach, aber mächtig"
  },
  {
    "objectID": "lectures/md/KED2023_02.html#recap-last-lecture",
    "href": "lectures/md/KED2023_02.html#recap-last-lecture",
    "title": "The ABC of Computational Text Analysis",
    "section": "Recap last Lecture",
    "text": "Recap last Lecture\n\n\n\ncomputer as …\n\n… an intelligent device\n… a tool for a new social science\n\n\n\n\ndatafication\n\nabundance of data\nexploit new form of data\n\n\n\n\n\n\nTechnologie verändert Welt. Seit immer, erneut grosser Schritt nach Industrialisierung.\nComputer als Werkzeug/interaktiver Partner\n\nnehmen an sozialen Prozessen teil -> wie verändert sich das Soziale/Ökonomische (Wissenschaft, Arbeit, Jobmarkt, Benachteiligung)?\nfür CSS: Daten wichtiger als ML\n\nDaten sind da -> erst Programmieren ermöglicht Auswertung"
  },
  {
    "objectID": "lectures/md/KED2023_02.html#reading",
    "href": "lectures/md/KED2023_02.html#reading",
    "title": "The ABC of Computational Text Analysis",
    "section": "Reading",
    "text": "Reading\n\nComputational Social Science [@Lazer2009]\n\n\ndata-driven\nnetwork analysis + text analysis\nhistorical perspective vs. real-time dynamics\nissues: limited access to data and new methods\n\n\n\nhochaktuell: Einsichten in Pandemiegeschehen durch Netzwerkanalyse\nmethodical focus because of Nature paper\nmore than self-reported data (survey)\ntlw. schwieriger Zugang\n\norganisationsintern Daten und Datenschutz\nhistorische und textuelle Daten einfacher zugänglich"
  },
  {
    "objectID": "lectures/md/KED2023_02.html#semiotic-triangle",
    "href": "lectures/md/KED2023_02.html#semiotic-triangle",
    "title": "The ABC of Computational Text Analysis",
    "section": "Semiotic Triangle",
    "text": "Semiotic Triangle\n\n\n\nLoose coupling between\n\nWorld\nCognition\nLanguage\n\n\n\n\n\n\nSemiotic Triangle [@Ogden1923]\n\n\n\n\n\n\nWas ist Sprache?\n\nKeine Philosophie-Vorlesung\ntechnisch auch von Bedeutung\n\nVersuch der Einheit: Ding, Konzept und Wort\nkeine direkte Beziehung zwischen Symbol & Gegenstand\n\nkeine Eineindeutigkeit wie in Datenbanken –> schwierig für Computer\n\nidentische Personen- und Ortsnamen\numfasst Früchte auch Hülsenfrüchte?\n\njede Ecke kann wechseln\n\nGleiches heisst anders, anderes heisst gleich"
  },
  {
    "objectID": "lectures/md/KED2023_02.html#section",
    "href": "lectures/md/KED2023_02.html#section",
    "title": "The ABC of Computational Text Analysis",
    "section": "",
    "text": "Language shapes the way we think,  and determines what we can think about.\nBenjamin Lee Whorf\n\n\n\nzweiteilige These umstritten (Sapir-Whorf-Hypothese)\n\nSprache formt das Denken\nunabhängig der Determination: überragende Bedeutung für das Soziale\nInuit-Anekdote zu Schnee bedingt durch Grammatik\n\nSprache ist das Soziale schlechthin\n\nVermittlungsmedium\nweitere Formen: Zeichnen, Mathematik, Fotos\n\nwenn nicht kommuniziert, dann gesellschaftlich ohne Bedeutung (aber nicht unbedingt unvorstellbar)\n\nWörter sind Unterscheidungen\nLink zu Luhmann\n\nAktuelles Beispiel Ukraine-Krieg\n\nKonflikt vs Krieg (Gewalt) vs Invasion (asymmetrisch), militärische Operation\nDefinitionskampf ist gut erkennbar von Russland, aber auch allen anderen"
  },
  {
    "objectID": "lectures/md/KED2023_02.html#a-micro-and-macro-perspective-i",
    "href": "lectures/md/KED2023_02.html#a-micro-and-macro-perspective-i",
    "title": "The ABC of Computational Text Analysis",
    "section": "A micro and macro perspective I",
    "text": "A micro and macro perspective I\n\nindividual cases vs. collective trends\n\n\n\n\n\nclose reading (src)\n\n\n\n\n\n\ndistant reading (src)\n\n\n\n\n\n\nNun klar, wieso Textanalyse wichtig, aber welche Herangehensweise?\nTraditionell\n\nInhaltsanalyse, close reading\nEinzeldokumente\nlange Zeit alternativlos\n\ncomputergestützte Textanalyse\n\nNLP, distant reading\nTextsammlungen\n\nRauszoomen bringt mehr/neues Verständnis, nicht nur Reinzoomen\nMethodik ändert evtl. Fragestellung\n\nNLP: nicht Individuum, sondern Diskurs/Gesellschaft/Gruppe\nstrukturelle Beschreibungen und Kultur/Stimmung"
  },
  {
    "objectID": "lectures/md/KED2023_02.html#a-micro-and-macro-perspective-ii",
    "href": "lectures/md/KED2023_02.html#a-micro-and-macro-perspective-ii",
    "title": "The ABC of Computational Text Analysis",
    "section": "A micro and macro perspective II",
    "text": "A micro and macro perspective II\n\nscalability vs. abstraction\n\n\n\n\n\ntons of text (src)\n\n\n\n\n\n\nmeaning of numbers (src)\n\n\n\n\n\n\nje ein Problem je Approach\n\nclose: nicht skalierbar\n\nist das generalisierbar?\n\ndistant: kontextlos, da Narrativ/Einzelheiten verloren gehen\n\nverlieren wer/was/wo/wie/wann/warum\nwas bedeuten Zahlen? Verweis: BIP (informelle Wirtschaft)"
  },
  {
    "objectID": "lectures/md/KED2023_02.html#from-micro-to-macro-bar_chart-and-back-again-bookmark_tabs",
    "href": "lectures/md/KED2023_02.html#from-micro-to-macro-bar_chart-and-back-again-bookmark_tabs",
    "title": "The ABC of Computational Text Analysis",
    "section": "From micro to macro :bar_chart:…and back again :bookmark_tabs:",
    "text": "From micro to macro :bar_chart:…and back again :bookmark_tabs:\n\n\nLösung: Vogelperspektive, dann Eintauchen und zurück\nGute Data Science besteht aus guter Kenntnis von Daten\nGrösser nicht immer besser"
  },
  {
    "objectID": "lectures/md/KED2023_02.html#two-research-paradigms",
    "href": "lectures/md/KED2023_02.html#two-research-paradigms",
    "title": "The ABC of Computational Text Analysis",
    "section": "Two Research Paradigms",
    "text": "Two Research Paradigms\n\ndata exploration vs. hypothesis testing [@Evans2016]\n\nadd nuance\ndevelop new narratives\nverify hypothesis\n\n\n\ngenauere Einordnung: exaktere Epochenbestimmung\nAgnostik/Induktion ausnutzen für anderes Narrativ\n\ndata-driven Diskurs ordnen\n\nModelvorhersagen zu Kausalitätsaussagen\n\nz.B. Klimawandel Berichterstattung -> Erfolg grüne Partei?\nMetadaten zu Kommunikationsflüsse nötig"
  },
  {
    "objectID": "lectures/md/KED2023_02.html#numbers-do-not-talk-no_mouth",
    "href": "lectures/md/KED2023_02.html#numbers-do-not-talk-no_mouth",
    "title": "The ABC of Computational Text Analysis",
    "section": "Numbers do not talk :no_mouth:",
    "text": "Numbers do not talk :no_mouth:\n\nThus, quantification and qualitative analysis go well together.\n\n\nalter Konflikt Quali/Quanti\n\nbeide Lager kritisch gegenüber NLP\nzu wenig rigoros, zu naiv mangels Kontext\n\nZahlen sprechen nicht für sich selbst\nkomplementär"
  },
  {
    "objectID": "lectures/md/KED2023_02.html#text-as-data",
    "href": "lectures/md/KED2023_02.html#text-as-data",
    "title": "The ABC of Computational Text Analysis",
    "section": "Text as Data",
    "text": "Text as Data\n\nsynonymy\nambiguities\ncompositonality of meaning\ndiscrete symbols\nunstructured, messy data\n\n[see also @Grimmer2013]\n\n\nLink zu semiotischem Dreieck\nText inhärent schwierig\n\nherausfordernste Datenform\nFront der AI\n\nWörter = diskrete Symbole\n\nnominales Skalenniveau\n\ncompositional\n\ngrosse Mäuse, kleine Elefanten\n\nunstrukturiert\n\nanders als Tabelle/Datenbank\nunterschiedliche Datenformate"
  },
  {
    "objectID": "lectures/md/KED2023_02.html#unstructured-text-thinking",
    "href": "lectures/md/KED2023_02.html#unstructured-text-thinking",
    "title": "The ABC of Computational Text Analysis",
    "section": "Unstructured Text? :thinking:",
    "text": "Unstructured Text? :thinking:\n\ncollection > documents > paragraphs > sentences > words\n\n\n\nTexts carry more meaning than a soup (=set) of words (Wikimedia)"
  },
  {
    "objectID": "lectures/md/KED2023_02.html#in-class-task-file-types",
    "href": "lectures/md/KED2023_02.html#in-class-task-file-types",
    "title": "The ABC of Computational Text Analysis",
    "section": "In-class Task: File Types",
    "text": "In-class Task: File Types\n\n\nWhat file formats do you know?\nOpen files of different types in a text editor.  Which ones look good?\n\n\n\n\nProblem ist nicht wirklich der Text, sondern das Format\nalle möglichen Filetypen, nicht nur Text\n\nzip/tar, exe, dmg/iso, jpg/png/gif\n\nöffnen von Editor?\nDateiendungen aktiviert auf Computer?"
  },
  {
    "objectID": "lectures/md/KED2023_02.html#file-formats",
    "href": "lectures/md/KED2023_02.html#file-formats",
    "title": "The ABC of Computational Text Analysis",
    "section": "File Formats",
    "text": "File Formats\n\nmachine-readability\n\nraw: txt, csv, tsv\nformatted: docx, pdf, html, xml\n\nopen vs. proprietary\ndigital sustainability\n\n\n\nam besten raw + open\nPapier altert langsamer als Software!\nPause"
  },
  {
    "objectID": "lectures/md/KED2023_02.html#counting-ngrams",
    "href": "lectures/md/KED2023_02.html#counting-ngrams",
    "title": "The ABC of Computational Text Analysis",
    "section": "Counting ngrams",
    "text": "Counting ngrams\n\nGoogle Ngram Viewer [@Michel2011]\n\nhistorical perspective with ngrams\n>5.2 million books\nrise and fall of cultural ideas and phenomena\n\n\n\nGoogle Books\n\nindexiert ganze (Uni-)Bibliotheken\nin 2009 mehr als 4% aller veröffentlichter Bücher\n\nSee how ideas evolve/change over time\ny: relative Worthäufigkeiten\nx: Bücher indexiert nach Publikationsjahr\npubliziert in Science, kein klassiches SoWi Journal\n\ndisziplinare Grenzen brechen auf"
  },
  {
    "objectID": "lectures/md/KED2023_02.html#in-class-task-investigate-the-environmental-discourse",
    "href": "lectures/md/KED2023_02.html#in-class-task-investigate-the-environmental-discourse",
    "title": "The ABC of Computational Text Analysis",
    "section": "In-Class Task: Investigate the Environmental Discourse",
    "text": "In-Class Task: Investigate the Environmental Discourse\n\nWhat other terms have been used to describe nature?\n\ne.g. environment\n\nWhat environmental issues are debated the strongest? When?\n\ne.g. nuclear power plant\n\nAre there any differences between languages?\n\ni.e. similar words with non-equivalent curves over time\n\n\n\n:dart: What do you conclude from your observations?\n\n\nDauer: 20 Minuten\nissues described by whom?\nHerumgehen + selbst ausprobieren\nWikipedia nutzen"
  },
  {
    "objectID": "lectures/md/KED2023_02.html#refine-your-queries",
    "href": "lectures/md/KED2023_02.html#refine-your-queries",
    "title": "The ABC of Computational Text Analysis",
    "section": "Refine your Queries",
    "text": "Refine your Queries\n\ncheck out case-sensitiveness, wildcards (*) ​an​d ​operators :nerd_face:\n\n\n\n\n\n\n\nOperator\nDescription\n\n\n\n\n+\nsums multiple expressions into one to aggregate trends.\n\n\n-\nsubtracts the expression on the right from the expression on the left, giving you a way to measure one ngram relative to another.\n\n\n/\ndivides the expression on the left by the expression on the right, which is useful for isolating the behavior of an ngram with respect to another.\n\n\n*\nmultiplies the expression on the left by the number on the right, making it easier to compare ngrams of very different frequencies. (Be sure to enclose the entire ngram in parentheses so that * isn’t interpreted as a wildcard.)"
  },
  {
    "objectID": "lectures/md/KED2023_02.html#ngram-pay-attention",
    "href": "lectures/md/KED2023_02.html#ngram-pay-attention",
    "title": "The ABC of Computational Text Analysis",
    "section": "Ngram ‘pay attention’",
    "text": "Ngram ‘pay attention’\n\n\n\nGoogle Ngram Viewer: Evolution of the phrase ‘attention’\n\n\n\n\nmajor shift: “call attention” -> “pay attention”\nexterner Faktor (call) vs. aktives Verhalten (pay)\npay attention as a form of currency\nZusammenhang? Aufmerksamkeitsökonomie, Individualismus\n“if you don’t want to call attention to yourself by giving an incorrect answer, then you should probably pay attention in class.”"
  },
  {
    "objectID": "lectures/md/KED2023_02.html#remember-thumbsup",
    "href": "lectures/md/KED2023_02.html#remember-thumbsup",
    "title": "The ABC of Computational Text Analysis",
    "section": "Remember :thumbsup:",
    "text": "Remember :thumbsup:\nHas the language evolved over time or the social perception?\n\n\nBoth, most likely.\n\nSimilarly, language may vary across regions and communities.\n\n\nGrosse Frage ist\n\nWird das gleiche anders benannt?\nGeht es um was anderes?\n\nLink zu Odgen Dreieck von nicht fixer Beziehungen"
  },
  {
    "objectID": "lectures/md/KED2023_02.html#no-culturomics-but-meaning-making",
    "href": "lectures/md/KED2023_02.html#no-culturomics-but-meaning-making",
    "title": "The ABC of Computational Text Analysis",
    "section": "No Culturomics but Meaning-Making",
    "text": "No Culturomics but Meaning-Making\n\nphenomena in collective memory\n\nsemantic drifts (meaning)\nlexical shifts (frequency)\n\nRead, read, read to complement stats with context!\n\n\nÄnderung von kontextueller Verwendung oder Wortfrequenz\nEigentum hat sich etabliert, Religion degeneriert\nPatterns EN\n\ndessert=>*_ADJ\n*=>public_ADJ\n*=>personal_ADJ\n\nPattern DE\n\nKulturen=>*_ADJ\nKinder=>*_ADJ\n\nonly entire words, yet: _INF"
  },
  {
    "objectID": "lectures/md/KED2023_02.html#questions-of-interpretation",
    "href": "lectures/md/KED2023_02.html#questions-of-interpretation",
    "title": "The ABC of Computational Text Analysis",
    "section": "Questions of Interpretation",
    "text": "Questions of Interpretation\n\npossible reasons of decreasing frequency\n\nloosing interest\nbecoming an established fact\nnew reference\n\nThe Great War → World War I\n\nselection of data sources\n\n\n\nnumbers don’t talk\nKommunikation\n\nWeisse Schafe nicht erwähnenswert, nur schwarze\nNachrichtenwerte\nThemenkonjunkturen\n\neinzelne Wörter bilden schlechte Evidenzbasis"
  },
  {
    "objectID": "lectures/md/KED2023_02.html#a-word-of-caution",
    "href": "lectures/md/KED2023_02.html#a-word-of-caution",
    "title": "The ABC of Computational Text Analysis",
    "section": "A Word of Caution",
    "text": "A Word of Caution\nThe unknowns of Google Ngram Viewer\n\nindex of books\n\ngenre, authors, quantity\n\nartifacts of digitalization\n\n:nerd_face: use better alternative: bookworm HathiTrust\n\n\nGoogle: ~4% of all books ever published\nCompared to the 2009 versions, the 2012 and 2019 versions:\n\nmore books, improved OCR, improved library and publisher metadata.\nngrams across page boundaries, no ngrams across sentence boundaries\nrule-based tokenization\n\nwissenschaftlicher Standard\n\nZiel: nicht Unfehlbarkeit, sondern methodisch nachvollziehbar und kritisierbar\nzitierfähig\n\nHathiTrust\n\ncurated collection\nfilter by meta data"
  },
  {
    "objectID": "lectures/md/KED2023_02.html#interacting-with-data",
    "href": "lectures/md/KED2023_02.html#interacting-with-data",
    "title": "The ABC of Computational Text Analysis",
    "section": "Interacting with Data",
    "text": "Interacting with Data\n\nIt is a lense, not a map.\n\n\n\nDIKW pyramid (Wikipedia)\n\n\n\n\nLens / transformation like biology/physics\n\nallerdings keine Labordaten\nSoziales ohne ceteris paribus\n\nnot just mapping but interacting\n\nDaten erlauben neue Sicht\ndeshalb nicht CS überlassen"
  },
  {
    "objectID": "lectures/md/KED2023_02.html#prepare-your-system",
    "href": "lectures/md/KED2023_02.html#prepare-your-system",
    "title": "The ABC of Computational Text Analysis",
    "section": "Prepare your System",
    "text": "Prepare your System\n\nbackup files + update system :construction:\nstart installation with this guide :construction_worker:\n\n\n\nNicht riskanter als anderes. Ein Backup gehört dazu, ein Datenverlust sicher nicht.\nUnklarheiten/Probleme unbedingt zurückmelden\nWer hat Python schon installiert? Welches OS/Installer?"
  },
  {
    "objectID": "lectures/md/KED2023_02.html#new-room-classical_building",
    "href": "lectures/md/KED2023_02.html#new-room-classical_building",
    "title": "The ABC of Computational Text Analysis",
    "section": "New room :classical_building:",
    "text": "New room :classical_building:\n\nseminar in lecture hall 5\nas of 17 March onwards"
  },
  {
    "objectID": "lectures/md/KED2023_02.html#references",
    "href": "lectures/md/KED2023_02.html#references",
    "title": "The ABC of Computational Text Analysis",
    "section": "References",
    "text": "References"
  },
  {
    "objectID": "lectures.html",
    "href": "lectures.html",
    "title": "Lectures",
    "section": "",
    "text": "Below you find a brief description of all the lectures. I make the slides available before the lecture starts. The slides are provided in the following formats and may be opened by clicking the icon:"
  },
  {
    "objectID": "lectures.html#week-1-introduction-where-is-the-digital-revolution",
    "href": "lectures.html#week-1-introduction-where-is-the-digital-revolution",
    "title": "Lectures",
    "section": "Week 1: Introduction + Where is the digital revolution?",
    "text": "Week 1: Introduction + Where is the digital revolution?\nOn the one hand, I present the goals and organization of the seminar. On the other hand, we look at some recent applications that give an impression of the fascinating prospects of computers in the area of artificial intelligence (AI) and digital humanities (DH).\n  \n\n\nRequired Reading\n\nLazer, David, Alex Pentland, Lada Adamic, Sinan Aral, Albert-László Barabási, Devon Brewer, Nicholas Christakis, Noshir Contractor, James Fowler, Myron Gutmann, Tony Jebara, Gary King, Michael Macy, Deb Roy, and Marshall Van Alstyne. 2009. “Computational Social Science.” Science 323(5915):721–23.\n\n\n\nOptional Reading\n\nGraham, Shawn, Ian Milligan, and Scott Weingart. 2015. Exploring Big Historical Data: The Historian’s Macroscope. Open Draft Version. Under contract with Imperial College Press. online"
  },
  {
    "objectID": "lectures.html#week-2-text-as-data",
    "href": "lectures.html#week-2-text-as-data",
    "title": "Lectures",
    "section": "Week 2: Text as Data",
    "text": "Week 2: Text as Data\nComputational text analysis comes with many challenges that are unique due to the fuzziness of natural language. In this session, we learn about its methodological foundation, and we conduct our first computational text analysis to understand how this translates into practice."
  },
  {
    "objectID": "lectures.html#week-3-setting-up-your-development-environment",
    "href": "lectures.html#week-3-setting-up-your-development-environment",
    "title": "Lectures",
    "section": "Week 3: Setting up your Development Environment",
    "text": "Week 3: Setting up your Development Environment\nThe title says it all. We are getting ready for the practical part of the course: Programming. As the installation of Python and non-standard command-line tools may be tricky, we do this in class rather than doing it as homework. Moreover, I will also introduce some principles to organize research and jargon that guide your way in the programmer’s brave new word.\n  \n\n\nOptional: pimp your workflow\n\nHealy, Kieran. 2019. “The Plain Person’s Guide to Plain Text Social Science.” online."
  },
  {
    "objectID": "lectures.html#week-4-introduction-to-the-command-line",
    "href": "lectures.html#week-4-introduction-to-the-command-line",
    "title": "Lectures",
    "section": "Week 4: Introduction to the Command-line",
    "text": "Week 4: Introduction to the Command-line\nThe command-line is a powerful tool at your disposal. It is the working horse for many data wrangling tasks. In this session, you learn the basics of shells and perform many operations by effectively substituting clicks on the screen with commands. Admittedly, it is not overly exciting at this stage, yet it is essential for more sophisticated automation later on.\n  \n\n\nRecommended Resources\n\nCheatsheet for this course\nThe Programming Historian\nDigitalOcean"
  },
  {
    "objectID": "lectures.html#week-5-basic-nlp-with-command-line",
    "href": "lectures.html#week-5-basic-nlp-with-command-line",
    "title": "Lectures",
    "section": "Week 5: Basic NLP with Command-line",
    "text": "Week 5: Basic NLP with Command-line\nCounting words is the most basic method to look at texts from a computational perspective. The command-line provides tools to quickly sift through a massive text collection to describe the use of words quantitatively. In no time, you can also take a systematic look at the word usage in context. Sounds like a Swiss knife for computational text analysis in social science? It certainly is."
  },
  {
    "objectID": "lectures.html#week-6-learning-regular-expressions",
    "href": "lectures.html#week-6-learning-regular-expressions",
    "title": "Lectures",
    "section": "Week 6: Learning Regular Expressions",
    "text": "Week 6: Learning Regular Expressions\nWhen working with text data, you spend a lot of time cleaning your documents and extracting some pieces of information. Doing this by hand is not only a pain but simply impossible when facing more than a few dozens of documents. Fortunately, a formal language named Regular Expressions allows writing expressive and generalizable patterns to match specific text. Using these patterns, you can systematically extract and remove any textual parts without missing a single instance.\n  \n\n\nRequired Reading\n\nBen Schmidt. 2019. Regular Expressions. online\n\n\n\nRecommended Resource\nEverything we have touched about text processing in greater detail.\n\nNikolaj Lindberg. egrep for Linguists. online\n\n\n\nOnline Regular Expression Editor\n\nregex101 is a visual editor to check your regular expressions."
  },
  {
    "objectID": "lectures.html#week-7-working-with-data",
    "href": "lectures.html#week-7-working-with-data",
    "title": "Lectures",
    "section": "Week 7: Working with Data",
    "text": "Week 7: Working with Data\nTo this point, you have acquired the skills to cut a document into pieces and, subsequently, to extract, replace, and count any textual elements. Unless you have interesting data, these tools are neat but of no greater use. Thus, we turn to relevant data resources for social science. Given you have plain text at hand, your tools cut through data like butter. For other formats like PDF or DOCX, we learn some remedies to convert them into plain text. Most notably, we perform optical character recognition (OCR) ."
  },
  {
    "objectID": "lectures.html#week-8-ethics-and-the-evolution-of-nlp",
    "href": "lectures.html#week-8-ethics-and-the-evolution-of-nlp",
    "title": "Lectures",
    "section": "Week 8: Ethics and the Evolution of NLP",
    "text": "Week 8: Ethics and the Evolution of NLP\nEthics is not just an abstract topic of Philosophy. Modern NLP is more powerful than ever before and, thus, embedded in many aspects of life. Unfortunately, it also exhibits severe and not yet well-understood bias that causes harm. With the recent data-driven deep learning turn, NLP overcame many theoretical limitations – yet, this comes at a cost. It is our duty to better understand the working and impact of this technology."
  },
  {
    "objectID": "lectures.html#week-9-introduction-to-python",
    "href": "lectures.html#week-9-introduction-to-python",
    "title": "Lectures",
    "section": "Week 9: Introduction to Python",
    "text": "Week 9: Introduction to Python\nIt may come as a surprise that we start with Python in the ninth session only. As the folks say, Python is among the coolest programming languages, relatively easy to learn, and provides excellent NLP packages so that you don’t have to implement everything yourself. All true as long as you have your data ready. In this session, we begin with an introduction to the basic syntax of Python. Starting with basics is a dry matter; however, it allows you to use third-party libraries and get a handle on more sophisticated NLP analyses."
  },
  {
    "objectID": "lectures.html#week-10-nlp-with-python",
    "href": "lectures.html#week-10-nlp-with-python",
    "title": "Lectures",
    "section": "Week 10: NLP with Python",
    "text": "Week 10: NLP with Python\nPython is the language of choice when it comes to advanced NLP. Have you ever wondered how the frequency of terms has evolved over the years? Or how the language differs between two groups whereby the groups may be formed by any metadata (people, organization, gender etc.)? In such an exploratory endeavour, using an interactive and visual mode is the most effective that complements basic statistics. In short, we finally arrived at the serious stuff in our journey. To make sure, you don’t get lost in the forest of yet unknown terms you will also learn the jargon of NLP.\n  \n\n\nCode\n Click to open the static code\n Click to run the code in your browser without any installation"
  },
  {
    "objectID": "lectures.html#week-11-nlp-with-python-ii-working-session",
    "href": "lectures.html#week-11-nlp-with-python-ii-working-session",
    "title": "Lectures",
    "section": "Week 11: NLP with Python II + Working Session",
    "text": "Week 11: NLP with Python II + Working Session\nIn today’s session, we continue our deep dive into NLP with Python. It is the last piece of our puzzle. During this course, you have learned about the entire workflow, from assembling datasets of documents to analyze their content and visualize your findings. As soon as you have a structured text collection along with basic metadata (e.g., publication date), you can take numerous perspectives to look at your data. At this stage, it is time to kick-off the mini-projects allowing you to work with your data of interest.\n  \n\n\nExplore interactively: 1 August Speeches by Swiss Federal Councilors\nAs a matter of tradition, Swiss Federal Councilors give an official speech on the Swiss National Day. Simon Schmid (journalist at Republik), with the collaboration of Prof. Andreas Kley (Faculty of Law, UZH), collected many of these speeches and kindly shared the resulting dataset with me. The collection comprises 166 speeches, which is a multiple of the publicly available here.\nThe interactive visualization linked below shows how the language differs between speakers of Social Democratic Party of Switzerland (SP) and speakers of other parties. The top right corner shows terms that have been frequently used by all parties. In contrast, the top left and the lower right corner reveal words that have been used primarily by the members of the SP and correspondingly by the centre-right parties.\nYou can search for the terms of your interest. Moreover, you may click on the points in the plot to show the context of the corresponding words within speeches. These functions allow for a quick investigation of the corpus along the dimensions of Swiss parties.\n Click to explore in your browser(it may take a few seconds to load)"
  },
  {
    "objectID": "lectures.html#week-12-mini-project-presentations-discussion",
    "href": "lectures.html#week-12-mini-project-presentations-discussion",
    "title": "Lectures",
    "section": "Week 12: Mini-Project Presentations + Discussion",
    "text": "Week 12: Mini-Project Presentations + Discussion\nIn this session, it is your turn. Going beyond mere toy examples, you present what you have worked on and show off your first harvest of computational text analysis.\nThe seminar is coming to an end, yet it doesn’t have to be a dead-end. You may have gotten more proficient in cursing your computer but also fighting your way through the jungle of technology. Keep going, cheers!"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "The ABC of Computational Text Analysis",
    "section": "",
    "text": "In this hands-on seminar, bachelor students of social and cultural sciences learn the basics of programming, among other essential technical skills. Building on a modern technology stack, it aims to prepare students to conduct data-driven text analysis and to make everyday life easier by fostering technological fundamentals. While learning about the importance of computation in solving problems, we also discuss the current developments in information technology. In short, the course promotes digital literacy on a practical and theoretical level.\nThis seminar focuses on the computational processing of digital and digitized texts using Python and the command-line. For any empirical research, the systematic preparation and aggregation of data and the swift retrieval of information are critical. These tasks require the handling of various data forms, including data that is not yet structured in a tabular format. The seminar covers the complete workflow, from gathering textual data to analyzing an entire text collection to producing interactive visualizations. Sounds cool? It certainly is.\nAlong the way, we deal with questions like these:\n\nHow can texts be quantitatively exploited to complement the qualitative content analysis?\nWhat are regular expressions, and why are they so powerful in the context of computational text analysis?\nHow to download data automatically from websites and process en masse?\nHow can historical texts be extracted from PDFs using Optical Character Recognition (OCR)?\n\n Go to UniLu website\n Get course syllabus as PDF (updated version)"
  },
  {
    "objectID": "materials/mini_projects/2022_gender_differences/MiniProject_Code.html",
    "href": "materials/mini_projects/2022_gender_differences/MiniProject_Code.html",
    "title": "KED2023",
    "section": "",
    "text": "# import modules\n\nimport spacy\nimport textacy\nimport scattertext as st\nimport pandas as pd\nimport plotnine as p9 \nfrom plotnine import ggplot, aes, geom_line, geom_point, stat_smooth, theme_classic\n\n\n# create corpus object --> define function\n\ndef get_texts_from_csv(f_csv, text_column):\n    \"\"\"\n   Read dataset from a csv file and sequentially stream the rows,\n    including metadata.\n    \"\"\"\n\n    # read dataframe\n    df = pd.read_csv(f_csv)\n\n    # keep only documents that have text\n    filtered_df = df[df[text_column].notnull()]\n    \n    # iterate over rows in dataframe\n    for idx, row in filtered_df.iterrows():\n        \n        #read text and join lines (hard line-breaks)\n        text = row[text_column].replace('\\n', ' ')\n\n        #use all columns as metadata, except the column with the actual text\n        metadata = row.to_dict()\n        del metadata[text_column]\n\n        # return documents one after another (sequentially)\n        yield (text, metadata)\n\n\n# create corpus object --> process documents and create corpus\n\n# stream texts from a given folder\nf_csv = '../KED2022/materials/data/dataset_speeches_federal_council_2019.csv'\ntexts = get_texts_from_csv(f_csv, text_column='text')\n\n# load german language model\nde = textacy.load_spacy_lang(\"de_core_news_sm\")\n\n# create corpus from processed documents\ncorpus_speeches_XY = textacy.Corpus(de, data=texts)\n\n\n# make Python understand gender\nm = \"male\"\nf = \"female\"\nGeschlecht = [m,f]\n\n\n# function to filter by metadata gender (all speeches held by a female person)\ndef filter_func_female(doc):\n    return doc._.meta.get(\"Geschlecht\") == 'f'\n\n# create new corpus after applying filter function\nsubcor_female = textacy.corpus.Corpus(de, data=corpus_speeches_XY.get(filter_func_female))\n\n\n# function to filter by metadata gender (all speeches held by a male person)\ndef filter_func_male(doc):\n    return doc._.meta.get(\"Geschlecht\") == 'm'\n\n# create new corpus after applying filter function\nsubcor_male = textacy.corpus.Corpus(de, data=corpus_speeches_XY.get(filter_func_male))\n\n\n# export corpus as csv dataset --> See Code provided by other Teammember"
  },
  {
    "objectID": "materials/mini_projects/2022_gender_differences/test_f.html",
    "href": "materials/mini_projects/2022_gender_differences/test_f.html",
    "title": "KED2023",
    "section": "",
    "text": "#Import modules\nimport textacy\nimport spacy\nimport scattertext as st\nimport pandas as pd\nfrom pathlib import Path\nfrom plotnine import *\n\n#change language to german\nde = textacy.load_spacy_lang(\"de_core_news_sm\")\n\n\n#make Python understand gender\nm = \"male\"\nf = \"female\"\nGeschlecht = [m,f]\n\n\n#read dataset (female) from csv file\nf_csv = '../KED2022/materials/data/dataset_speeches_f.csv'\ndf = pd.read_csv(f_csv)\n\n# filter out non-german texts or very short texts\ndf_sub = df[(df['Sprache'] == 'de') & (df['text'].str.len() > 10)]\n\n# make new column containing all relevant metadata (showing in plot later on)\ndf_sub['descripton'] = df_sub[['Redner', 'Partei', 'Jahr']].astype(str).agg(', '.join, axis=1)\n\n# sneak peek of dataset\ndf_sub.head()\n\n\n\n\n\n  \n    \n      \n      Jahr\n      Status\n      Vollständigkeit\n      Redner\n      Geschlecht\n      Funktion\n      Partei\n      Partei-Original\n      Typ\n      Bemerkung\n      Sprache\n      Originalsprache\n      Ort\n      Titel\n      Anrede\n      Originaltext\n      Quelle\n      text\n      descripton\n    \n  \n  \n    \n      0\n      2018\n      done\n      vollständig\n      Doris Leuthard\n      f\n      BR\n      CVP\n      CVP\n      Lokal\n      NaN\n      de\n      NaN\n      Villmergen\n      NaN\n      Liebe Mitbürgerinnen und Mitbürger\n      NaN\n      https://www.admin.ch/gov/de/start/dokumentatio...\n      Ich bedanke mich für die Einladung zu Ihrer 1....\n      Doris Leuthard, CVP, 2018\n    \n    \n      1\n      2018\n      done\n      vollständig\n      Simonetta Sommaruga\n      f\n      BR\n      SP\n      SP\n      Lokal\n      NaN\n      de\n      NaN\n      Muttenz\n      Heimat kennt keine Grenzen\n      Liebe Festgemeinde,\\nSehr geehrter Herr Regier...\n      NaN\n      https://www.ejpd.admin.ch/ejpd/de/home/aktuell...\n      Als ich die Einladung zu dieser Bundesfeier ge...\n      Simonetta Sommaruga, SP, 2018\n    \n    \n      2\n      2017\n      done\n      vollständig\n      Doris Leuthard\n      f\n      BP\n      CVP\n      CVP\n      BP-Rede\n      NaN\n      de\n      NaN\n      NaN\n      NaN\n      Sehr geehrte Damen und Herren\n      NaN\n      https://www.admin.ch/gov/de/start/dokumentatio...\n      Ich hoffe, sie konnten den bisherigen Sommer g...\n      Doris Leuthard, CVP, 2017\n    \n    \n      3\n      2017\n      done\n      vollständig\n      Simonetta Sommaruga\n      f\n      BR\n      SP\n      SP\n      Lokal\n      NaN\n      de\n      fr\n      Môtiers\n      «Val-de-Travers, val ouvert»\n      Cher Président du Conseil d’Etat,\\nCher Présid...\n      Quel bonheur de célébrer la fête nationale dan...\n      https://www.ejpd.admin.ch/ejpd/fr/home/aktuell...\n      Was für eine Freude, den Nationalfeiertag an e...\n      Simonetta Sommaruga, SP, 2017\n    \n    \n      4\n      2016\n      done\n      vollständig\n      Doris Leuthard\n      f\n      BR\n      CVP\n      CVP\n      Lokal\n      NaN\n      de\n      NaN\n      Schaffhausen\n      Wer entscheidet, muss zuhören!\n      Sehr geehrter Herr Regierungspräsident (Reto D...\n      NaN\n      https://www.admin.ch/gov/de/start/dokumentatio...\n      Ich freue mich, heute in Schaffhausen mit Ihne...\n      Doris Leuthard, CVP, 2016\n    \n  \n\n\n\n\n\n#create corpus for female dataset\ndef get_texts_from_csv(f_csv, text_column):\n    \"\"\"\n    Read dataset from a csv file and sequentially stream the rows,\n    including metadata.\n    \"\"\"\n    \n    # read dataframe\n    df = pd.read_csv(f_csv)\n    \n    # keep only documents that have text\n    filtered_df = df[df[text_column].notnull()]\n    \n    # iterate over rows in dataframe\n    for idx, row in filtered_df.iterrows():\n        \n        # read text and join lines (remove hard line-breaks)\n        text = row[text_column].replace('\\n', ' ')\n\n        # use all columns as metadata, except the column with the actual text\n        metadata = row.to_dict()\n        del metadata[text_column]\n\n        yield (text, metadata)\n\nf_csv = '../KED2022/materials/data/dataset_speeches_f.csv'\ntexts = get_texts_from_csv(f_csv, text_column='text')\n\ncorpus_speeches_f = textacy.Corpus(de, data=texts)\n\n\n# define what groups are formed and what terms should be included\n# here, groups by year and words are lowercased (incl. stop words)\ntokenized_docs, groups = textacy.io.unzip(\n        (textacy.extract.utils.terms_to_strings(textacy.extract.words(doc, filter_stops=False), by=\"lower\"),\n        doc._.meta[\"Jahr\"])\n        for doc in corpus_speeches_f)\n\n# define how to count\n# here relative term frequency\nvectorizer = textacy.representations.vectorizers.GroupVectorizer(\n        tf_type='linear', # absolute term frequency\n        dl_type=\"linear\", # normalized by document length\n        vocabulary_grps=range(1950, 2019)) # limit to years from 1950 to 2019\n\n# create group-term-matrix with with frequency counts\ngrp_term_matrix = vectorizer.fit_transform(tokenized_docs, groups)\n\n# create dataframe from matrix\ndf_terms = pd.DataFrame.sparse.from_spmatrix(grp_term_matrix, index=vectorizer.grps_list, columns=vectorizer.terms_list)\ndf_terms['year'] = df_terms.index\n\n# change shape of dataframe\ndf_tidy = df_terms.melt(id_vars='year', var_name=\"term\", value_name=\"frequency\")\ndf_tidy\n\n/home/nilsos/.local/lib/python3.8/site-packages/textacy/representations/vectorizers.py:673: RuntimeWarning: divide by zero encountered in true_divide\n\n\n\n\n\n\n  \n    \n      \n      year\n      term\n      frequency\n    \n  \n  \n    \n      0\n      1950\n      -unterhalt\n      0.00000\n    \n    \n      1\n      1951\n      -unterhalt\n      0.00000\n    \n    \n      2\n      1952\n      -unterhalt\n      0.00000\n    \n    \n      3\n      1953\n      -unterhalt\n      0.00000\n    \n    \n      4\n      1954\n      -unterhalt\n      0.00000\n    \n    \n      ...\n      ...\n      ...\n      ...\n    \n    \n      511423\n      2014\n      ►\n      0.00000\n    \n    \n      511424\n      2015\n      ►\n      0.00000\n    \n    \n      511425\n      2016\n      ►\n      0.00000\n    \n    \n      511426\n      2017\n      ►\n      0.00000\n    \n    \n      511427\n      2018\n      ►\n      0.00039\n    \n  \n\n511428 rows × 3 columns\n\n\n\n\ndocs_per_year = df_sub.groupby('Jahr').agg({'text': \"count\" }).reset_index().rename(columns={'text':'count'})\n\n(ggplot(docs_per_year, aes(x='Jahr', y='count'))\n + geom_line(color='darkblue')\n +  labs(title = \"Number of Speeches per Year (f)\", x = \"Year\", y = \"absolute frequency\")\n + xlim(2000,2020)\n + scale_y_continuous(breaks=range(0, 11))\n + theme_minimal())\n\n/home/nilsos/.local/lib/python3.8/site-packages/plotnine/geoms/geom_path.py:75: PlotnineWarning: geom_path: Removed 1 rows containing missing values.\n\n\n\n\n\n<ggplot: (8790088269234)>\n\n\n\n# filter the dataset for the five most used terms in speeches read by a female speaker\nterms = [\"gemeinsam\", \"grenzen\", \"brauchen\", \"geschaffen\", \"identität\"]\ndf_terms = df_tidy[df_tidy['term'].isin(terms)]\n\n# plot the relative frequency for the terms above\n(ggplot(df_terms, aes(x='year', y='frequency', color='term'))\n + ggtitle('Female speaker with top five terms used by women') # give plot a name to differentiate from male plot\n + geom_point() # show individual points\n + stat_smooth(method='lowess', span=0.15, se=False) # overlay points with a smoothed line\n + ylim(0,0.003)\n + xlim(2000,2020) # change x-axis numbers to match the data\n + theme_minimal()) # make the plot look nicer\n\n/home/nilsos/.local/lib/python3.8/site-packages/plotnine/layer.py:401: PlotnineWarning: geom_point : Removed 253 rows containing missing values.\n\n\n\n\n\n<ggplot: (8790085365986)>\n\n\n\n# filter the dataset for the five most used terms in speeches read by a female speaker\nterms = [\"geschichte\", \"tessin\", \"unabhängigkeit\", \"werte\", \"zusammenhalt\"]\ndf_terms = df_tidy[df_tidy['term'].isin(terms)]\n\n# plot the relative frequency for the terms above\n(ggplot(df_terms, aes(x='year', y='frequency', color='term'))\n + ggtitle('Female speaker with top five terms used by men') # give plot a name to differentiate from male plot\n + geom_point() # show individual points\n + stat_smooth(method='lowess', span=0.15, se=False) # overlay points with a smoothed line\n + ylim(0,0.003)\n + xlim(2000,2020) # change x-axis numbers to match the data\n + theme_minimal()) # make the plot look nicer\n\n/home/nilsos/.local/lib/python3.8/site-packages/plotnine/layer.py:401: PlotnineWarning: geom_point : Removed 200 rows containing missing values.\n\n\n\n\n\n<ggplot: (8790084210463)>"
  },
  {
    "objectID": "materials/mini_projects/2022_gender_differences/MiniProject_Scattertext.html",
    "href": "materials/mini_projects/2022_gender_differences/MiniProject_Scattertext.html",
    "title": "KED2023",
    "section": "",
    "text": "VORGEHEN:\n\nModule importieren\nDefine Function\nCreate Corpus\nCreate Subcorpus (beide Geschlechter, nach 2000)\nSubcorpus als csv abspeichern\ncsv mit scattertext importieren\ndie beiden Achsen aufspannen auf Basis der Variable Geschlecht (m/f)\n\n(Wenn alle Spaltennamnen stimmen sollte es so klappen)\n\n### Step 1) Import modules\n\nimport spacy\nimport textacy\nimport scattertext as st\nimport pandas as pd\nimport plotnine as p9 \n\n\n### Step 2) Define function\n\ndef get_texts_from_csv(f_csv, text_column):\n    \"\"\"\n   Read dataset from a csv file and sequentially stream the rows,\n    including metadata.\n    \"\"\"\n\n    # read dataframe\n    df = pd.read_csv(f_csv)\n\n    # keep only documents that have text\n    filtered_df = df[df[text_column].notnull()]\n    \n    # iterate over rows in dataframe\n    for idx, row in filtered_df.iterrows():\n        \n        #read text and join lines (hard line-breaks)\n        text = row[text_column].replace('\\n', ' ')\n\n        #use all columns as metadata, except the column with the actual text\n        metadata = row.to_dict()\n        del metadata[text_column]\n\n        # return documents one after another (sequentially)\n        yield (text, metadata)\n\n\n### Step 3) Create Corpus\n\n# stream texts from a given folder\nf_csv = '../KED2022/materials/data/dataset_speeches_federal_council_2019.csv'\ntexts = get_texts_from_csv(f_csv, text_column='text')\n\n# load german language model\nde = textacy.load_spacy_lang(\"de_core_news_sm\")\n\n# create corpus from processed documents\ncorpus_speeches_XY = textacy.Corpus(de, data=texts)\n\n\n### Step 4) Create Subcorpus (both genders, DE, after the year 2000)\n\n## subcor (filtering by meta attributes \"language\" and \"after 2000\")\n\n# function to filter by metadata \ndef filter_func_1(doc):\n    return doc._.meta.get(\"Jahr\") > 2000\n\n# create new corpus after applying filter function\nsubcor = textacy.corpus.Corpus(de, data=corpus_speeches_XY.get(filter_func_1))\n\n\n### Step 5) Export corpus as csv dataset\n\n# merge metadata and actual content for each document in the corpus\n# ugly, verbose syntax to merge two dictionaries\ndata = [{**doc._.meta, **{'text': doc.text}} for doc in subcor]\n\n# export corpus as csv\nf_csv = '../KED2022/materials/data/dataset_speeches.csv'\ntextacy.io.csv.write_csv(data, f_csv, fieldnames=data[0].keys())\n\n# csv format is the best to load in scattertext\ndata[0]\n\n\n### Step 6) Import csv to use in scattertext: load file\n\n# read dataset from csv file\nf_csv = '../KED2022/materials/data/dataset_speeches.csv'\ndf = pd.read_csv(f_csv)\n\n# filter out non-german texts or very short texts\ndf_sub = df[(df['Sprache'] == 'de') & (df['text'].str.len() > 10)]\n\n# make new column containing all relevant metadata (showing in plot later on)\ndf_sub['descripton'] = df_sub[['Redner', 'Partei', 'Jahr']].astype(str).agg(', '.join, axis=1)\n\n# sneak peek of dataset\ndf_sub.head()\n\n\n### Step 7) create scattertext plot, axes basing on the variable \"gender\"\n\ncensor_tags = set(['CARD']) # tags to ignore in corpus, e.g. numbers\n\n# stop words to ignore in corpus\nde_stopwords = spacy.lang.de.stop_words.STOP_WORDS # default stop words\ncustom_stopwords = set(['[', ']', '%', '*', '•', '2.', '19.', '21.', '9.', '3.'])\nde_stopwords = de_stopwords.union(custom_stopwords) # extend with custom stop words\n\n# create corpus from dataframe\n# lowercased terms, no stopwords, no numbers\n# use lemmas for English only, German quality is too bad\ncorpus_speeches = st.CorpusFromPandas(df_sub, # dataset\n                             category_col='Geschlecht', # index differences by ...\n                             text_col='text', \n                             nlp=de, # German model\n                             feats_from_spacy_doc=st.FeatsFromSpacyDoc(tag_types_to_censor=censor_tags, use_lemmas=False),\n                             ).build().get_stoplisted_unigram_corpus(de_stopwords)\n# produce visualization (interactive html)\nhtml = st.produce_scattertext_explorer(corpus_speeches,\n            category='m', # set attribute to divide corpus into two parts\n            category_name='male',\n            not_category_name='female',\n            metadata=df_sub['descripton'],\n            width_in_pixels=1000,\n            minimum_term_frequency=5, # drop terms occurring less than 5 times\n            save_svg_button=True,                          \n)\n\n# write visualization to html file\nfname = '..KED2022/materials/data/gender_differences_final.html'\nopen(fname, 'wb').write(html.encode('utf-8'))\n\n\nAbout the scattertext - Explanation for Orientation:\n\nTerms in upper right    = frequently used by male and female speakers alike\nTerms in lower right    = often used by female speakers\nTerms in upper left     = often used by male speakers\nTerms in lower left     = infrequently used by male and female speakers alike\n\n### What can we see?\n\nTop 3 terms in male speeches:       Geschichte, Zusammenhalt, Tessin  \nTop 3 terms in female speeches:     gemeinsam, Grenzen, brauchen\n\nTop 3 terms in both genders:        Menschen, Land, Schweiz\n\n### Important to keep in mind\n\nDocument count total:   96\nmale document count:    67\nfemale document count:  29"
  },
  {
    "objectID": "materials/mini_projects/2022_gender_differences/test_m.html",
    "href": "materials/mini_projects/2022_gender_differences/test_m.html",
    "title": "KED2023",
    "section": "",
    "text": "#Import modules\nimport textacy\nimport spacy\nimport scattertext as st\nimport pandas as pd\nfrom pathlib import Path\nfrom plotnine import *\n\n#change language to german\nde = textacy.load_spacy_lang(\"de_core_news_sm\")\n\n\n#make Python understand gender\nm = \"male\"\nf = \"female\"\nGeschlecht = [m,f]\n\n\n#read dataset (male) from csv file\nf_csv = '../KED2022/materials/data/dataset_speeches_m.csv'\ndf = pd.read_csv(f_csv)\n\n# filter out non-german texts or very short texts\ndf_sub = df[(df['Sprache'] == 'de') & (df['text'].str.len() > 10)]\n\n# make new column containing all relevant metadata (showing in plot later on)\ndf_sub['descripton'] = df_sub[['Redner', 'Partei', 'Jahr']].astype(str).agg(', '.join, axis=1)\n\n# sneak peek of dataset\ndf_sub.head()\n\n\n\n\n\n  \n    \n      \n      Jahr\n      Status\n      Vollständigkeit\n      Redner\n      Geschlecht\n      Funktion\n      Partei\n      Partei-Original\n      Typ\n      Bemerkung\n      Sprache\n      Originalsprache\n      Ort\n      Titel\n      Anrede\n      Originaltext\n      Quelle\n      text\n      descripton\n    \n  \n  \n    \n      0\n      2018\n      done\n      vollständig\n      Alain Berset\n      m\n      BP\n      SP\n      SP\n      BP-Rede\n      NaN\n      de\n      NaN\n      NaN\n      NaN\n      Sehr geehrte Damen und Herren\n      NaN\n      https://www.admin.ch/gov/de/start/dokumentatio...\n      Wir leben in der Schweiz in Frieden und Wohlst...\n      Alain Berset, SP, 2018\n    \n    \n      3\n      2018\n      done\n      vollständig\n      Guy Parmelin\n      m\n      BR\n      SVP\n      SVP\n      Lokal\n      NaN\n      de\n      NaN\n      NaN\n      «Armbrust und Hellebarde»\n      Sehr geehrte Eidgenossen, Meine Damen und Herren\n      NaN\n      https://www.admin.ch/gov/de/start/dokumentatio...\n      Eine 1.-August-Rede ist eine der heikelsten rh...\n      Guy Parmelin, SVP, 2018\n    \n    \n      4\n      2018\n      done\n      vollständig\n      Ignazio Cassis\n      m\n      BR\n      FDP\n      FDP\n      Lokal\n      NaN\n      de\n      NaN\n      Rorschach\n      Die Italianità hat ihre Wurzeln in Rorschach\n      Gueten Obig mitenand!\n      NaN\n      https://www.admin.ch/gov/de/start/dokumentatio...\n      Und danke für diese freundliche Einladung! Ich...\n      Ignazio Cassis, FDP, 2018\n    \n    \n      6\n      2017\n      done\n      vollständig\n      Didier Burkhalter\n      m\n      BR\n      FDP\n      FDP\n      Lokal\n      NaN\n      de\n      fr\n      Aigle-Les Diablerets\n      „Le 1er août est un voyage à travers la Suisse...\n      Mesdames et Messieurs, et chers amis,\n      Le 1er août est un voyage. Et pour mon épouse ...\n      https://www.admin.ch/gov/de/start/dokumentatio...\n      Der 1. August ist eine Reise. Und für meine Fr...\n      Didier Burkhalter, FDP, 2017\n    \n    \n      7\n      2017\n      done\n      vollständig\n      Guy Parmelin\n      m\n      BR\n      SVP\n      SVP\n      Lokal\n      NaN\n      de\n      NaN\n      Bern\n      «Ein Land, das stolz auf seine Traditionen ist...\n      Sehr geehrte Eidgenossen\\nMeine Damen und Herren\n      NaN\n      https://www.admin.ch/gov/de/start/dokumentatio...\n      Für uns ist der 1. August mehr als ein Nationa...\n      Guy Parmelin, SVP, 2017\n    \n  \n\n\n\n\n\n#create corpus for male dataset\ndef get_texts_from_csv(f_csv, text_column):\n    \"\"\"\n    Read dataset from a csv file and sequentially stream the rows,\n    including metadata.\n    \"\"\"\n    \n    # read dataframe\n    df = pd.read_csv(f_csv)\n    \n    # keep only documents that have text\n    filtered_df = df[df[text_column].notnull()]\n    \n    # iterate over rows in dataframe\n    for idx, row in filtered_df.iterrows():\n        \n        # read text and join lines (remove hard line-breaks)\n        text = row[text_column].replace('\\n', ' ')\n\n        # use all columns as metadata, except the column with the actual text\n        metadata = row.to_dict()\n        del metadata[text_column]\n\n        yield (text, metadata)\n\nf_csv = '../KED2022/materials/data/dataset_speeches_m.csv'\ntexts = get_texts_from_csv(f_csv, text_column='text')\n\ncorpus_speeches_m = textacy.Corpus(de, data=texts)\n\n\n# define what groups are formed and what terms should be included\n# here, groups by year and words are lowercased (incl. stop words)\ntokenized_docs, groups = textacy.io.unzip(\n        (textacy.extract.utils.terms_to_strings(textacy.extract.words(doc, filter_stops=False), by=\"lower\"),\n        doc._.meta[\"Jahr\"])\n        for doc in corpus_speeches_m)\n\n# define how to count\n# here relative term frequency\nvectorizer = textacy.representations.vectorizers.GroupVectorizer(\n        tf_type='linear', # absolute term frequency\n        dl_type=\"linear\", # normalized by document length\n        vocabulary_grps=range(1950, 2019)) # limit to years from 1950 to 2019\n\n# create group-term-matrix with with frequency counts\ngrp_term_matrix = vectorizer.fit_transform(tokenized_docs, groups)\n\n# create dataframe from matrix\ndf_terms = pd.DataFrame.sparse.from_spmatrix(grp_term_matrix, index=vectorizer.grps_list, columns=vectorizer.terms_list)\ndf_terms['year'] = df_terms.index\n\n# change shape of dataframe\ndf_tidy = df_terms.melt(id_vars='year', var_name=\"term\", value_name=\"frequency\")\ndf_tidy\n\n/home/nilsos/.local/lib/python3.8/site-packages/textacy/representations/vectorizers.py:673: RuntimeWarning: divide by zero encountered in true_divide\n\n\n\n\n\n\n  \n    \n      \n      year\n      term\n      frequency\n    \n  \n  \n    \n      0\n      1950\n      's\n      0.0\n    \n    \n      1\n      1951\n      's\n      0.0\n    \n    \n      2\n      1952\n      's\n      0.0\n    \n    \n      3\n      1953\n      's\n      0.0\n    \n    \n      4\n      1954\n      's\n      0.0\n    \n    \n      ...\n      ...\n      ...\n      ...\n    \n    \n      1119175\n      2014\n      −\n      0.0\n    \n    \n      1119176\n      2015\n      −\n      0.0\n    \n    \n      1119177\n      2016\n      −\n      0.0\n    \n    \n      1119178\n      2017\n      −\n      0.0\n    \n    \n      1119179\n      2018\n      −\n      0.0\n    \n  \n\n1119180 rows × 3 columns\n\n\n\n\n# filter the dataset for the five most used terms in speeches read by a female speaker\nterms = [\"geschichte\", \"zusammenhalt\", \"tessin\", \"unabhängigkeit\", \"werte\"]\ndf_terms = df_tidy[df_tidy['term'].isin(terms)]\n\n# plot the relative frequency for the terms above\n(ggplot(df_terms, aes(x='year', y='frequency', color='term'))\n + ggtitle('Male speaker with top five terms used by men') # give plot a name to differentiate from male plot\n + geom_point() # show individual points\n + stat_smooth(method='lowess', span=0.15, se=False) # overlay points with a smoothed line\n + ylim(0,0.003)\n + xlim(2000,2020) # change x-axis numbers to match the data\n + theme_minimal()) # make the plot look nicer\n\n/home/nilsos/.local/lib/python3.8/site-packages/plotnine/layer.py:401: PlotnineWarning: geom_point : Removed 250 rows containing missing values.\n\n\n\n\n\n<ggplot: (8747025210257)>\n\n\n\n# filter the dataset for the five most used terms in speeches read by a female speaker\nterms = [\"gemeinsam\", \"grenzen\", \"brauchen\", \"geschaffen\", \"identität\"]\ndf_terms = df_tidy[df_tidy['term'].isin(terms)]\n\n# plot the relative frequency for the terms above\n(ggplot(df_terms, aes(x='year', y='frequency', color='term'))\n + ggtitle('Male speaker with top five terms used by women') # give plot a name to differentiate from male plot\n + geom_point() # show individual points\n + stat_smooth(method='lowess', span=0.15, se=False) # overlay points with a smoothed line\n + ylim(0,0.003)\n + xlim(2000,2020) # change x-axis numbers to match the data\n + theme_minimal()) # make the plot look nicer\n\n/home/nilsos/.local/lib/python3.8/site-packages/plotnine/layer.py:401: PlotnineWarning: geom_point : Removed 250 rows containing missing values.\n\n\n\n\n\n<ggplot: (8747020953965)>"
  },
  {
    "objectID": "materials/cheatsheet_command_line.html",
    "href": "materials/cheatsheet_command_line.html",
    "title": "Cheatsheet Shell Commands",
    "section": "",
    "text": "Shell Command\nExplanation\n\n\n\n\ncd filepath\nchange directory aka move into a different folder\n\n\nls -lh folder\nlist the files and folders in your current directory\n\n\npwd\nshow path of working directory aka the folder that you’re in right now\n\n\ntouch fname\nmake a new file\n\n\nmkdir dirname\nmake a new directory aka a folder\n\n\nrm fname\nremove aka delete a file or directory\n\n\ncp original-fname copied-fname\ncopy a file or directory\n\n\nmv original-fname new-fname\nmove or rename a file or directory\n\n\ncat fname\nshow all the contents of a file\n\n\nmore fname\nshow snippet of a file that allows you to scroll through the entire thing\n\n\nhead fname\nshow the first 10 lines of a file (change number of lines by adding a flag, e.g. head -100)\n\n\ntail fname\nshow the last 10 lines of a file (change number of lines by adding a flag, e.g. tail -100)\n\n\nwc -w -l fname\nshow how many words or lines in a file\n\n\nman command\nshow the manual aka the documentation that tells you what a particular command does\n\n\necho\nprint text to the command line\n\n\negrep \"search pattern\" fname or dirname\nsearch for lines that include search term in file. See below for the arguments of egrep.\n\n\nwget url\nget a file from the web\n\n\n\nThis cheatsheet is based on this resource. Please also refer to this resource for a more in-dept explanation in prose. You should follow the guide for macOS and Unix even as a Windows user as we have installed a Unix environment.\n\n\nThe most common arguments of egrep:\n\n-i search case insensitive\n-r search recursively in folder\n-o show exact matches only instead of entire lines with matches\n-h suppress the file path where the match occurred\n\n\n\n\n\n|: A pipe takes the output of one command and passes it as the input to another.\necho \"pass this text to next command\" | cat\n>: This operator redirects the output to a file (overwrites if it already exists). Example:\necho \"first line of file1\" > file1\n>>: This operator redirects and appends the output to an existing file: Example:\necho \"line following existing content of file1\" >> file1"
  },
  {
    "objectID": "materials/cheatsheet_command_line.html#example-patterns",
    "href": "materials/cheatsheet_command_line.html#example-patterns",
    "title": "Cheatsheet Shell Commands",
    "section": "Example Patterns",
    "text": "Example Patterns\n# alle Kleinbuchstaben\necho \"Das ist ein Satz mit der Zahl 1000\" | egrep --colour \"[a-z]\"\n\n# alle Grossbuchstaben\necho \"Das ist ein Satz mit der Zahl 1000\" | egrep --colour \"[A-Z]\"\n\n# das Wort \"ist\" und das nächste Wort\necho \"Das ist ein Satz mit der Zahl 1000\" | egrep --colour \"ist [a-z]*\"\n\n# das Wort \"Zahl\" gefolgt von einer Ziffer\necho \"Das ist ein Satz mit der Zahl 1000\" | egrep --colour \"Zahl [0-9]\" \n\n# das Wort \"Zahl\" gefolgt von beliebig vielen Ziffern\necho \"Das ist ein Satz mit der Zahl 1000\" | egrep --colour \"Zahl [0-9]*\""
  },
  {
    "objectID": "materials/cheatsheet_command_line.html#pattern-equivalence",
    "href": "materials/cheatsheet_command_line.html#pattern-equivalence",
    "title": "Cheatsheet Shell Commands",
    "section": "Pattern Equivalence",
    "text": "Pattern Equivalence\na+ == aa*               # \"a\" once or more than once\na? == (a|_)             # \"a\" once or nothing\na{3} == aaa             # three \"a\"\na{2,3} == (aa|aaa)      # two or three \"a\"\n[ab] == (a|b)           # \"a\" or \"b\"\n[0-9] == (0|1|2|3|4|5|6|7|8|9)  #any digit"
  },
  {
    "objectID": "materials/installation_guide.html",
    "href": "materials/installation_guide.html",
    "title": "Setting up your Development Environment",
    "section": "",
    "text": "IMPORTANT NOTE: When you use a Mac and have Python 3/Anaconda installed already, let me know first before proceeding. Installing naively multiple Python versions may cause problems.\nThis guide aims to ease setting up your development environment for Windows 10 and macOS. Depending on your operating system, you have to install additional components to program in Python and perform common data wrangling tasks in the command-line. The instructions cover the installation of the following components:\n\nPython 3.8\nVS Code Editor\nvarious Bash tools\nTesseract (Optical Character Recognition)\n\nThe proposed installation strikes a good balance between relative simplicity (e.g., no virtual environments in Python) and cross-platform usage of the tools. Be aware that the installation and setup of software are sometimes more difficult and poorer documented than its usage. Beyond potential problems during the installation, there are also different ways to set up the development environment.\nWe write Python code using the VS Code. Although lightweight, it is a fully-fledged integrated development environment (IDE) supporting all major operating systems and many programming languages. Other than competing editors, it doesn’t require much configuration and, out-of-the-box, it comes with indispensable features like auto-completion, code formatting, linting (error flagging) and debugging. In VS Code, you can even open and run Jupyter Notebooks to perform interactive data science showing the output next to the code that created it.\nBefore you proceed with the installation, back up your files1 and make sure that your computer fulfils the following two requirements:\n\nruns the most current version of your operating system (Windows, macOS). If not, update your system.\nhas at least 15 GB of free disk space.\n\nKeep in mind that you will not get any feedback in the command-line unless there is an issue. At first, the lack of feedback after a successful action may be confusing as it runs counter the general experience when working with graphical interfaces. However, the lack of feedback just means the command was executed as expected; thus, there is no need to bother you with further messages.\nLet me know if you are struggling with any of the steps."
  },
  {
    "objectID": "materials/installation_guide.html#install-command-line-tools",
    "href": "materials/installation_guide.html#install-command-line-tools",
    "title": "Setting up your Development Environment",
    "section": "Install Command Line Tools",
    "text": "Install Command Line Tools\nDespite being Unix-based, macOS is lacking some essential command-line tools out-of-the-box. In this section, we install the Command Line Tools first since these tools are also a requirement for the subsequent installation of the package manager Homebrew.\n\nOpen a Terminal to get a command-line interface. When you cannot find the application in your system tray, press the command and spacebar keys to search and type Terminal to search for it.\nType the following command and press enter:\n xcode-select --install\nIn the dialogue that pops up, click Install and accept the terms and conditions.\nAs soon as the installer goes away, Command Line Tools should be installed successfully. To ensure that everything works as expected, run the following command:\ngit --version\nWhen you see something like this git version 2.x, the installation was successfull.\n\nSource: OSXDaily"
  },
  {
    "objectID": "materials/installation_guide.html#install-package-manager-homebrew",
    "href": "materials/installation_guide.html#install-package-manager-homebrew",
    "title": "Setting up your Development Environment",
    "section": "Install Package Manager Homebrew",
    "text": "Install Package Manager Homebrew\nHomebrew is a powerful package manager for macOS systems. With Homebrew, you can easily install programms using the Terminal.\n\nTo install Homebrew, type the following command into your Terminal window:\n/bin/bash -c \"$(curl -fsSL \\ \nhttps://raw.githubusercontent.com/Homebrew/install/master/install.sh)\"\nWhen there is an issue executing this command (i.e., unprintable characters), copy the identical looking installation command from the offical website into your Terminal.\nYou can make sure that Homebrew was successfully installed by typing:\nbrew doctor\nTo ensure that your installation of Homebrew is up to date, run:\nbrew update\nAlthough it is not needed for now, you can upgrade outdated packages altogether with:\nbrew upgrade\nOnce you’ve installed Homebrew, make Homebrew’s Python the primary environment by setting the PATH variable. In a Terminal, run the following command to add a new PATH variable to the ~/.profile file:\necho 'export PATH=\"/usr/local/opt/python/libexec/bin:$PATH\"' >> ~/.profile\nsource ~/.profile\n\nSource: Homebrew"
  },
  {
    "objectID": "materials/installation_guide.html#install-python-3-with-homebrew",
    "href": "materials/installation_guide.html#install-python-3-with-homebrew",
    "title": "Setting up your Development Environment",
    "section": "Install Python 3 with Homebrew",
    "text": "Install Python 3 with Homebrew\nHomebrew makes it easy to install Python 3.\n\nRun the following command in a Terminal:\nbrew install python\nWhen Python 3 has been installed correctly on your system, you should see version 3.x after issuing the following command:\npython --version\n\nSource: The Hitchhiker’s Guide to Python"
  },
  {
    "objectID": "materials/installation_guide.html#install-additional-command-line-tools",
    "href": "materials/installation_guide.html#install-additional-command-line-tools",
    "title": "Setting up your Development Environment",
    "section": "Install additional command-line tools",
    "text": "Install additional command-line tools\n\nTesseract\n\nInstall the text recognition engine Tesseract, which allows extracting text from images, with:\n brew install tesseract\nInstall the various language models for Tesseract with:\n brew install tesseract-lang\n\nSource: Tesseract\n\n\nwget\n\nInstall the tool wget that allows you to retrieve content from web servers via the command-line with:\n brew install wget"
  },
  {
    "objectID": "materials/installation_guide.html#install-vs-code",
    "href": "materials/installation_guide.html#install-vs-code",
    "title": "Setting up your Development Environment",
    "section": "Install VS Code",
    "text": "Install VS Code\nUnless you have another favourite coding editor, install and customize VS Code editor.\n\nDownload and install VS Code from the official website: https://code.visualstudio.com/Download. You may want to add a shortcut icon to the desktop.\nOpen VS Code. You can skip the initial configuration of VS Code by clicking Next Section.\nTo make programming in Python easier, you should install two more extensions: VS Code Python extension and Tabnine. Launch the VS Code Quick Open again by pressing CMD + P , paste the command, and press enter:\next install ms-python.python\nSimilarly, install the extension Tabnine using the following command:\next install tabnine.tabnine-vscode"
  },
  {
    "objectID": "materials/installation_guide.html#install-ubuntu-in-a-windows-subsystem",
    "href": "materials/installation_guide.html#install-ubuntu-in-a-windows-subsystem",
    "title": "Setting up your Development Environment",
    "section": "Install Ubuntu in a Windows Subsystem",
    "text": "Install Ubuntu in a Windows Subsystem\nTo use the powerful Bash tools on your Windows computer, we install a Ubuntu Linux system within the Windows environment using WSL1.\n\nOpen PowerShell as administrator by right-clicking on the application icon in the start menu and run the following command in the shell:\nwsl --set-default-version 1\nwsl --install -d Ubuntu\nReboot your computer to complete the installation of WSL and Ubuntu.\nThe installation should be completed automatically after the reboot. If not, you can launch Ubuntu from your Windows start menu. When you have any problems, let me know and I will gladly assist. It may require additional configuration of your BIOS.\nOnce the installation is complete, you will be prompted to create a new user account and set a password. IMPORTANT: Remember these credentials as they are used to switch to the administrator mode on your Linux system. You may choose the same account name and password as on your hosting Windows system.\nAfter the login, update your freshly installed Ubuntu system with the following commands:\nsudo apt update && sudo apt upgrade\nCreate a symbolic link in the Bash to easily access your files on Windows. For example, you may want to link the folder Documents on Windows to the symbolic folder documents on Ubuntu. To do this, replace the <YOUR_USERNAME> with the actual one and run the following commands:\ncd ~\nln -s /mnt/c/Users/<YOUR_WINDOWS_USERNAME>/Documents documents\nYour Documents on Windows can be accessed like this:\nls documents\nAnnoyingly, the copy/paste behaviour is different in command lines on Windows. Any selected text is copied automatically, and to paste it, you have to right-click on your mouse. You may want to reassign the shortcuts to Ctrl+Shift+C and Ctrl+Shift+V in the menu (right-click on the windows title bar → Properties → Options ). Using Ctrl+C is not possible as it is used to cancel a running program.\n\nSource: Microsoft"
  },
  {
    "objectID": "materials/installation_guide.html#install-additional-command-line-tools-1",
    "href": "materials/installation_guide.html#install-additional-command-line-tools-1",
    "title": "Setting up your Development Environment",
    "section": "Install additional command-line tools",
    "text": "Install additional command-line tools\n\nOpen Ubuntu from your start menu in Windows.\nInstall essential tools via the command-line:\nsudo apt-get install build-essential\nInstall the Python package manager PIP:\nsudo apt install python3-pip\n\n\nTesseract\n\nInstall the text recognition engine Tesseract, which allows extracting text from images, with:\nsudo add-apt-repository -y ppa:alex-p/tesseract-ocr5\nsudo apt install -y tesseract-ocr\nInstall the German language model for Tesseract with:\nsudo apt install tesseract-ocr-deu\n\nSource: Tesseract"
  },
  {
    "objectID": "materials/installation_guide.html#install-the-editor-vs-code",
    "href": "materials/installation_guide.html#install-the-editor-vs-code",
    "title": "Setting up your Development Environment",
    "section": "Install the editor VS Code",
    "text": "Install the editor VS Code\nUnless you have another favourite coding editor, install and customize VS Code editor.\n\nDownload and install VS Code from the official website: https://code.visualstudio.com/Download. You may want to add a shortcut icon to the desktop.\nOpen VS Code. You can skip all the configuration of VS Code by clicking Next Section.\nWe want to execute all the Python code within the Ubuntu subsystem. For this, we need to install a particular extension. Launch the VS Code Quick Open by pressing CTRL+P, paste the following command, and press enter:\next install ms-vscode-remote.vscode-remote-extensionpack\nIn the left lower corner, you should now see a green label with WSL: Ubuntu (see figure 1). If not, press F1, select Remote-WSL: New Window to connect the subsystem.\nTo make programming in Python easier, you should install two more extensions: VS Code Python extension and Tabnine. Launch again the VS Code Quick Open by pressing CTRL+P, paste the command, and press enter:\next install ms-python.python\nSimilarly, install the extension Tabnine using the following command:\next install tabnine.tabnine-vscode\n\nSource: Microsoft\n\n\n\nWSL Ubuntu is successfully connected"
  },
  {
    "objectID": "materials/installation_guide.html#install-python-packages",
    "href": "materials/installation_guide.html#install-python-packages",
    "title": "Setting up your Development Environment",
    "section": "Install Python packages",
    "text": "Install Python packages\nWe need to install some additional packages that are not included in the Python standard library.\nA common and easy way to install everything needed in a project is by using a provided requirements.txt file that lists all the used Python packages. You find such a file in the repository of this seminar, available on GitHub. We will learn more about GitHub and the installed packages later in the course. For now, you can think of a GitHub repository as a publicly available project folder that is versioned.\n\nOpen a command-line (Terminal on Mac, Ubuntu on Windows) and download the GitHub repository of this course.\n# Windows user only:\n# Change into the `Documents` folder \n# to make is easily accessible from Windows\ncd /mnt/c/Users/<YOUR_WINDOWS_USERNAME>/Documents\n\n# everyone\ngit clone https://github.com/aflueckiger/KED2023.git\ncd KED2023\nNow, you can install the necessary packages listed in the requirements.txt with a single command:\npython3 -m pip install -r requirements.txt"
  },
  {
    "objectID": "materials/installation_guide.html#first-steps-in-python",
    "href": "materials/installation_guide.html#first-steps-in-python",
    "title": "Setting up your Development Environment",
    "section": "First Steps in Python",
    "text": "First Steps in Python\nAs a kind of initiation ritual, say hello to the programming world in Python.\n\nOpen the VS Code editor and create a new file called hello_world.py.\nWrite the following code in that file:\nmsg = \"Hello World!\"\nprint(msg)\nExecute the code by right-clicking in the window and choosing Run Python File in Terminal.\n\nCongrats, you wrote your first little program in Python. It may not be as impressive as you would have imagined, but you can go along and will learn by practising. The list of tutorials below provides a great starting point to learn the basics of Python by solving little exercises interactively:\n\nPython Principles\nLearnPython\n\n\n\n\nWrite your first Python script in VS Code"
  },
  {
    "objectID": "materials/code/KED2023_10.html",
    "href": "materials/code/KED2023_10.html",
    "title": "\nThe ABC of Computational Text Analysis\n",
    "section": "",
    "text": "Alex Flückiger\n\n\n12/19 May 2023"
  },
  {
    "objectID": "materials/code/KED2023_10.html#modules",
    "href": "materials/code/KED2023_10.html#modules",
    "title": "\nThe ABC of Computational Text Analysis\n",
    "section": "Modules",
    "text": "Modules\n\nStanding of the Shoulders of Giants\n\nspaCy: use or build state-of-the-art NLP pipeline\ntextaCy: do high-level analysis, extends spaCy\nscattertext: visualize differences across corpora\npandas: analyze tabular data\nplotnine: visualize anything (ggplot for Python)"
  },
  {
    "objectID": "materials/code/KED2023_10.html#linguistic-features",
    "href": "materials/code/KED2023_10.html#linguistic-features",
    "title": "\nThe ABC of Computational Text Analysis\n",
    "section": "Linguistic Features",
    "text": "Linguistic Features\nFeatures per token and their dependencies\n\n# visualize dependencies\nspacy.displacy.render(doc, style=\"dep\")\n\n\n\n    Apple\n    PROPN\n\n\n\n    's\n    PART\n\n\n\n    CEO\n    PROPN\n\n\n\n    Tim\n    PROPN\n\n\n\n    Cook\n    PROPN\n\n\n\n    is\n    AUX\n\n\n\n    looking\n    VERB\n\n\n\n    at\n    ADP\n\n\n\n    buying\n    VERB\n\n\n\n    U.K.\n    PROPN\n\n\n\n    startup\n    VERB\n\n\n\n    for\n    ADP\n\n\n\n    $\n    SYM\n\n\n\n    1\n    NUM\n\n\n\n    billion.\n    NUM\n\n\n\n    \n    \n        poss\n    \n    \n\n\n\n    \n    \n        case\n    \n    \n\n\n\n    \n    \n        nsubj\n    \n    \n\n\n\n    \n    \n        compound\n    \n    \n\n\n\n    \n    \n        nsubj\n    \n    \n\n\n\n    \n    \n        aux\n    \n    \n\n\n\n    \n    \n        prep\n    \n    \n\n\n\n    \n    \n        pcomp\n    \n    \n\n\n\n    \n    \n        dobj\n    \n    \n\n\n\n    \n    \n        dep\n    \n    \n\n\n\n    \n    \n        prep\n    \n    \n\n\n\n    \n    \n        quantmod\n    \n    \n\n\n\n    \n    \n        compound\n    \n    \n\n\n\n    \n    \n        pobj"
  },
  {
    "objectID": "materials/code/KED2023_10.html#get-linguistic-features",
    "href": "materials/code/KED2023_10.html#get-linguistic-features",
    "title": "\nThe ABC of Computational Text Analysis\n",
    "section": "Get linguistic features",
    "text": "Get linguistic features\n\n# iterate over tokens of a document\nfor token in doc:\n    print(token.text, \"-->\", token.lemma_, token.pos_,\n            token.dep_, token.shape_, token.is_alpha, token.is_stop)\n\nApple --> Apple PROPN poss Xxxxx True False\n's --> 's PART case 'x False True\nCEO --> CEO PROPN nsubj XXX True False\nTim --> Tim PROPN compound Xxx True False\nCook --> Cook PROPN nsubj Xxxx True False\nis --> be AUX aux xx True True\nlooking --> look VERB ROOT xxxx True False\nat --> at ADP prep xx True True\nbuying --> buy VERB pcomp xxxx True False\nU.K. --> U.K. PROPN dobj X.X. False False\nstartup --> startup VERB dep xxxx True False\nfor --> for ADP prep xxx True True\n$ --> $ SYM quantmod $ False False\n1 --> 1 NUM compound d False False\nbillion --> billion NUM pobj xxxx True False\n. --> . PUNCT punct . False False"
  },
  {
    "objectID": "materials/code/KED2023_10.html#named-entity-recognition-ner",
    "href": "materials/code/KED2023_10.html#named-entity-recognition-ner",
    "title": "\nThe ABC of Computational Text Analysis\n",
    "section": "Named Entity Recognition (NER)",
    "text": "Named Entity Recognition (NER)\n\n# visualize named entities\nspacy.displacy.render(doc, style=\"ent\")\n\n\n\n    Apple\n    ORG\n\n's CEO \n\n    Tim Cook\n    PERSON\n\n is looking at buying \n\n    U.K.\n    GPE\n\n startup for \n\n    $1 billion\n    MONEY\n\n.\n\n\n\n# iterate over named entities of a document\nfor ent in doc.ents:\n    print(f\"{ent.text} --> {ent.label_} ({spacy.explain(ent.label_)})\" )\n\nApple --> ORG (Companies, agencies, institutions, etc.)\nTim Cook --> PERSON (People, including fictional)\nU.K. --> GPE (Countries, cities, states)\n$1 billion --> MONEY (Monetary values, including unit)"
  },
  {
    "objectID": "materials/code/KED2023_10.html#read-from-a-file",
    "href": "materials/code/KED2023_10.html#read-from-a-file",
    "title": "\nThe ABC of Computational Text Analysis\n",
    "section": "Read from a file",
    "text": "Read from a file\n\n# alternatively, read from a single txt file \nf_text = '../data/swiss_party_programmes/txt/sp_programmes/1920_parteiprogramm_d.txt'\ntext = textacy.io.read_text(f_text)\n\n# show content\n# special generator syntax as text is read just-in-time (stream individual lines)\nprint(next(text)[:200])\n\nProgramm der Sozialdemokratischen Partei der\nSchweiz\n(Angenommen durch den Parteitag vom 10./12. Dezember 1920 in Bern.)\n\nPrinzipienerklärung.\nDas Endziel der Sozialdemokratie bildet eine Gesellschaft"
  },
  {
    "objectID": "materials/code/KED2023_10.html#steps-to-create-a-corpus",
    "href": "materials/code/KED2023_10.html#steps-to-create-a-corpus",
    "title": "\nThe ABC of Computational Text Analysis\n",
    "section": "Steps to create a Corpus",
    "text": "Steps to create a Corpus\nHow to make a corpus from many text files?\n\nlist all files of a folder\nread text from each file\nparse metadata from file name\nreturn each document sequentially\n\n→ wrap all this in a function get_texts()"
  },
  {
    "objectID": "materials/code/KED2023_10.html#define-function",
    "href": "materials/code/KED2023_10.html#define-function",
    "title": "\nThe ABC of Computational Text Analysis\n",
    "section": "Define Function",
    "text": "Define Function\n\ndef get_texts(dir_texts):\n    \"\"\"\n    Sequentially stream all documents from a given folder, including metadata.\n    \"\"\"\n    p = Path(dir_texts) # set base directory\n    \n    # iterate over all documents in base directory\n    for fname in p.glob('**/*.txt'):\n        \n        print('Parsing file:', fname.name)\n        \n        text = next(textacy.io.text.read_text(fname))\n        # join lines as there are hard line-breaks\n        text = text.replace('\\n', ' ')\n        # further modify the text here if needed\n\n        # parse year from filename and set a metadata\n        # example: 1920_parteiprogramm_d.txt --> year=1920\n        try:\n            year = int(fname.name.split('_')[0])\n        except ValueError:\n            print('WARNING: Parsing meta data has failed:', fname.name)\n            continue\n\n        # add more metadata here if needed\n        metadata = {'fname': fname.name, 'year': year}\n        \n        # return documents one after another (sequentially)\n        yield (text, metadata)"
  },
  {
    "objectID": "materials/code/KED2023_10.html#create-a-corpus-from-txt",
    "href": "materials/code/KED2023_10.html#create-a-corpus-from-txt",
    "title": "\nThe ABC of Computational Text Analysis\n",
    "section": "Create a Corpus from TXT",
    "text": "Create a Corpus from TXT\nProcess documents and create corpus\n\n# stream texts from a given folder\ndir_texts = '../data/swiss_party_programmes/txt/sp_programmes/'\ntexts = get_texts(dir_texts)\n\n# load German language model\nde = textacy.load_spacy_lang(\"de_core_news_sm\")\n\n# create corpus from processed documents\ncorpus = textacy.Corpus(de, data=texts)\n\nParsing file: 1920_parteiprogramm_d.txt\nParsing file: 1982_parteiprogramm_d_0.txt\nParsing file: 1888_parteiprogramm_d_0.txt\nParsing file: sp-parteiprogramm_definitiv-de_0.txt\nWARNING: Parsing meta data has failed: sp-parteiprogramm_definitiv-de_0.txt\nParsing file: 1904_parteiprogramm_d_0.txt\nParsing file: 1959_parteiprogramm_d_0.txt\nParsing file: 1870_parteiprogramm_d_0.txt\nParsing file: 1935_parteiprogramm_d_0.txt"
  },
  {
    "objectID": "materials/code/KED2023_10.html#export-word-counts",
    "href": "materials/code/KED2023_10.html#export-word-counts",
    "title": "\nThe ABC of Computational Text Analysis\n",
    "section": "Export Word Counts",
    "text": "Export Word Counts\n\n# get lowercased and filtered corpus vocabulary\nvocab = corpus.word_counts(by= 'lower_', weighting='count', filter_stops = True, filter_punct = True, filter_nums = True)\n\n# sort vocabulary by descending frequency\nvocab_sorted = sorted(vocab.items(), key=lambda x: x[1], reverse=True)\n\n# write to file, one word and its frequency per line\nfname = '../analysis/vocab_frq.txt'\nwith open(fname, 'w') as f:   \n    for word, frq in vocab_sorted:\n        line = f\"{word}\\t{frq}\\n\"\n        f.write(line)\n\nvocab_sorted[:5]\n\n[('menschen', 112),\n ('partei', 101),\n ('demokratie', 94),\n ('entwicklung', 92),\n ('staat', 92)]"
  },
  {
    "objectID": "materials/code/KED2023_10.html#load-csv-file",
    "href": "materials/code/KED2023_10.html#load-csv-file",
    "title": "\nThe ABC of Computational Text Analysis\n",
    "section": "Load CSV File",
    "text": "Load CSV File\nload a dataset of 1 August speeches by Swiss federal councillors\n\n# read dataset from csv file\nf_csv = '../data/dataset_speeches_federal_council_2019.csv'\ndf = pd.read_csv(f_csv)\n\n# filter out non-german texts or very short texts\ndf_sub = df[(df['Sprache'] == 'de') & (df['Text'].str.len() > 10)]\n\n# make new column containing all relevant metadata (showing in plot later on)\ndf_sub['descripton'] = df_sub[['Redner', 'Partei', 'Jahr']].astype(str).agg(', '.join, axis=1)\n\n# sneak peek of dataset\ndf_sub.head()\n\n\n\n\n\n  \n    \n      \n      Jahr\n      Status\n      Vollständigkeit\n      Redner\n      Geschlecht\n      Funktion\n      Partei\n      Partei-Original\n      Typ\n      Bemerkung\n      Sprache\n      Originalsprache\n      Ort\n      Titel\n      Anrede\n      Text\n      Originaltext\n      Quelle\n      descripton\n    \n  \n  \n    \n      0\n      2018\n      done\n      vollständig\n      Alain Berset\n      m\n      BP\n      SP\n      SP\n      BP-Rede\n      NaN\n      de\n      NaN\n      NaN\n      NaN\n      Sehr geehrte Damen und Herren\n      Wir leben in der Schweiz in Frieden und Wohlst...\n      NaN\n      https://www.admin.ch/gov/de/start/dokumentatio...\n      Alain Berset, SP, 2018\n    \n    \n      3\n      2018\n      done\n      vollständig\n      Doris Leuthard\n      f\n      BR\n      CVP\n      CVP\n      Lokal\n      NaN\n      de\n      NaN\n      Villmergen\n      NaN\n      Liebe Mitbürgerinnen und Mitbürger\n      Ich bedanke mich für die Einladung zu Ihrer 1....\n      NaN\n      https://www.admin.ch/gov/de/start/dokumentatio...\n      Doris Leuthard, CVP, 2018\n    \n    \n      4\n      2018\n      done\n      vollständig\n      Guy Parmelin\n      m\n      BR\n      SVP\n      SVP\n      Lokal\n      NaN\n      de\n      NaN\n      NaN\n      «Armbrust und Hellebarde»\n      Sehr geehrte Eidgenossen, Meine Damen und Herren\n      Eine 1.-August-Rede ist eine der heikelsten rh...\n      NaN\n      https://www.admin.ch/gov/de/start/dokumentatio...\n      Guy Parmelin, SVP, 2018\n    \n    \n      5\n      2018\n      done\n      vollständig\n      Ignazio Cassis\n      m\n      BR\n      FDP\n      FDP\n      Lokal\n      NaN\n      de\n      NaN\n      Rorschach\n      Die Italianità hat ihre Wurzeln in Rorschach\n      Gueten Obig mitenand!\n      Und danke für diese freundliche Einladung!\\nIc...\n      NaN\n      https://www.admin.ch/gov/de/start/dokumentatio...\n      Ignazio Cassis, FDP, 2018\n    \n    \n      6\n      2018\n      done\n      vollständig\n      Simonetta Sommaruga\n      f\n      BR\n      SP\n      SP\n      Lokal\n      NaN\n      de\n      NaN\n      Muttenz\n      Heimat kennt keine Grenzen\n      Liebe Festgemeinde,\\nSehr geehrter Herr Regier...\n      Als ich die Einladung zu dieser Bundesfeier ge...\n      NaN\n      https://www.ejpd.admin.ch/ejpd/de/home/aktuell...\n      Simonetta Sommaruga, SP, 2018"
  },
  {
    "objectID": "materials/code/KED2023_10.html#create-scattertext-plot",
    "href": "materials/code/KED2023_10.html#create-scattertext-plot",
    "title": "\nThe ABC of Computational Text Analysis\n",
    "section": "Create Scattertext Plot",
    "text": "Create Scattertext Plot\n\ncensor_tags = set(['CARD']) # tags to ignore in corpus, e.g. numbers\n\n# stop words to ignore in corpus\nde_stopwords = spacy.lang.de.stop_words.STOP_WORDS # default stop words\ncustom_stopwords = set(['[', ']', '%'])\nde_stopwords = de_stopwords.union(custom_stopwords) # extend with custom stop words\n\n# create corpus from dataframe\n# lowercased terms, no stopwords, no numbers\n# use lemmas for English only, German quality is too bad\ncorpus_speeches = st.CorpusFromPandas(df_sub, # dataset\n                             category_col='Partei', # index differences by ...\n                             text_col='Text', \n                             nlp=de, # German model\n                             feats_from_spacy_doc=st.FeatsFromSpacyDoc(tag_types_to_censor=censor_tags, use_lemmas=False),\n                             ).build().get_stoplisted_unigram_corpus(de_stopwords)\n# produce visualization (interactive html)\nhtml = st.produce_scattertext_explorer(corpus_speeches,\n            category='SP', # set attribute to divide corpus into two parts\n            category_name='SP',\n            not_category_name='other parties',\n            metadata=df_sub['descripton'],\n            width_in_pixels=1000,\n            minimum_term_frequency=5, # drop terms occurring less than 5 times\n            save_svg_button=True,                          \n)\n\n# write visualization to html file\nfname = \"../analysis/viz_party_differences.html\"\nopen(fname, 'wb').write(html.encode('utf-8'))\n\n2479069"
  },
  {
    "objectID": "materials/code/KED2023_10.html#create-corpus-from-csv",
    "href": "materials/code/KED2023_10.html#create-corpus-from-csv",
    "title": "\nThe ABC of Computational Text Analysis\n",
    "section": "Create Corpus from CSV",
    "text": "Create Corpus from CSV\nHow to make a corpus from a dataset in .csv-format?\n→ define a new function get_texts_from_csv, similar to get_texts\n\ndef get_texts_from_csv(f_csv, text_column):\n    \"\"\"\n    Read dataset from a csv file and sequentially stream the rows,\n    including metadata.\n    \"\"\"\n    \n    # read dataframe\n    df = pd.read_csv(f_csv)\n    \n    # keep only documents that have text\n    filtered_df = df[df[text_column].notnull()]\n    \n    # iterate over rows in dataframe\n    for idx, row in filtered_df.iterrows():\n        \n        # read text and join lines (remove hard line-breaks)\n        text = row[text_column].replace('\\n', ' ')\n\n        # use all columns as metadata, except the column with the actual text\n        metadata = row.to_dict()\n        del metadata[text_column]\n\n        yield (text, metadata)\n\nf_csv = '../data/dataset_speeches_federal_council_2019.csv'\ntexts = get_texts_from_csv(f_csv, text_column='Text')\n\ncorpus_speeches = textacy.Corpus(de, data=texts)"
  },
  {
    "objectID": "materials/code/KED2023_10.html#create-a-group-term-matrix",
    "href": "materials/code/KED2023_10.html#create-a-group-term-matrix",
    "title": "\nThe ABC of Computational Text Analysis\n",
    "section": "Create a Group-Term Matrix",
    "text": "Create a Group-Term Matrix\n\n# define what groups are formed and what terms should be included\n# here, groups by year and words are lowercased (incl. stop words)\ntokenized_docs, groups = textacy.io.unzip(\n        (textacy.extract.utils.terms_to_strings(textacy.extract.words(doc, filter_stops=False), by=\"lower\"),\n        doc._.meta[\"Jahr\"])\n        for doc in corpus_speeches)\n\n# define how to count\n# here relative term frequency\nvectorizer = textacy.representations.vectorizers.GroupVectorizer(\n        tf_type='linear', # absolute term frequency\n        dl_type=\"linear\", # normalized by document length\n        vocabulary_grps=range(1950, 2019)) # limit to years from 1950 to 2019\n\n# create group-term-matrix with with frequency counts\ngrp_term_matrix = vectorizer.fit_transform(tokenized_docs, groups)\n\n# create dataframe from matrix\ndf_terms = pd.DataFrame.sparse.from_spmatrix(grp_term_matrix, index=vectorizer.grps_list, columns=vectorizer.terms_list)\ndf_terms['year'] = df_terms.index\n\n# change shape of dataframe\ndf_tidy = df_terms.melt(id_vars='year', var_name=\"term\", value_name=\"frequency\")\ndf_tidy\n\n\n\n\n\n  \n    \n      \n      year\n      term\n      frequency\n    \n  \n  \n    \n      0\n      1950\n      's\n      0.000000\n    \n    \n      1\n      1951\n      's\n      0.000000\n    \n    \n      2\n      1952\n      's\n      0.000000\n    \n    \n      3\n      1953\n      's\n      0.000000\n    \n    \n      4\n      1954\n      's\n      0.000000\n    \n    \n      ...\n      ...\n      ...\n      ...\n    \n    \n      1332040\n      2014\n      ►\n      0.000000\n    \n    \n      1332041\n      2015\n      ►\n      0.000000\n    \n    \n      1332042\n      2016\n      ►\n      0.000000\n    \n    \n      1332043\n      2017\n      ►\n      0.000000\n    \n    \n      1332044\n      2018\n      ►\n      0.000125\n    \n  \n\n1332045 rows × 3 columns"
  },
  {
    "objectID": "materials/code/KED2023_10.html#plot-frequencies-over-time",
    "href": "materials/code/KED2023_10.html#plot-frequencies-over-time",
    "title": "\nThe ABC of Computational Text Analysis\n",
    "section": "Plot frequencies over time",
    "text": "Plot frequencies over time\n\n# filter the dataset for the following terms\nterms = [\"volk\", \"schweiz\", \"nation\"]\ndf_terms = df_tidy[df_tidy['term'].isin(terms)]\n\n# plot the relative frequency for the terms above\n(ggplot(df_terms, aes(x='year', y='frequency', color='term'))\n + geom_point() # show individual points\n + stat_smooth(method='lowess', span=0.15, se=False) # overlay points with a smoothed line\n + theme_classic()) # make the plot look nicer\n\n\n\n\n<ggplot: (8773265377169)>\n\n\n\n# check some other terms\nterms = [\"solidarität\", \"kultur\", \"werte\"]\n\ndf_terms = df_tidy[df_tidy['term'].isin(terms)]\n\n(ggplot(df_terms, aes('year', 'frequency', color='term'))\n + geom_point(alpha=0.5, stroke = 0)\n + stat_smooth(method='lowess', span=0.10, se=False)\n + theme_classic())\n\n\n\n\n<ggplot: (8773255190459)>"
  },
  {
    "objectID": "materials/code/KED2023_10.html#save-plot",
    "href": "materials/code/KED2023_10.html#save-plot",
    "title": "\nThe ABC of Computational Text Analysis\n",
    "section": "Save Plot",
    "text": "Save Plot\n\n# check some other terms\nterms = [\"schweizer\", \"schweizerinnen\"]\ndf_terms = df_tidy[df_tidy['term'].isin(terms)]\n\np = (ggplot(df_terms, aes('year', 'frequency', color='term'))\n + geom_point(alpha=0.5, stroke = 0) # set transparency\n + stat_smooth(method='lowess', span=0.2, se=False)\n + theme_classic())\n\n# save as png\nfname = '../analysis/rel_term_frq_gender.png'\np.save(filename=fname, dpi=150, verbose = False)\np\n\n\n\n\n<ggplot: (8773252464604)>"
  },
  {
    "objectID": "materials/code/python_basics.html",
    "href": "materials/code/python_basics.html",
    "title": "KED2023",
    "section": "",
    "text": "# define variable*\nx = \"at your service\"\ny = 2\nz = \", most of the time.\"\n\n# combine variables\nint_combo = y * y  # for numbers any mathematical operation\nstr_combo = x + z  # for text only concatenation with +\n\n# show content of variable\nprint(str_combo)\n\nat your service, most of the time.\n\n\n\n\n\n\n# check the type\ntype(x)\n\nstr\n\n\n\n# convert types (similar for other types)\nint('100')  # convert to integer\nstr(100)    # convert to string\n\n# include variable in a f-string\nx = 3\nmixed = f\"x has the value: {x}\"\nprint(mixed)\n\nx has the value: 3\n\n\n\n\n\n\n# assign a value to a variable\nx = 1\nword = \"Test\"\n\n# compare two values if they are identical\n1 == 2  #  False\nword == \"Test\"  # True\n\nTrue\n\n\n\n\n\n\nsentence = [\"This\", \"is\", \"a\", \"sentence\"]\n\n# iterate over each element\nfor token in sentence:\n\n    # do something with the element\n    print(token)\n\nThis\nis\na\nsentence\n\n\n\n\n\n\nsentence = ['This', 'is', 'a', 'sentence']\n\nif len(sentence) == 3:\n    print('This sentence has exactly 3 tokens')\nelif len(sentence) < 5:\n    print('This sentence has less than 5 tokens')\nelse:\n    print('This sentence is longer than 5 tokens')\n\nThis sentence has less than 5 tokens\n\n\n\n\n\n\ntokens = \"This is a sentence\".split(\" \")  # split at whitespace\nprint(tokens, type(tokens))  # check the variable\n\ntokens.append(\".\")  # add something to a list\n\ntokens = \" \".join(tokens)  # join elements to string\nprint(tokens, type(tokens))  # check the variable\n\n['This', 'is', 'a', 'sentence'] <class 'list'>\nThis is a sentence . <class 'str'>\n\n\n\n\n\n\n# define a new function\ndef word_properties(word):\n    \"\"\"\n    Print some properties of the provided word.\n    It takes any string as argument (variable word).\n    \"\"\"\n\n    # print(), len() and sorted() are functions themeselves\n    word_length = len(word)\n    sorted_letters = sorted(word, reverse=True)\n    print(f\"Properties for the word '{word}':\")\n    print(\"length:\", word_length, \"letters:\", sorted_letters)\n\nword_properties(\"computer\") # call function with the word \"computer\"\n\nProperties for the word 'computer':\nlength: 8 letters: ['u', 't', 'r', 'p', 'o', 'm', 'e', 'c']\n\n\n\n\n\n\nsentence =[\"This\", \"is\", \"a\", \"sentence\"]\n\n# element at position X\nfirst_tok = sentence[0]  # 'This'\nprint(first_tok)\n\n# elements of subsequence [start:end]\nsub_seq = sentence[0:3]  # ['This', 'is', 'a']\nprint(sub_seq)\n\n# elements of subsequence backwards\nsub_seq_back = sentence[-2:]  # ['a', 'sentence']\nprint(sub_seq_back)\n\nThis\n['This', 'is', 'a']\n['a', 'sentence']"
  },
  {
    "objectID": "schedule.html",
    "href": "schedule.html",
    "title": "Schedule",
    "section": "",
    "text": "We have 13 seminar sessions together.\nThe plan below is provisional. I am happy to adapt the topics, as well as the schedule, to the needs and interests of the students. Likely, we will change some topics and orderings as we go.\n\n\n\nDate\nTopic\n\n\n\n\n23 February 2023\nIntroduction + Where is the digital revolution?\n\n\n02 March 2023\nText as Data\n\n\n09 March 2023\nSetting up your Development Environment\n\n\n16 March 2023\nIntroduction to the Command-line\n\n\n23 March 2023\nBasic NLP with Command-line\n\n\n30 March 2023 (Zoom)\nLearning Regular Expressions\n\n\n06 April 2023 (Zoom)\nWorking with (your own) Data\n\n\n13 April 2023\nno lecture (Osterpause)\n\n\n20 April 2023\nEthics and the Evolution of NLP\n\n\n27 April 2023\nIntroduction to Python + VS Code\n\n\n04 May 2023\nPandas und LIRI-Dataset\n\n\n11 May 2023\nNLP with Python\n\n\n18 May 2023\nno lecture (Christi Himmelfahrt)\n\n\n25 May 2023\nNLP with Python II + Working Session\n\n\n01 June 2023\nMini-Project Presentations + Discussion"
  },
  {
    "objectID": "assignments.html",
    "href": "assignments.html",
    "title": "Assignments",
    "section": "",
    "text": "You have to submit three assignments to complete the seminar successfully. The purpose of the assignments is not making the course hard to pass but rather to foster your engagement with the covered topics. As you like, you may prefer to work in teams to discuss different approaches. Nonetheless, each student has to come up with their own solution and submit it before the deadline."
  },
  {
    "objectID": "assignments.html#formal-instructions",
    "href": "assignments.html#formal-instructions",
    "title": "Assignments",
    "section": "Formal Instructions",
    "text": "Formal Instructions\nYour submission is a single script, meaning that is readily executable, and is named as follows:\n\nBash scripts: SURNAME_KED2023_NR.sh\nPython scripts: SURNAME_KED2023_NR.py\n\nThe script follows the order of the tasks in the assignment. In addition to the commands you have used in your solution, you also provide a short, yet concise explanation to your solution. You should include these comments directly in the script by starting the line with #.\nPlease use the following examples as template:\n\nBash\n#!/bin/bash\n\n##################################################\n### assignment 1\n### Seminar: The ABC of Computational Text Analysis\n### University of Lucerne\n##################################################\n\n### task 1a)\necho \"this is a test\"\n# solution: echo prints out the provided text in the command-line \n\n### task 1b)\necho -n \"test\" | wc\n# solution: wc counts the lines, words and characters. \n# The argument -n is necessary to omit the trailing new-line symbol.\n# \"test\" has 4 characters.\n\n...\n\n\nPython\n#!/usr/bin/env python3\n# -*- coding: utf-8 -*-\n\n##################################################\n### assignment 1\n### Seminar: The ABC of Computational Text Analysis\n### University of Lucerne\n##################################################\n\n### task 1a)\nprint(\"Hello, World\")\n# outputs the provided string to the prompt\n\n..."
  },
  {
    "objectID": "assignments.html#inspiring-student-projects",
    "href": "assignments.html#inspiring-student-projects",
    "title": "Assignments",
    "section": "Inspiring Student Projects",
    "text": "Inspiring Student Projects\n\nGender differences in 1 August speeches [Code], Dario Haab, Valentina Meyer, Nils Brun, 2023\nAnalysis of Bulletin Board Systeme (BBS), Josias Bruderer, 2021"
  }
]